{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune a llama3 model with m-a-p/Code-Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers[torch] datasets\n",
    "#!pip install -q bitsandbytes trl peft\n",
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 66383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"m-a-p/Code-Feedback\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT data in train/test\n",
    "\n",
    "indices_1 = range(0,100)\n",
    "indices_2 = range(101,201)\n",
    "dataset_dict = {\n",
    "    \"train\": raw_datasets[\"train\"].select(indices_1),\n",
    "    \"test\": raw_datasets[\"train\"].select(indices_2)\n",
    "}\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': 'Write a Ruby code to convert a double-linked list to a single-linked list without using any built-in methods or data structures.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Here is a Ruby code that converts a double-linked list to a single-linked list without using any built-in methods or data structures:\\n\\n```ruby\\nclass Node\\n  attr_accessor :value, :next_node, :prev_node\\n\\n  def initialize(value, next_node = nil, prev_node = nil)\\n    @value = value\\n    @next_node = next_node\\n    @prev_node = prev_node\\n  end\\nend\\n\\ndef convert_to_single_linked_list(head)\\n  current_node = head\\n  while current_node.next_node != nil\\n    current_node.next_node.prev_node = nil\\n    current_node = current_node.next_node\\n  end\\n  current_node\\nend\\n\\n# Create a double-linked list\\nnode1 = Node.new(1)\\nnode2 = Node.new(2)\\nnode3 = Node.new(3)\\n\\nnode1.next_node = node2\\nnode2.prev_node = node1\\nnode2.next_node = node3\\nnode3.prev_node = node2\\n\\n# Convert to single-linked list\\nsingle_linked_list_head = convert_to_single_linked_list(node1)\\n\\n# Print single-linked list\\ncurrent_node = single_linked_list_head\\nwhile current_node != nil\\n  puts current_node.value\\n  current_node = current_node.next_node\\nend\\n```\\n\\nThis code defines a `Node` class representing each node in the double-linked list. The `convert_to_single_linked_list` method takes the head of the double-linked list as input and iterates through the list, removing the previous node references. It returns the new head of the single-linked list.\\n\\nIn this example, a double-linked list with nodes `1 -> 2 -> 3` is created. After conversion, the single-linked list will have nodes `1 -> 2 -> 3`, and it is printed using a loop that iterates through each node in the single-linked list.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Write a code snippet using Python to pop an element from a list, but without using the built-in `pop()` function, slicing, or any additional data structures.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Here\\'s a code snippet that removes an element from a list without using the built-in `pop()`, slicing, or any additional data structures:\\n\\n```python\\ndef pop_element(lst, index):\\n    if index < 0 or index >= len(lst):\\n        raise IndexError(\"Index out of range\")\\n    \\n    element = lst[index]\\n    \\n    for i in range(index, len(lst)-1):\\n        lst[i] = lst[i+1]\\n    \\n    lst.pop()\\n    \\n    return element\\n```\\n\\nHere\\'s how you can use this function:\\n\\n```python\\nmy_list = [1, 2, 3, 4, 5]\\nindex = 2\\n\\nremoved_element = pop_element(my_list, index)\\n\\nprint(\"Removed element:\", removed_element)\\nprint(\"Updated list:\", my_list)\\n```\\n\\nOutput:\\n```\\nRemoved element: 3\\nUpdated list: [1, 2, 4, 5]\\n```\\n\\nNote that this function modifies the original list and returns the removed element. If the index is out of range, it raises an `IndexError` exception.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'messages']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(raw_dataset[\"train\"].features)\n",
    "column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 81 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "You are tasked with implementing a multithreaded program to process a list of tasks concurrently. Each task is represented by a function that takes no arguments and returns a result. Your goal is to design a solution that efficiently utilizes multiple threads to execute these tasks in parallel.\n",
      "\n",
      "You are given a list of `n` tasks and a predefined number of threads to use for concurrent execution. Your task is to implement a function `execute_tasks(tasks: List[Callable[[], Any]], num_threads: int) -> List[Any]` that takes in the list of tasks and the number of threads and returns a list of results obtained after executing all the tasks concurrently using the specified number of threads.\n",
      "\n",
      "The function signature is as follows:\n",
      "```python\n",
      "from typing import List, Callable, Any\n",
      "\n",
      "def execute_tasks(tasks: List[Callable[[], Any]], num_threads: int) -> List[Any]:\n",
      "    pass\n",
      "```\n",
      "\n",
      "Your implementation should ensure that the tasks are distributed among the available threads for parallel execution. Once all tasks are completed, the results should be collected and returned in the same order as the input tasks.\n",
      "\n",
      "For example, given the input `tasks = [task1, task2, task3, task4]` and `num_threads = 2`, the function should execute `task1` and `task2` concurrently using the available threads, and then execute `task3` and `task4` concurrently. The results obtained from these tasks should be returned in the order `[result1, result2, result3, result4]`.\n",
      "\n",
      "Assume that the tasks are independent and do not share any resources, and the order of execution does not affect the results.<|end_of_text|>\n",
      "<|assistant|>\n",
      "```python\n",
      "import threading\n",
      "\n",
      "def execute_tasks(tasks: List[Callable[[], Any]], num_threads: int) -> List[Any]:\n",
      "    results = [None] * len(tasks)\n",
      "    task_index = 0\n",
      "    lock = threading.Lock()\n",
      "\n",
      "    def execute_task(task_index):\n",
      "        nonlocal results\n",
      "        while task_index < len(tasks):\n",
      "            task = tasks[task_index]\n",
      "            result = task()\n",
      "            with lock:\n",
      "                results[task_index] = result\n",
      "                task_index += 1\n",
      "\n",
      "    threads = []\n",
      "    for _ in range(num_threads):\n",
      "        thread = threading.Thread(target=execute_task, args=(task_index,))\n",
      "        threads.append(thread)\n",
      "        thread.start()\n",
      "\n",
      "    for thread in threads:\n",
      "        thread.join()\n",
      "\n",
      "    return results\n",
      "```\n",
      "\n",
      "In the solution, the `execute_tasks` function initializes a list `results` to store the results of the tasks. It also creates a lock to ensure thread-safe access to the `results` list. The `execute_task` function is defined to execute individual tasks and update the `results` list using the lock.\n",
      "\n",
      "The main function creates and starts multiple threads, each executing the `execute_task` function with a different task index. Once all threads have completed their tasks, the results are collected and returned in the original order.<|end_of_text|>\n",
      "<|user|>\n",
      "You are tasked with implementing a dependency resolution algorithm for a software package manager. The package manager maintains a list of software components and their dependencies. Your goal is to write a function that takes a list of software components and their dependencies and returns a valid installation order for the components, considering their dependencies.\n",
      "\n",
      "You are given the following information:\n",
      "- The list `detail_parts` contains details of the software components and their dependencies. Each element in `detail_parts` is a string representing a software component and its dependencies in the following format:\n",
      "  - The string is in the form \"component: dependency1, dependency2, ...\" where `component` is the name of the software component and `dependency1`, `dependency2`, etc. are its dependencies.\n",
      "  - If a component has no dependencies, the string is in the form \"component:\".\n",
      "  - If a component has no details, the string is an empty string.\n",
      "\n",
      "You need to implement a function `resolve_dependencies(detail_parts)` that returns a valid installation order for the software components, considering their dependencies. If there are multiple valid installation orders, return any one of them.\n",
      "\n",
      "For example, given the input `detail_parts = [\"A: B, C\", \"B: D\", \"C: D\", \"D:\"]`, the function should return `[\"D\", \"B\", \"C\", \"A\"]` as a valid installation order.<|end_of_text|>\n",
      "<|assistant|>\n",
      "```python\n",
      "def resolve_dependencies(detail_parts):\n",
      "    dependencies = {}\n",
      "    for part in detail_parts:\n",
      "        if part:\n",
      "            component, *deps = part.split(':')\n",
      "            dependencies[component.strip()] = [dep.strip() for dep in deps[0].split(',')]\n",
      "\n",
      "    def dfs(component, visited, order):\n",
      "        visited.add(component)\n",
      "        for dep in dependencies.get(component, []):\n",
      "            if dep not in visited:\n",
      "                dfs(dep, visited, order)\n",
      "        order.append(component)\n",
      "\n",
      "    visited = set()\n",
      "    installation_order = []\n",
      "    for component in dependencies:\n",
      "        if component not in visited:\n",
      "            dfs(component, visited, installation_order)\n",
      "\n",
      "    return installation_order[::-1]\n",
      "```\n",
      "\n",
      "The `resolve_dependencies` function first parses the input `detail_parts` to build a dictionary `dependencies` where each component is associated with its dependencies. Then, it performs a depth-first search (DFS) to traverse the components and their dependencies, adding them to the `installation_order` list in reverse order. Finally, it returns the `installation_order` list reversed to obtain a valid installation order for the software components.<|end_of_text|>\n",
      "\n",
      "Sample 14 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Design a sophisticated text editor with advanced functionalities. Along with essential functions such as syntax highlighting, line numbering, auto-indentation, and bracket matching, implement features like multi-language support, regular expression search, code folding, real-time syntax error detection and auto code-completion. Furthermore, ensure that the editor supports both light and dark themes, and provide an interface for adding custom themes. The editor should also have a 'read-only' mode, and feature elements of a standard IDE like debuggers and built-in terminals for shell command executions. The challenge is to optimize the editor for performance, provide seamless multi-file handling and support for large files.<|end_of_text|>\n",
      "<|assistant|>\n",
      "Creating a sophisticated text editor with advanced functionalities requires a comprehensive understanding of UI/UX design, frontend development, and integrating various libraries and APIs. Implementing all the requested features may require significant effort and expertise. Here's a high-level approach to building such a text editor:\n",
      "\n",
      "1. User Interface: Design a modern and user-friendly interface for the text editor. Incorporate essential features like syntax highlighting, line numbering, auto-indentation, and bracket matching. Provide options to switch between light and dark themes, as well as the ability to add custom themes.\n",
      "\n",
      "2. Multi-Language Support: Implement language-specific syntax highlighting and auto-completion for multiple programming languages. You can utilize libraries like CodeMirror or Monaco Editor, which provide built-in support for various languages.\n",
      "\n",
      "3. Regular Expression Search: Integrate regular expression search functionality to enable users to search and replace text using regex patterns. Libraries like XRegExp can help with this feature.\n",
      "\n",
      "4. Code Folding: Implement code folding to allow users to collapse and expand sections of code, improving readability for large files. CodeMirror and Monaco Editor provide built-in support for code folding.\n",
      "\n",
      "5. Real-Time Syntax Error Detection: Utilize language-specific linters or parsers to detect syntax errors as the user types. These tools can provide real-time error highlighting and suggestions. For example, ESLint for JavaScript and Pylint for Python.\n",
      "\n",
      "6. Auto Code-Completion: Use language-specific autocompletion libraries or APIs to provide intelligent code suggestions while typing. Tools like Tern.js or Language Server Protocol (LSP) can assist with this feature.\n",
      "\n",
      "7. Read-Only Mode: Implement a read-only mode to prevent accidental modifications to important files, allowing users to view code without editing capabilities.\n",
      "\n",
      "8. Debugger Integration: Integrate debuggers for different programming languages to enable step-by-step debugging within the editor. The specific implementation will depend on the debugger and language being used.\n",
      "\n",
      "9. Built-in Terminal: Implement a terminal within the text editor to execute shell commands without leaving the editor's environment. You can use libraries like xterm.js or ETerm for this feature.\n",
      "\n",
      "10. Performance Optimization: Optimize the text editor's performance for seamless handling of large files. Techniques like virtualization of the text area, using worker threads, and lazy loading can be applied to enhance performance.\n",
      "\n",
      "11. Multi-File Handling: Implement features to handle multiple files simultaneously, such as tabs or a file explorer panel. Users should be able to switch between open files easily.\n",
      "\n",
      "Remember, building a sophisticated text editor with all the requested features is a complex task. It may require expertise in JavaScript, frontend frameworks like React or Angular, and integration with various libraries. It's recommended to break down the development process into smaller tasks and work on them incrementally.<|end_of_text|>\n",
      "<|user|>\n",
      "The assistant hasn't generated any code yet. Could you please generate the code for the text editor as described?<|end_of_text|>\n",
      "<|assistant|>\n",
      "I apologize for any misunderstanding, but creating a full-featured, advanced text editor in a conversation like this is practically impossible. The development of such an application would require a team of developers and a fair amount of time. It involves various aspects of programming including front-end design, back-end development, linking the two, and testing.\n",
      "\n",
      "However, to help you get started, I can provide some sample Python code that creates a basic text editor using built-in libraries. From there, you could add more functionalities step by step.\n",
      "\n",
      "Here is a basic Python code snippet that creates a text editor using the tkinter library.\n",
      "\n",
      "```python\n",
      "import tkinter as tk\n",
      "from tkinter import filedialog\n",
      "\n",
      "def open_file():\n",
      "    file_path = filedialog.askopenfilename()\n",
      "    file = open(file_path, \"r\")\n",
      "    text.insert(\"1.0\", file.read())\n",
      "    file.close()\n",
      "\n",
      "def save_file():\n",
      "    file_path = filedialog.asksaveasfilename()\n",
      "    file = open(file_path, \"w\")\n",
      "    file.write(text.get(\"1.0\", tk.END))\n",
      "    file.close()\n",
      "\n",
      "root = tk.Tk()\n",
      "\n",
      "text = tk.Text(root)\n",
      "text.pack()\n",
      "\n",
      "menu_bar = tk.Menu(root)\n",
      "file_menu = tk.Menu(menu_bar, tearoff=0)\n",
      "file_menu.add_command(label=\"Open\", command=open_file)\n",
      "file_menu.add_command(label=\"Save\", command=save_file)\n",
      "menu_bar.add_cascade(label=\"File\", menu=file_menu)\n",
      "\n",
      "root.config(menu=menu_bar)\n",
      "root.mainloop()\n",
      "```\n",
      "\n",
      "This code implements basic functionality such as opening files, editing text, and saving files. However, it lacks advanced features, such as syntax highlighting, auto-indentation, bracket matching, multi-language support, regular expression search, code folding, real-time syntax error detection, auto code-completion, theme support, read-only mode, debugger, built-in terminal, performance optimization, and multi-file handling. \n",
      "\n",
      "For these advanced features, you might want to consider using web technologies (HTML, CSS, JavaScript) along with libraries like CodeMirror or Monaco Editor. For a Python-based solution, you could consider using a toolkit like PyQt, but be aware that it will be far more involved and not as powerful or versatile as a web-based solution. \n",
      "\n",
      "Please consult with a developer or a development team for a full-scale project like this.<|end_of_text|>\n",
      "<|user|>\n",
      "Consider adding comments for better understanding. Introduce more advanced features like syntax highlighting or a find and replace functionality.<|end_of_text|>\n",
      "<|assistant|>\n",
      "I've added some code to create a basic text editor with the Tkinter Python library, including the requested \"Find & Replace\" feature and some more comments for better understanding. However, due to the limitations of this environment, I can't run the Tkinter based GUI application here, but you can try running this code in your local Python environment.\n",
      "\n",
      "Code explanation:\n",
      "\n",
      "1. We start off by importing the required modules.\n",
      "\n",
      "2. The `open_file` function uses 'askopenfilename' to prompt the user to select a file to open. The chosen file is opened in read mode and its contents are inserted into the text widget.\n",
      "\n",
      "3. The `save_file` function prompts the user to choose a location to save their file using 'asksaveasfilename.' The content of the text widget is then written to the chosen file.\n",
      "\n",
      "4. The `find_replace` function prompts the user to enter the text they want to find and the text they want to replace it with. It then replaces all instances of the find text with the replace text in the text widget.\n",
      "\n",
      "5. We create a root widget which will be the main window of our application.\n",
      "\n",
      "6. A text widget is created and added to the root widget. This is where the user can write and edit text.\n",
      "\n",
      "7. We create a menu bar which will contain all our options.\n",
      "\n",
      "8. We add a \"File\" menu to the menu bar. This menu contains options to open and save files.\n",
      "\n",
      "9. We also add an \"Edit\" menu to the menu bar. This menu contains the \"Find & Replace\" option.\n",
      "\n",
      "10. Finally, we go into the Tkinter event loop by calling 'root.mainloop()'. This is where the program will wait for events (like button clicks and keyboard presses) and respond to them.\n",
      "\n",
      "Please note that adding syntax highlighting is a far more complex task that would require an understanding of the syntax of the language you're trying to highlight, and would involve parsing the text and applying different styles to different elements. It is outside the scope of this simple text editor. However, you could use a more advanced text widget that supports syntax highlighting, such as TextMate or Scintilla, if you're interested in this feature.<|end_of_text|>\n",
      "<|user|>\n",
      "Please generate the initial version of the text editor code with basic functionalities like opening, editing, and saving a file.<|end_of_text|>\n",
      "<|assistant|>\n",
      "```python\n",
      "import tkinter as tk\n",
      "from tkinter import filedialog\n",
      "\n",
      "def open_file():\n",
      "    \"\"\"Prompt the user to select a file to open\"\"\"\n",
      "    file_path = filedialog.askopenfilename()\n",
      "    file = open(file_path, \"r\")\n",
      "    text.insert(\"1.0\", file.read())\n",
      "    file.close()\n",
      "\n",
      "def save_file():\n",
      "    \"\"\"Prompt the user to select a location to save their file\"\"\"\n",
      "    file_path = filedialog.asksaveasfilename()\n",
      "    file = open(file_path, \"w\")\n",
      "    file.write(text.get(\"1.0\", tk.END))\n",
      "    file.close()\n",
      "\n",
      "root = tk.Tk()\n",
      "\n",
      "text = tk.Text(root)\n",
      "text.pack()\n",
      "\n",
      "menu_bar = tk.Menu(root)\n",
      "file_menu = tk.Menu(menu_bar, tearoff=0)\n",
      "file_menu.add_command(label=\"Open\", command=open_file)\n",
      "file_menu.add_command(label=\"Save\", command=save_file)\n",
      "menu_bar.add_cascade(label=\"File\", menu=file_menu)\n",
      "\n",
      "root.config(menu=menu_bar)\n",
      "root.mainloop()\n",
      "```<|end_of_text|>\n",
      "<|user|>\n",
      "Execution result: \n",
      "---------------------------------------------------------------------------\n",
      "TclError                                  Traceback (most recent call last)\n",
      "Cell In[2], line 18\n",
      "     15     file.write(text.\n",
      "[Output too much, the middle part output is omitted]\n",
      " End part of output:\n",
      "tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n",
      "   2271 if useTk:\n",
      "   2272     self._loadtk()\n",
      "\n",
      "TclError: no display name and no $DISPLAY environment variable<|end_of_text|>\n",
      "<|assistant|>\n",
      "Apologies for the confusion. It seems there is a misunderstanding about the code execution environment. The Python code for creating a GUI application using the tkinter library cannot be executed here as it requires an environment with a display for viewing the graphical interface of the application.\n",
      "\n",
      "However, you can copy the code I provided and execute it in any Python environment with a graphical interface on your local machine. Here it is:\n",
      "\n",
      "```python\n",
      "import tkinter as tk\n",
      "from tkinter import filedialog\n",
      "\n",
      "def open_file():\n",
      "    \"\"\"Prompt the user to select a file to open\"\"\"\n",
      "    file_path = filedialog.askopenfilename()\n",
      "    file = open(file_path, \"r\")\n",
      "    text.insert(\"1.0\", file.read())\n",
      "    file.close()\n",
      "\n",
      "def save_file():\n",
      "    \"\"\"Prompt the user to select a location to save their file\"\"\"\n",
      "    file_path = filedialog.asksaveasfilename()\n",
      "    file = open(file_path, \"w\")\n",
      "    file.write(text.get(\"1.0\", tk.END))\n",
      "    file.close()\n",
      "\n",
      "root = tk.Tk()\n",
      "\n",
      "text = tk.Text(root)\n",
      "text.pack()\n",
      "\n",
      "menu_bar = tk.Menu(root)\n",
      "file_menu = tk.Menu(menu_bar, tearoff=0)\n",
      "file_menu.add_command(label=\"Open\", command=open_file)\n",
      "file_menu.add_command(label=\"Save\", command=save_file)\n",
      "menu_bar.add_cascade(label=\"File\", menu=file_menu)\n",
      "\n",
      "root.config(menu=menu_bar)\n",
      "root.mainloop()\n",
      "```\n",
      "\n",
      "Create a new Python file in your local environment, paste this code into it, and run it. A window will appear, giving you options to open and save files.<|end_of_text|>\n",
      "\n",
      "Sample 3 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "You are tasked with implementing a function that processes a given set of style rules and returns a formatted CSS string. The input will be a JavaScript object representing the style rules, where each key-value pair corresponds to a CSS selector and its associated style properties. The keys are the CSS selectors, and the values are objects containing the style properties for that selector.\n",
      "\n",
      "For example, the input object might look like this:\n",
      "```javascript\n",
      "const styleRules = {\n",
      "  inputRoot: {\n",
      "\n",
      "  },\n",
      "  menuItem: {\n",
      "\n",
      "  },\n",
      "  title: {\n",
      "    display: \"inline-flex\",\n",
      "  },\n",
      "  label: {\n",
      "    position: 'relative',\n",
      "    paddingBottom: theme.spacing.unit,\n",
      "  }\n",
      "};\n",
      "```\n",
      "In this example, the `styleRules` object contains style rules for four different CSS selectors: `inputRoot`, `menuItem`, `title`, and `label`. Each selector is associated with an object containing its style properties.\n",
      "\n",
      "Your task is to implement a function `formatCSS` that takes in the `styleRules` object and returns a formatted CSS string. The CSS string should follow the standard CSS syntax, with each selector and its associated style properties properly formatted.<|end_of_text|>\n",
      "<|assistant|>\n",
      "```javascript\n",
      "function formatCSS(styleRules) {\n",
      "  let cssString = '';\n",
      "\n",
      "  for (const selector in styleRules) {\n",
      "    cssString += `${selector} {\\n`;\n",
      "    const properties = styleRules[selector];\n",
      "    for (const property in properties) {\n",
      "      cssString += `  ${property}: ${properties[property]};\\n`;\n",
      "    }\n",
      "    cssString += '}\\n\\n';\n",
      "  }\n",
      "\n",
      "  return cssString;\n",
      "}\n",
      "\n",
      "// Example usage\n",
      "const styleRules = {\n",
      "  inputRoot: {\n",
      "\n",
      "  },\n",
      "  menuItem: {\n",
      "\n",
      "  },\n",
      "  title: {\n",
      "    display: \"inline-flex\",\n",
      "  },\n",
      "  label: {\n",
      "    position: 'relative',\n",
      "    paddingBottom: '8px',\n",
      "  }\n",
      "};\n",
      "\n",
      "console.log(formatCSS(styleRules));\n",
      "```\n",
      "\n",
      "The `formatCSS` function iterates through the input `styleRules` object, constructing a formatted CSS string by concatenating the selector and its associated style properties. It properly indents the style properties and adds line breaks to follow the standard CSS syntax. The example usage demonstrates how to use the `formatCSS` function with the given `styleRules` object and logs the resulting CSS string to the console.<|end_of_text|>\n",
      "<|user|>\n",
      "You are tasked with creating a program that processes a given CSS code snippet and extracts specific CSS rules based on a provided selector. Your program should be able to handle multiple selectors and return the corresponding CSS rules for each selector.\n",
      "\n",
      "Write a function `extractCSSRules` that takes two parameters:\n",
      "1. `cssCode` (string): A string representing the CSS code snippet.\n",
      "2. `selectors` (array of strings): An array of CSS selectors for which the corresponding CSS rules need to be extracted.\n",
      "\n",
      "The function should return an object where the keys are the selectors and the values are the corresponding CSS rules. If a selector is not found in the CSS code, its value should be an empty string.\n",
      "\n",
      "For example, given the following CSS code snippet:\n",
      "```css\n",
      "input {\n",
      "  font-size: 2em;\n",
      "  width: 90%;\n",
      "}\n",
      "button {\n",
      "  background-color: #008CBA;\n",
      "  color: white;\n",
      "}\n",
      "```\n",
      "and the selectors `['input', 'button', 'textarea']`, the function should return:\n",
      "```javascript\n",
      "{\n",
      "  input: 'font-size: 2em; width: 90%;',\n",
      "  button: 'background-color: #008CBA; color: white;',\n",
      "  textarea: ''\n",
      "}\n",
      "```\n",
      "\n",
      "Ensure that the function handles cases where the CSS code contains comments, multiple selectors in a single rule, and other valid CSS syntax.<|end_of_text|>\n",
      "<|assistant|>\n",
      "```javascript\n",
      "function extractCSSRules(cssCode, selectors) {\n",
      "  const cssRules = {};\n",
      "  const regex = /([^{]+)\\s*{([^}]+)}/g;\n",
      "  let match;\n",
      "\n",
      "  while ((match = regex.exec(cssCode)) !== null) {\n",
      "    const selector = match[1].trim();\n",
      "    const rules = match[2].trim();\n",
      "    cssRules[selector] = rules;\n",
      "  }\n",
      "\n",
      "  const result = {};\n",
      "  selectors.forEach(selector => {\n",
      "    result[selector] = cssRules[selector] || '';\n",
      "  });\n",
      "\n",
      "  return result;\n",
      "}\n",
      "\n",
      "// Example usage\n",
      "const cssCode = `\n",
      "input {\n",
      "  font-size: 2em;\n",
      "  width: 90%;\n",
      "}\n",
      "button {\n",
      "  background-color: #008CBA;\n",
      "  color: white;\n",
      "}\n",
      "`;\n",
      "const selectors = ['input', 'button', 'textarea'];\n",
      "console.log(extractCSSRules(cssCode, selectors));\n",
      "```\n",
      "\n",
      "The `extractCSSRules` function uses a regular expression to match CSS rules and extract the selectors and corresponding rules. It then iterates through the provided selectors and populates the result object with the corresponding CSS rules. If a selector is not found, an empty string is assigned as the value. The example usage demonstrates how to use the function with a given CSS code snippet and selectors.<|end_of_text|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = raw_dataset[\"train\"]\n",
    "test_data = raw_dataset[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_dataset[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_dataset['train'][index]['text']}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QloRa SFT\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "device_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    #attnt_implementation=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../model_saved/llama_code_feedback-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "PyTorch: setting up devices\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:155: UserWarning: You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:185: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/model.safetensors.index.json\n",
      "Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298d8227b40f4d499802271e39449bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2627411dc042d19fe04db413ceefeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 63\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 54,525,952\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 1\n",
      "  Num examples = 65\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model_saved/llama_code_feedback-8B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =  5450195GF\n",
      "  train_loss               =     0.5522\n",
      "  train_runtime            = 0:04:01.95\n",
      "  train_samples            =        100\n",
      "  train_samples_per_second =       0.26\n",
      "  train_steps_per_second   =      0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ../model_saved/llama_code_feedback-8B/tokenizer_config.json\n",
      "Special tokens file saved in ../model_saved/llama_code_feedback-8B/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = 100\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_data))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba62edec83dc4e459a4b223a379e1003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"../../model_saved/llama_code_feedback-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# inference\\noutputs = model.generate(\\n        input_ids=input_ids,\\n        max_new_tokens=256,\\n        do_sample=True,\\n        temperature=0.7,\\n        top_k=50,\\n        top_p=0.95\\n)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a Ruby code to convert a double-linked list to a single-linked list\"},\n",
    "]\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "'''\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
