{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune a llama3 model with m-a-p/Code-Feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Let's start by installing all the lib we need to do supervised fine-tuning. We're going to use\n",
    "\n",
    "Transformers for the LLM which we're going to fine-tune\n",
    "Datasets for loading a SFT dataset from the hub, and preparing it for the model\n",
    "BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging Q-LoRa, a technique which drastically reduces the compute requirements for fine-tuning\n",
    "TRL, a library which includes useful Trainer classes for LLM fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers[torch] datasets\n",
    "#!pip install -q bitsandbytes trl peft\n",
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data + Preprocessing\n",
    "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do supervised fine-tuning (SFT), only the \"train_sft\" and \"test_sft\" splits are relevant for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea06e3c2631447b85dd9180edd7d540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb23de8645a475993869efebb193d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861144af80e143429fba897fc2f82a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/66383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 66383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"m-a-p/Code-Feedback\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT data in train/test\n",
    "indices_1 = range(0,1000)\n",
    "indices_2 = range(1001,2001)\n",
    "dataset_dict = {\n",
    "    \"train\": raw_datasets[\"train\"].select(indices_1),\n",
    "    \"test\": raw_datasets[\"train\"].select(indices_2)\n",
    "}\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': 'Write a Ruby code to convert a double-linked list to a single-linked list without using any built-in methods or data structures.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Here is a Ruby code that converts a double-linked list to a single-linked list without using any built-in methods or data structures:\\n\\n```ruby\\nclass Node\\n  attr_accessor :value, :next_node, :prev_node\\n\\n  def initialize(value, next_node = nil, prev_node = nil)\\n    @value = value\\n    @next_node = next_node\\n    @prev_node = prev_node\\n  end\\nend\\n\\ndef convert_to_single_linked_list(head)\\n  current_node = head\\n  while current_node.next_node != nil\\n    current_node.next_node.prev_node = nil\\n    current_node = current_node.next_node\\n  end\\n  current_node\\nend\\n\\n# Create a double-linked list\\nnode1 = Node.new(1)\\nnode2 = Node.new(2)\\nnode3 = Node.new(3)\\n\\nnode1.next_node = node2\\nnode2.prev_node = node1\\nnode2.next_node = node3\\nnode3.prev_node = node2\\n\\n# Convert to single-linked list\\nsingle_linked_list_head = convert_to_single_linked_list(node1)\\n\\n# Print single-linked list\\ncurrent_node = single_linked_list_head\\nwhile current_node != nil\\n  puts current_node.value\\n  current_node = current_node.next_node\\nend\\n```\\n\\nThis code defines a `Node` class representing each node in the double-linked list. The `convert_to_single_linked_list` method takes the head of the double-linked list as input and iterates through the list, removing the previous node references. It returns the new head of the single-linked list.\\n\\nIn this example, a double-linked list with nodes `1 -> 2 -> 3` is created. After conversion, the single-linked list will have nodes `1 -> 2 -> 3`, and it is printed using a loop that iterates through each node in the single-linked list.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Write a code snippet using Python to pop an element from a list, but without using the built-in `pop()` function, slicing, or any additional data structures.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Here\\'s a code snippet that removes an element from a list without using the built-in `pop()`, slicing, or any additional data structures:\\n\\n```python\\ndef pop_element(lst, index):\\n    if index < 0 or index >= len(lst):\\n        raise IndexError(\"Index out of range\")\\n    \\n    element = lst[index]\\n    \\n    for i in range(index, len(lst)-1):\\n        lst[i] = lst[i+1]\\n    \\n    lst.pop()\\n    \\n    return element\\n```\\n\\nHere\\'s how you can use this function:\\n\\n```python\\nmy_list = [1, 2, 3, 4, 5]\\nindex = 2\\n\\nremoved_element = pop_element(my_list, index)\\n\\nprint(\"Removed element:\", removed_element)\\nprint(\"Updated list:\", my_list)\\n```\\n\\nOutput:\\n```\\nRemoved element: 3\\nUpdated list: [1, 2, 4, 5]\\n```\\n\\nNote that this function modifies the original list and returns the removed element. If the index is out of range, it raises an `IndexError` exception.'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer\n",
    "Next, we instantiate the tokenizer, which is required to prepare the text for the model. The model doesn't directly take strings as input, but rather input_ids, which represent integer indices in the vocabulary of a Transformer model. \n",
    "\n",
    "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
    "\n",
    "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length.\n",
    "- the model max length: this is required in order to truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
    "- the chat template. A chat template determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as <|user|> to indicate a user message and <|assistant|> to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply chat template\n",
    "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to each list of messages. Here we basically turn each list of (instruction, completion) messages into a tokenizable string for the model.\n",
    "\n",
    "Note that we specify tokenize=False here, since the SFTTrainer which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'messages']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(raw_dataset[\"train\"].features)\n",
    "column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527c21bd1fbe409f8b4cffbbbfe93e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4a76a838d6470ca6bd4a9ab572496a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# applies the apply_chat_template function to each element in raw_dataset using multiple CPU cores, passing a tokenizer and removing specified columns, with a progress description.\n",
    "raw_dataset = raw_dataset.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 143 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Could you expound on the step-by-step processes necessary for the utilization of Amazon Kinesis in real-time data streaming within a distributed system architecture, ensuring rapid and dependable data transmission? In addition, could you contemplate the probable challenges and their respective resolutions during the execution of this system, considering factors such as scalability, data protection, and cost-effectiveness, particularly in an environment characterized by significant data volume and velocity? Could you also delve into the potential impact of these challenges on the overall system performance and the strategies to mitigate them?<|end_of_text|>\n",
      "<|assistant|>\n",
      "To utilize Amazon Kinesis for real-time data streaming in a distributed system architecture, the following steps need to be followed:\n",
      "\n",
      "1. Design the Data Stream Architecture: Identify the data sources and determine how the data will flow through the system. Determine if you need multiple streams or data partitions based on the volume and velocity of the data.\n",
      "\n",
      "2. Create an Amazon Kinesis Data Stream: Use the Amazon Kinesis console or the AWS Command Line Interface (CLI) to create a data stream. Specify the number of shards based on the expected data throughput.\n",
      "\n",
      "3. Prepare the Data Producers: Implement the data producers that will send data to the Kinesis Data Stream. This could be applications, devices, or services that generate the data.\n",
      "\n",
      "4. Send Data to the Kinesis Data Stream: Use the Kinesis API or SDKs to send data records to the data stream. Each record must have a partition key to determine the shard where it will be placed.\n",
      "\n",
      "5. Process the Data: Create one or more consumer applications that will process the data from the data stream. These applications can process the data in real-time, perform analytics, or store it in a data store.\n",
      "\n",
      "6. Monitor and Scale: Monitor the performance of the data stream and the consumer applications. Use Amazon CloudWatch to set up alarms and scale the resources as needed to handle the data volume and velocity.\n",
      "\n",
      "Now, let's discuss some of the challenges that can arise during the execution of this system and their potential resolutions:\n",
      "\n",
      "1. Scalability: As the data volume and velocity increase, it may be necessary to scale the resources dynamically. Amazon Kinesis allows you to scale up or down the number of shards based on the workload. You can use the auto-scaling feature or manually adjust the number of shards.\n",
      "\n",
      "2. Data Protection: To ensure data protection, you can enable server-side encryption for your Kinesis data stream. This encrypts the data at rest. Additionally, you can implement access control policies using AWS Identity and Access Management (IAM) to restrict access to the data stream.\n",
      "\n",
      "3. Cost-effectiveness: Handling significant data volume and velocity can incur high costs. You can optimize costs by using Kinesis Data Analytics or Kinesis Data Firehose to process and store data directly in other AWS services like Amazon S3 or Amazon Redshift. This can reduce the compute and storage costs associated with processing the data.\n",
      "\n",
      "These challenges can impact the overall system performance in various ways, such as increased latency, higher resource utilization, and potential data loss if not addressed properly. To mitigate these challenges, the following strategies can be implemented:\n",
      "\n",
      "1. Performance Optimization: Use batching and compression techniques when sending data to Kinesis to reduce the number of API calls and decrease network latency. Use efficient algorithms and processing techniques in the consumer applications to handle the data efficiently.\n",
      "\n",
      "2. Monitoring and Alerting: Implement comprehensive monitoring using Amazon CloudWatch to track the performance of the data stream, consumer applications, and other associated resources. Set up alarms to get real-time notifications when performance metrics exceed predefined thresholds.\n",
      "\n",
      "3. Resilience and Fault Tolerance: Implement fault-tolerant architectures by distributing clients and consumers across multiple Availability Zones. Design applications to handle failures and retries gracefully to minimize data loss and ensure continuity of data processing.\n",
      "\n",
      "Overall, Amazon Kinesis provides a scalable and reliable platform for real-time data streaming. By considering factors like scalability, data protection, and cost-effectiveness, and implementing appropriate strategies to mitigate challenges, you can achieve rapid and dependable data transmission within a distributed system architecture.<|end_of_text|>\n",
      "<|user|>\n",
      "No code has been generated yet. Could you please generate a code snippet for the discussed scenario?<|end_of_text|>\n",
      "<|assistant|>\n",
      "Apologies for the confusion, but creating a real-time data streaming infrastructure using Amazon Kinesis involves a combination of administrative setup via the AWS management console, configuration, and coding tasks. Most of these tasks, like creating a Kinesis Stream, configuring IAM roles, and managing shards, are typically done using the AWS console or command-line interface (CLI) which does not involve Python coding. These tasks might include sensitive details such as AWS credentials, thus is not recommended to showcase them publicly.\n",
      "\n",
      "However, I can help craft a simple Python code snippet using the Boto3 library (AWS SDK for Python) to put (send) data into a Kinesis stream and then retrieve that data. Assuming that a stream called 'my_stream' has been already created and the AWS access details have been configured.\n",
      "\n",
      "Here's a Python code to send data to the 'my_stream' Kinesis stream:\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import json\n",
      "\n",
      "def put_to_stream(data, partition_key):\n",
      "    kinesis = boto3.client('kinesis')\n",
      "    kinesis.put_record(StreamName='my_stream',\n",
      "                       Data=json.dumps(data),\n",
      "                       PartitionKey=partition_key)\n",
      "                       \n",
      "data = {\"example\": \"data\"}\n",
      "put_to_stream(data, 'partition_key')\n",
      "```\n",
      "\n",
      "And here's a code snippet to fetch the data from 'my_stream' Kinesis stream:\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import json\n",
      "import time\n",
      "\n",
      "def get_from_stream(shard_iterator):\n",
      "    kinesis = boto3.client('kinesis')\n",
      "    while True:\n",
      "        response = kinesis.get_records(ShardIterator=shard_iterator)\n",
      "        for record in response['Records']:\n",
      "            print(json.loads(record['Data']))\n",
      "        time.sleep(1)\n",
      "        shard_iterator = response['NextShardIterator']\n",
      "```\n",
      "\n",
      "You would have to first get the ShardIterator using the `get_shard_iterator` method from the boto3 Kinesis client, then pass it to the `get_from_stream` function. Note that this will continuously poll the Kinesis stream for new records every second.\n",
      "\n",
      "Keep in mind that this is a very basic example and a real-world scenario would require proper error handling and additional logic according to the specific needs of your application.<|end_of_text|>\n",
      "\n",
      "Sample 337 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Create a program that takes in an array of integers as input and returns a sorted array in ascending order. The program should have a time complexity of O(n^2) or worse. Additionally, the program should use a bubble sort algorithm to sort the array. The program should also handle duplicate integers in the input array and ensure that they appear in the sorted array according to their original order. The program should not use any built-in sorting functions or libraries. \n",
      "\n",
      "Example:\n",
      "Input: [7, 3, 5, 9, 11, 10, 7]\n",
      "Output: [3, 5, 7, 7, 9, 10, 11]<|end_of_text|>\n",
      "<|assistant|>\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n-1):\n",
      "        for j in range(n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    return arr\n",
      "\n",
      "input_array = [7, 3, 5, 9, 11, 10, 7]\n",
      "sorted_array = bubble_sort(input_array)\n",
      "print(sorted_array)<|end_of_text|>\n",
      "<|user|>\n",
      "Generate a Swift program which takes as input two arrays of integers, each array holding single-digit non-negative integers. The program should add the numbers represented by the arrays (each digit in the array is one digit of the number), and return an array of integers representing the result. Instead of using arithmetic operations +, -, *, /, or any other math library functions, the program should implement the addition manually using loops and if statements.\n",
      "\n",
      "Input 1: [5]\n",
      "Input 2: [1, 0]\n",
      "Output: [1, 5]\n",
      "\n",
      "Input 1: [3, 7, 8]\n",
      "Input 2: [2, 5, 1]\n",
      "Output: [6, 2, 9]<|end_of_text|>\n",
      "<|assistant|>\n",
      "Here is a Swift program that does exactly what you described:\n",
      "\n",
      "```swift\n",
      "func addArrays(input1: [Int], input2: [Int]) -> [Int] {\n",
      "    var maxLength = max(input1.count, input2.count)\n",
      "    var reversedRes = [Int]()\n",
      "    \n",
      "    var i1 = input1.count - 1\n",
      "    var i2 = input2.count - 1\n",
      "    \n",
      "    var carry = 0\n",
      "    while i1 >= 0 || i2 >= 0 || carry > 0 {\n",
      "        let num1 = i1 >= 0 ? input1[i1] : 0\n",
      "        let num2 = i2 >= 0 ? input2[i2] : 0\n",
      "        let sum = num1 + num2 + carry\n",
      "        reversedRes.append(sum % 10)\n",
      "        carry = sum / 10\n",
      "        i1 -= 1\n",
      "        i2 -= 1\n",
      "    }\n",
      "    \n",
      "    // reverse the result and return\n",
      "    var res = [Int]()\n",
      "    while !reversedRes.isEmpty {\n",
      "        res.append(reversedRes.removeLast())\n",
      "    }\n",
      "    \n",
      "    return res\n",
      "}\n",
      "```\n",
      "\n",
      "Please note that this code assumes that the input arrays represent the digits of the numbers in the right order, e.g., [1, 2, 3] represents the number 123. The \"carry\" variable keeps track of the carry from the previous digit addition, which starts from the least significant digit. It is important to note that the result is initially determined in reversed order due to the addition from right to left, hence reversedRes is used first. It is then reversed again to the correct order for the final result.<|end_of_text|>\n",
      "\n",
      "Sample 810 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Develop code to implement neural network in Java.<|end_of_text|>\n",
      "<|assistant|>\n",
      "Apologies for any misunderstanding, but as an AI I am capable of executing Python code only. The present environment isn't suitably designed to run Java code. \n",
      "\n",
      "If you're keen on designing a neural network in Python using libraries like TensorFlow or PyTorch, I'd be more than happy to help.<|end_of_text|>\n",
      "<|user|>\n",
      "No code has been generated yet. Could you please generate the code for the neural network?<|end_of_text|>\n",
      "<|assistant|>\n",
      "Sure, I can generate a simple feed forward Neural Network code using the Keras library in Python. This is a high-level neural networks API, written in Python and is capable of running on top of TensorFlow, CNTK, or Theano. \n",
      "\n",
      "Here is a simple implementation of feed forward Neural Network for binary classification:\n",
      "\n",
      "```python\n",
      "# Importing the necessary libraries\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "import numpy as np\n",
      "\n",
      "# set a seed for reproducibility\n",
      "np.random.seed(7)\n",
      "\n",
      "# load the dataset\n",
      "# Note: You'll need to replace this with your actual data, this is just an example\n",
      "# dataset = np.loadtxt('Your_dataset.csv', delimiter=',')\n",
      "\n",
      "# split into input (X) and output (Y) variables\n",
      "X = dataset[:,0:8]\n",
      "Y = dataset[:,8]\n",
      "\n",
      "# create the model\n",
      "model = Sequential()\n",
      "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
      "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
      "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
      "\n",
      "# compile the model\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "# fit the model to the data\n",
      "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
      "\n",
      "# evaluate the model\n",
      "scores = model.evaluate(X, Y, verbose=0)\n",
      "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
      "```\n",
      "\n",
      "This code builds a feed-forward neural network for binary classification and trains this network on some data (which needs to be defined by the user).\n",
      "\n",
      "Please substitute `'Your_dataset.csv'` with the actual dataset you would like to train your model on. \n",
      "\n",
      "For the purposes of this exercise, please remember to install the `Keras` library before you run the code. You can do it by using this command: `!pip install keras`.<|end_of_text|>\n",
      "<|user|>\n",
      "The code is functional and fairly documented, but it lacks efficiency optimizations. Consider reviewing loops or data loading mechanisms for performance improvements.<|end_of_text|>\n",
      "<|assistant|>\n",
      "Since the code is built using the Keras library, it already comes with many efficiency and performance optimizations out of the box. However, there are a few tweaks we can consider to further optimize the code:\n",
      "\n",
      "1. **Batch size and number of epochs:** We need to strike a balance between these two. Larger batch sizes speed up the learning process by allowing us to process more data at once, but it also consumes more memory and could lead to a less accurate model. Similarly, more epochs means the model is trained longer and may achieve higher accuracy, but also involves greater computational expense.\n",
      "\n",
      "2. **Normalization:** It's a good practice to normalize input data to ensure that all features have the same scale and the model trains faster.\n",
      "\n",
      "3. **Model architecture:** There can be many layers and nodes in each layer in a neural network. The optimal structure often found through a process of trial and error. An overly complex model may lead to overfitting, while an oversimplified model may not capture all the nuances of the data.\n",
      "\n",
      "4. **Early stopping**: Keras provides an \"EarlyStopping callback\" function that you can use to stop the training process if the model's performance on a validation dataset does not improve after a given number of epochs, which certainly saves computational resources.\n",
      "\n",
      "5. **Use a pre-trained model:** If you're working on a problem that's similar to one that has been solved before, using pre-trained models could save a lot of computational time. This technique is called transfer learning.\n",
      "\n",
      "6. **Parallelization:** If you have access to a GPU, you can leverage Keras's capabilities to perform computations on GPU which would be significantly faster than CPU computations.\n",
      "\n",
      "Unfortunately, without more knowledge of the specific dataset and problem context, I can't provide a more detailed optimization strategy. If you have more information, please provide so that I can give you a more specific solution.<|end_of_text|>\n",
      "<|user|>\n",
      "Consider reviewing loops or data handling for performance. Could resource usage be optimized further?<|end_of_text|>\n",
      "<|assistant|>\n",
      "Optimising Python code for certain operations can have substantial impact on speed of execution, for example:\n",
      "\n",
      "- **Looping over lists/dicts:**\n",
      "Python list/dict comprehension is faster because it is specifically optimized for the Python interpreter, using it over traditional for-loops can lead to significant speed up.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "[column for column in mat]  # Faster\n",
      "for column in mat: pass  # Slower\n",
      "```\n",
      "\n",
      "- **Choose optimal Data Structures:**\n",
      "Choosing the right data structure to handle data could optimize memory usage and speed. For example, using `sets` for membership tests is much faster than lists.\n",
      "\n",
      "But in case of Neural Network, our main operation is matrix multiplication, which is handled by the backend (TensorFlow, Theano, etc.), which is written in a low-level language that is faster over these operations than Python.\n",
      "\n",
      "Few things we can consider in case of Neural Network:\n",
      "\n",
      "- **Reducing Precision of numbers:**\n",
      "Neural networks do not require high precision calculations, reducing precision of the numbers could optimise memory usage and speed up calculations.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "model.add(Dense(12, input_shape=(8,), dtype='float32'))  # Using float32 instead of float64\n",
      "```\n",
      "\n",
      "- **Using simpler optimization algorithms:**\n",
      "Some optimization algorithms are faster but may provide good enough results in many cases. For example, `SGD` (Stochastic Gradient Descent) is much simpler and faster compared to `Adam`.\n",
      "\n",
      "Again, the code optimization heavily relies on which part of your code is the bottleneck. Python's built-in `cProfile` module is a helpful tool to measure the time each function in your code takes, which helps in figuring out which part of the code to optimize. However, for Neural Networks most of the things are abstracted out by the libraries (Keras, TensorFlow) and already optimized using low-level languages which are much faster for these kind of mathematical operations than Python.<|end_of_text|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = raw_dataset[\"train\"]\n",
    "test_data = raw_dataset[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_dataset[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_dataset['train'][index]['text']}\")\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "With regular LoRa, one would keep the base model in 32 or 16 bits in memory, and then train the parameter weights. However, there have been new methods developed to shrink the size of a model considerably, to 8 or 4 bits per parameter (we call this \"quantization\"). Hence, if we apply LoRa to a quantized model (like a 4-bit model), then we call this QLoRa. We have a blog post that tells you all about it. There are various quantization methods available, here we're going to use the BitsandBytes integration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QloRa SFT\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load the model with 4-bit precision to reduce memory usage.\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use the \"nf4\" quantization type for 4-bit precision.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Perform computations using the bfloat16 data type.\n",
    ")\n",
    "\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "device_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    #attnt_implementation=True,  # (Commented out) Use a specific attention implementation.\n",
    "    torch_dtype=\"auto\",  # Automatically choose the appropriate torch data type (e.g., float32, float16).\n",
    "    use_cache=False,  # Disable caching to save memory since gradient checkpointing is used.\n",
    "    device_map=device_map,  # Map model layers to specific devices (e.g., GPUs).\n",
    "    quantization_config=quant_config.to_dict()  # Apply model quantization settings by converting them to a dictionary.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Trainer\n",
    "\n",
    "Next, we define the SFTTrainer available in the TRL library. This class inherits from the Trainer class available in the Transformers library, but is specifically optimized for supervised fine-tuning (instruction tuning). It can be used to train out-of-the-box on one or more GPUs, using Accelerate as backend.\n",
    "\n",
    "Most notably, it supports packing, where multiple short examples are packed in the same input sequence to increase training efficiency.\n",
    "\n",
    "As we're going to use QLoRa, the PEFT library provides a handy LoraConfig which defines on which layers of the base model to apply the adapters. One typically applies LoRa on the linear projection matrices of the attention layers of a Transformer. We then provide this configuration to the SFTTrainer class. The weights of the base model will be loaded as we specify the model_id (this requires some time).\n",
    "\n",
    "We also specify various hyperparameters regarding training, such as:\n",
    "\n",
    "- we're going to fine-tune for 1 epoch\n",
    "- the learning rate and its scheduler\n",
    "- we're going to use gradient checkpointing (yet another way to save memory during training)\n",
    "- and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../model_saved/llama_code_feedback-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,  # Use 16-bit floating point precision (faster and less memory).\n",
    "    do_eval=True,  # Perform evaluation during training.\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each training epoch.\n",
    "    gradient_accumulation_steps=128,  # Accumulate gradients over 128 steps before updating weights.\n",
    "    gradient_checkpointing=True,  # Save memory by not storing intermediate activations.\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Disable reentrant autograd for gradient checkpointing.\n",
    "    learning_rate=2.0e-05,  # Set the learning rate for the optimizer.\n",
    "    log_level=\"info\",  # Set the level of detail for logging information.\n",
    "    logging_steps=5,  # Log information every 5 steps.\n",
    "    logging_strategy=\"steps\",  # Log based on steps, not epochs.\n",
    "    lr_scheduler_type=\"cosine\",  # Use a cosine schedule to adjust the learning rate.\n",
    "    max_steps=-1,  # Train indefinitely unless a stopping criterion is met.\n",
    "    num_train_epochs=1,  # Train for 1 full epoch.\n",
    "    output_dir=output_dir,  # Directory to save training outputs and checkpoints.\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists.\n",
    "    per_device_eval_batch_size=1,  # Use a batch size of 1 per device for evaluation (previously set to 8).\n",
    "    per_device_train_batch_size=1,  # Use a batch size of 1 per device for training (previously set to 8).\n",
    "    save_strategy=\"no\",  # Don't save model checkpoints during training.\n",
    "    save_total_limit=None,  # No limit on the number of checkpoints saved.\n",
    "    seed=42,  # Set the random seed for reproducibility.\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "    r=64,  # Set the rank for the LoRA (low-rank adaptation) matrices.\n",
    "    lora_alpha=16,  # Scaling factor for the LoRA updates.\n",
    "    lora_dropout=0.1,  # Apply dropout with a probability of 0.1 to LoRA layers.\n",
    "    bias=\"none\",  # No additional bias terms are added in the LoRA layers.\n",
    "    task_type=\"CAUSAL_LM\",  # Specify the task type as causal language modeling.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply LoRA to these specific model modules.\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:155: UserWarning: You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:185: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee07a8ca715e4c36aa1447d21b3a8db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method', '_load_in_8bit', '_load_in_4bit']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c678b6cf74d94f3a81720d6a33ff3524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a00dce9c7840f4bb4b6975562dc0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab978467c7847d7ad6fbd2c611ad652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d60e4e68fa44e80bad80ff2fbca13d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570abb6626ea4d5aa9810719ef7ad7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae978a4bc3794add8d113cccfdb53b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc360dd6d0e4ed9901d69b166cbac48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1308b90a1494b0db4cb87b81bd9d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2eae6202c0b4243b53f309602304874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2151 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060890ef22f6436b9e844205ff358dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model_id,  # Use the specified model ID for training.\n",
    "    model_init_kwargs=model_kwargs,  # Pass in model initialization arguments (like device mapping and dtype).\n",
    "    args=training_args,  # Apply the defined training arguments (e.g., learning rate, batch size).\n",
    "    train_dataset=train_data,  # Use the provided dataset for training.\n",
    "    eval_dataset=test_data,  # Use the provided dataset for evaluation.\n",
    "    dataset_text_field=\"text\",  # Specify the field containing text data in the datasets.\n",
    "    tokenizer=tokenizer,  # Use the specified tokenizer for preprocessing the text data.\n",
    "    packing=True,  # Enable sequence packing to maximize training efficiency.\n",
    "    peft_config=peft_config,  # Apply the specified parameter-efficient fine-tuning (PEFT) configuration.\n",
    "    max_seq_length=tokenizer.model_max_length,  # Set the maximum sequence length for the model based on the tokenizer's max length.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 661\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 5\n",
      "  Number of trainable parameters = 54,525,952\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = 100\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_data))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"../../model_saved/llama_code_feedback-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a Ruby code to convert a double-linked list to a single-linked list\"},\n",
    "]\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "\n",
    "# Prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Ensure input_ids are valid\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "# Inference with model output checks\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,  # Adjusted for stability\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "decoded_output = tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)[0]\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
