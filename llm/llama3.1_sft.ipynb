{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune a llama3 model with m-a-p/Code-Feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Let's start by installing all the lib we need to do supervised fine-tuning. We're going to use\n",
    "\n",
    "Transformers for the LLM which we're going to fine-tune\n",
    "Datasets for loading a SFT dataset from the hub, and preparing it for the model\n",
    "BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging Q-LoRa, a technique which drastically reduces the compute requirements for fine-tuning\n",
    "TRL, a library which includes useful Trainer classes for LLM fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers[torch] datasets\n",
    "#!pip install -q bitsandbytes trl peft\n",
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data + Preprocessing\n",
    "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do supervised fine-tuning (SFT), only the \"train_sft\" and \"test_sft\" splits are relevant for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 66383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"m-a-p/Code-Feedback\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT data in train/test\n",
    "indices_1 = range(0,1000)\n",
    "indices_2 = range(1001,2001)\n",
    "dataset_dict = {\n",
    "    \"train\": raw_datasets[\"train\"].select(indices_1),\n",
    "    \"test\": raw_datasets[\"train\"].select(indices_2)\n",
    "}\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# load data from s3 bucket\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region_name = 'eu-central-1'\n",
    "\n",
    "session = boto3.Session(region_name=region_name)\n",
    "s3_sess = session.client('s3')\n",
    "sm_session = sagemaker.Session(boto_session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': 's3://sagemaker-eu-central-1-505049265445/datasets/answer-gen-coherence/train_dataset.json'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_input_path = f's3://{sm_session.default_bucket()}/datasets/answer-gen-coherence/train_dataset.json'\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    training: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data\n",
    "    )\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_1 = range(0,1000)\n",
    "indices_2 = range(1001,1269)\n",
    "dataset_dict = {\n",
    "    \"train\": raw_dataset[\"training\"].select(indices_1),\n",
    "    \"test\": raw_dataset[\"training\"].select(indices_2)\n",
    "}\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "raw_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer\n",
    "Next, we instantiate the tokenizer, which is required to prepare the text for the model. The model doesn't directly take strings as input, but rather input_ids, which represent integer indices in the vocabulary of a Transformer model. \n",
    "\n",
    "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
    "\n",
    "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length.\n",
    "- the model max length: this is required in order to truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
    "- the chat template. A chat template determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as <|user|> to indicate a user message and <|assistant|> to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply chat template\n",
    "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to each list of messages. Here we basically turn each list of (instruction, completion) messages into a tokenizable string for the model.\n",
    "\n",
    "Note that we specify tokenize=False here, since the SFTTrainer which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['messages']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(raw_dataset[\"train\"].features)\n",
    "column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the apply_chat_template function to each element in raw_dataset using multiple CPU cores, passing a tokenizer and removing specified columns, with a progress description.\n",
    "raw_dataset = raw_dataset.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 199 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "You are an student learning English. You have to answer the prompt with a level of coherence from 1 to 6 based on your ability to structure and connect ideas.\n",
      "The coherence criteria range from 1 to 6, with each level representing increasing proficiency in coherence and organization.\n",
      "\n",
      "Here is the scale you should use to build your answer:\n",
      "\n",
      "1: Your answer lacks coherence: able to make very basic connections between words using basic conjunctions such as 'and'.\n",
      "\n",
      "2: Your answer has limited coherence: able to make connections between short phrases using basic conjunctions such as 'because' and 'but'.\n",
      "\n",
      "3: Your answer has moderate coherence: able to connect short phrases into a sequential series of ideas or events.\n",
      "\n",
      "4: Your answer is fairly coherent: logically structures and organizes utterances and connects ideas using a limited number of discourse markers and conjunctions. A lack of organization or logical connection may be evident in longer responses.\n",
      "\n",
      "5: Your answer has good coherence: logically structures and organizes utterances and connects ideas using appropriate discourse markers and conjunctions.\n",
      "\n",
      "6: Your answer has advanced coherence: logically structures and organizes utterances and connects ideas using a wide variety of discourse markers and conjunctions. Evidence of complex organizational patterns is demonstrated.\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Answer this prompt: Describe the differences between people your age and the older generation. Why do you think these differences exist? with a level of coherence: 1.0<|end_of_text|>\n",
      "<|assistant|>\n",
      " This is a problem for all generation because two generation are very different and is different.<|end_of_text|>\n",
      "\n",
      "Sample 951 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "You are an student learning English. You have to answer the prompt with a level of coherence from 1 to 6 based on your ability to structure and connect ideas.\n",
      "The coherence criteria range from 1 to 6, with each level representing increasing proficiency in coherence and organization.\n",
      "\n",
      "Here is the scale you should use to build your answer:\n",
      "\n",
      "1: Your answer lacks coherence: able to make very basic connections between words using basic conjunctions such as 'and'.\n",
      "\n",
      "2: Your answer has limited coherence: able to make connections between short phrases using basic conjunctions such as 'because' and 'but'.\n",
      "\n",
      "3: Your answer has moderate coherence: able to connect short phrases into a sequential series of ideas or events.\n",
      "\n",
      "4: Your answer is fairly coherent: logically structures and organizes utterances and connects ideas using a limited number of discourse markers and conjunctions. A lack of organization or logical connection may be evident in longer responses.\n",
      "\n",
      "5: Your answer has good coherence: logically structures and organizes utterances and connects ideas using appropriate discourse markers and conjunctions.\n",
      "\n",
      "6: Your answer has advanced coherence: logically structures and organizes utterances and connects ideas using a wide variety of discourse markers and conjunctions. Evidence of complex organizational patterns is demonstrated.\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Answer this prompt: In what ways has technology impacted your life, both positively and negatively? with a level of coherence: 1.0<|end_of_text|>\n",
      "<|assistant|>\n",
      " Positively, I think my work is very technically negative. Maybe we're not that social anymore. We are on our phones.<|end_of_text|>\n",
      "\n",
      "Sample 738 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "You are an student learning English. You have to answer the prompt with a level of coherence from 1 to 6 based on your ability to structure and connect ideas.\n",
      "The coherence criteria range from 1 to 6, with each level representing increasing proficiency in coherence and organization.\n",
      "\n",
      "Here is the scale you should use to build your answer:\n",
      "\n",
      "1: Your answer lacks coherence: able to make very basic connections between words using basic conjunctions such as 'and'.\n",
      "\n",
      "2: Your answer has limited coherence: able to make connections between short phrases using basic conjunctions such as 'because' and 'but'.\n",
      "\n",
      "3: Your answer has moderate coherence: able to connect short phrases into a sequential series of ideas or events.\n",
      "\n",
      "4: Your answer is fairly coherent: logically structures and organizes utterances and connects ideas using a limited number of discourse markers and conjunctions. A lack of organization or logical connection may be evident in longer responses.\n",
      "\n",
      "5: Your answer has good coherence: logically structures and organizes utterances and connects ideas using appropriate discourse markers and conjunctions.\n",
      "\n",
      "6: Your answer has advanced coherence: logically structures and organizes utterances and connects ideas using a wide variety of discourse markers and conjunctions. Evidence of complex organizational patterns is demonstrated.\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Answer this prompt: When is your birthday? with a level of coherence: 1.0<|end_of_text|>\n",
      "<|assistant|>\n",
      " My best is in the 8th March 1992. I was born in Freudian and I.<|end_of_text|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = raw_dataset[\"train\"]\n",
    "test_data = raw_dataset[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_dataset[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_dataset['train'][index]['text']}\")\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "With regular LoRa, one would keep the base model in 32 or 16 bits in memory, and then train the parameter weights. However, there have been new methods developed to shrink the size of a model considerably, to 8 or 4 bits per parameter (we call this \"quantization\"). Hence, if we apply LoRa to a quantized model (like a 4-bit model), then we call this QLoRa. We have a blog post that tells you all about it. There are various quantization methods available, here we're going to use the BitsandBytes integration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QloRa SFT\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load the model with 4-bit precision to reduce memory usage.\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use the \"nf4\" quantization type for 4-bit precision.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Perform computations using the bfloat16 data type.\n",
    ")\n",
    "\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "device_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    #attnt_implementation=True,  # (Commented out) Use a specific attention implementation.\n",
    "    torch_dtype=\"auto\",  # Automatically choose the appropriate torch data type (e.g., float32, float16).\n",
    "    use_cache=False,  # Disable caching to save memory since gradient checkpointing is used.\n",
    "    device_map=device_map,  # Map model layers to specific devices (e.g., GPUs).\n",
    "    quantization_config=quant_config.to_dict()  # Apply model quantization settings by converting them to a dictionary.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Trainer\n",
    "\n",
    "Next, we define the SFTTrainer available in the TRL library. This class inherits from the Trainer class available in the Transformers library, but is specifically optimized for supervised fine-tuning (instruction tuning). It can be used to train out-of-the-box on one or more GPUs, using Accelerate as backend.\n",
    "\n",
    "Most notably, it supports packing, where multiple short examples are packed in the same input sequence to increase training efficiency.\n",
    "\n",
    "As we're going to use QLoRa, the PEFT library provides a handy LoraConfig which defines on which layers of the base model to apply the adapters. One typically applies LoRa on the linear projection matrices of the attention layers of a Transformer. We then provide this configuration to the SFTTrainer class. The weights of the base model will be loaded as we specify the model_id (this requires some time).\n",
    "\n",
    "We also specify various hyperparameters regarding training, such as:\n",
    "\n",
    "- we're going to fine-tune for 1 epoch\n",
    "- the learning rate and its scheduler\n",
    "- we're going to use gradient checkpointing (yet another way to save memory during training)\n",
    "- and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../model_saved/llama_coherence_answer-gen-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,  # Use 16-bit floating point precision (faster and less memory).\n",
    "    do_eval=True,  # Perform evaluation during training.\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each training epoch.\n",
    "    gradient_accumulation_steps=128,  # Accumulate gradients over 128 steps before updating weights.\n",
    "    gradient_checkpointing=True,  # Save memory by not storing intermediate activations.\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Disable reentrant autograd for gradient checkpointing.\n",
    "    learning_rate=2.0e-05,  # Set the learning rate for the optimizer.\n",
    "    log_level=\"info\",  # Set the level of detail for logging information.\n",
    "    logging_steps=5,  # Log information every 5 steps.\n",
    "    logging_strategy=\"steps\",  # Log based on steps, not epochs.\n",
    "    lr_scheduler_type=\"cosine\",  # Use a cosine schedule to adjust the learning rate.\n",
    "    max_steps=-1,  # Train indefinitely unless a stopping criterion is met.\n",
    "    num_train_epochs=1,  # Train for 1 full epoch.\n",
    "    output_dir=output_dir,  # Directory to save training outputs and checkpoints.\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists.\n",
    "    per_device_eval_batch_size=1,  # Use a batch size of 1 per device for evaluation (previously set to 8).\n",
    "    per_device_train_batch_size=1,  # Use a batch size of 1 per device for training (previously set to 8).\n",
    "    save_strategy=\"no\",  # Don't save model checkpoints during training.\n",
    "    save_total_limit=None,  # No limit on the number of checkpoints saved.\n",
    "    seed=42,  # Set the random seed for reproducibility.\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "    r=64,  # Set the rank for the LoRA (low-rank adaptation) matrices.\n",
    "    lora_alpha=16,  # Scaling factor for the LoRA updates.\n",
    "    lora_dropout=0.1,  # Apply dropout with a probability of 0.1 to LoRA layers.\n",
    "    bias=\"none\",  # No additional bias terms are added in the LoRA layers.\n",
    "    task_type=\"CAUSAL_LM\",  # Specify the task type as causal language modeling.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Apply LoRA to these specific model modules.\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:155: UserWarning: You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:185: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "Unused kwargs: ['quant_method', '_load_in_8bit', '_load_in_4bit']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113dbefebbac43aa8e2416eb7776aec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cccd003e66400eb6e0764ac0d415b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8e41cabebb4cd78f01e1b1b7a4d6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model_id,  # Use the specified model ID for training.\n",
    "    model_init_kwargs=model_kwargs,  # Pass in model initialization arguments (like device mapping and dtype).\n",
    "    args=training_args,  # Apply the defined training arguments (e.g., learning rate, batch size).\n",
    "    train_dataset=train_data,  # Use the provided dataset for training.\n",
    "    eval_dataset=test_data,  # Use the provided dataset for evaluation.\n",
    "    dataset_text_field=\"text\",  # Specify the field containing text data in the datasets.\n",
    "    tokenizer=tokenizer,  # Use the specified tokenizer for preprocessing the text data.\n",
    "    packing=True,  # Enable sequence packing to maximize training efficiency.\n",
    "    peft_config=peft_config,  # Apply the specified parameter-efficient fine-tuning (PEFT) configuration.\n",
    "    max_seq_length=tokenizer.model_max_length,  # Set the maximum sequence length for the model based on the tokenizer's max length.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 168\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 54,525,952\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.76/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 45\n",
      "  Batch size = 1\n",
      "  Num examples = 45\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model_saved/llama_coherence_answer-gen-8B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     0.7619\n",
      "  total_flos               = 11073414GF\n",
      "  train_loss               =     0.9628\n",
      "  train_runtime            = 0:06:46.53\n",
      "  train_samples            =        100\n",
      "  train_samples_per_second =      0.413\n",
      "  train_steps_per_second   =      0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ../../model_saved/llama_coherence_answer-gen-8B/tokenizer_config.json\n",
      "Special tokens file saved in ../../model_saved/llama_coherence_answer-gen-8B/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = 100\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_data))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file model.safetensors from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef1f64280934c2389be782ee612ddc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"../../model_saved/llama_coherence_answer-gen-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[   198,  16533,    264,  10137,   1093,   3639,   1550,    499,    656,\n",
      "           2391,    701,   1566,  20769,    449,    264,   2237,    315,  78925,\n",
      "            220,     17,     13, 128001,    271]], device='cuda:0')\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2.\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "\n",
      "Answer a prompt like What did you do during your last vacation with a level of coherence 2. respond\n",
      "Answer a prompt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Answer a prompt like What did you do during your last vacation with a level of coherence 2.\"},\n",
    "]\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "\n",
    "# Prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Ensure input_ids are valid\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "# Inference with model output checks\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,  # Adjusted for stability\n",
    "        top_k=2,\n",
    "        top_p=0.9,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "decoded_output = tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)[0]\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
