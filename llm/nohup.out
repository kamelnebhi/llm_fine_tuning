sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml
sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause
severe performance issues, see also https://github.com/dask/dask/issues/10276

To fix, you should specify a lower version bound on s3fs, or
update the current installation.

  warnings.warn(s3_msg)
Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]Creating json from Arrow format:  43%|████▎     | 3/7 [00:00<00:00, 13.18ba/s]Creating json from Arrow format:  86%|████████▌ | 6/7 [00:00<00:00, 16.11ba/s]Creating json from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 17.43ba/s]
Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 146.20ba/s]
Training data uploaded to:
s3://sagemaker-eu-central-1-505049265445/datasets/llama3-writing_acc-instruct/train/dataset.json
s3://sagemaker-eu-central-1-505049265445/datasets/llama3-writing_acc-instruct/test/dataset.json
https://s3.console.aws.amazon.com/s3/buckets/sagemaker-eu-central-1-505049265445/?region=eu-central-1&prefix=datasets/llama3-writing_acc-instruct/
Training config uploaded to:
s3://sagemaker-eu-central-1-505049265445/datasets/llama3-writing_acc-instruct/config/llama_3_8b_instruct_fsdp_qlora.yaml
INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.
INFO:sagemaker:Creating training-job with name: llama3-instruct-8b-writing-acc-exp1-2024-09-07-18-34-22-607
2024-09-07 18:34:26 Starting - Starting the training job
2024-09-07 18:34:26 Pending - Training job waiting for capacity......
2024-09-07 18:34:59 Pending - Preparing the instances for training...
2024-09-07 18:35:43 Downloading - Downloading the training image..................
2024-09-07 18:38:55 Training - Training image download completed. Training in progress......bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
2024-09-07 18:39:39,917 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training
2024-09-07 18:39:39,952 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)
2024-09-07 18:39:39,963 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.
2024-09-07 18:39:39,964 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...
2024-09-07 18:39:39,964 sagemaker_pytorch_container.training INFO     Invoking user training script.
2024-09-07 18:39:41,442 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:
/opt/conda/bin/python3.10 -m pip install -r requirements.txt
Collecting transformers==4.40.0 (from -r requirements.txt (line 1))
Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)
Requirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.18.0)
Collecting accelerate==0.29.3 (from -r requirements.txt (line 3))
Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)
Requirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)
Collecting bitsandbytes==0.43.1 (from -r requirements.txt (line 5))
Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)
Collecting huggingface_hub==0.22.2 (from -r requirements.txt (line 6))
Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)
Collecting trl==0.8.6 (from -r requirements.txt (line 7))
Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)
Collecting peft==0.10.0 (from -r requirements.txt (line 8))
Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (3.13.1)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2024.7.24)
Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2.32.3)
Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0->-r requirements.txt (line 1))
Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (0.4.4)
Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (4.66.4)
Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (17.0.0)
Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.8)
Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.2.1)
Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)
Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.16)
Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2024.2.0)
Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.10.1)
Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 3)) (5.9.8)
Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 3)) (2.1.0)
Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2->-r requirements.txt (line 6)) (4.10.0)
Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.6->-r requirements.txt (line 7)) (0.8.5)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (2.3.5)
Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.5)
Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)
Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.2.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (1.26.19)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (2024.7.4)
Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (1.12)
Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (3.2.1)
Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (3.1.4)
Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.16)
Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (13.7.1)
Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (1.7.1)
Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2024.1)
Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (2.17.2)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (2.1.5)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (1.3.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.1.2)
Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/9.0 MB 130.7 MB/s eta 0:00:00
Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)
Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 101.6 MB/s eta 0:00:00
Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)
Downloading trl-0.8.6-py3-none-any.whl (245 kB)
Downloading peft-0.10.0-py3-none-any.whl (199 kB)
Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 123.3 MB/s eta 0:00:00
Installing collected packages: huggingface_hub, tokenizers, bitsandbytes, accelerate, transformers, trl, peft
Attempting uninstall: huggingface_hub
Found existing installation: huggingface-hub 0.24.5
Uninstalling huggingface-hub-0.24.5:
Successfully uninstalled huggingface-hub-0.24.5
Attempting uninstall: tokenizers
Found existing installation: tokenizers 0.15.2
Uninstalling tokenizers-0.15.2:
Successfully uninstalled tokenizers-0.15.2
Attempting uninstall: bitsandbytes
Found existing installation: bitsandbytes 0.43.3
Uninstalling bitsandbytes-0.43.3:
Successfully uninstalled bitsandbytes-0.43.3
Attempting uninstall: accelerate
Found existing installation: accelerate 0.25.0
Uninstalling accelerate-0.25.0:
Successfully uninstalled accelerate-0.25.0
Attempting uninstall: transformers
Found existing installation: transformers 4.36.0
Uninstalling transformers-4.36.0:
Successfully uninstalled transformers-4.36.0
Attempting uninstall: trl
Found existing installation: trl 0.7.4
Uninstalling trl-0.7.4:
Successfully uninstalled trl-0.7.4
Attempting uninstall: peft
Found existing installation: peft 0.7.1
Uninstalling peft-0.7.1:
Successfully uninstalled peft-0.7.1
Successfully installed accelerate-0.29.3 bitsandbytes-0.43.1 huggingface_hub-0.22.2 peft-0.10.0 tokenizers-0.19.1 transformers-4.40.0 trl-0.8.6
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
2024-09-07 18:39:53,445 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.
2024-09-07 18:39:53,445 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.
2024-09-07 18:39:53,497 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)
2024-09-07 18:39:53,544 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)
2024-09-07 18:39:53,556 sagemaker-training-toolkit INFO     Starting distributed training through torchrun
2024-09-07 18:39:53,591 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)
2024-09-07 18:39:53,603 sagemaker-training-toolkit INFO     Invoking user script
Training Env:
{
    "additional_framework_parameters": {
        "sagemaker_instance_type": "ml.g5.12xlarge",
        "sagemaker_torch_distributed_enabled": true
    },
    "channel_input_dirs": {
        "config": "/opt/ml/input/data/config",
        "test": "/opt/ml/input/data/test",
        "train": "/opt/ml/input/data/train"
    },
    "current_host": "algo-1",
    "current_instance_group": "homogeneousCluster",
    "current_instance_group_hosts": [
        "algo-1"
    ],
    "current_instance_type": "ml.g5.12xlarge",
    "distribution_hosts": [
        "algo-1"
    ],
    "distribution_instance_groups": [
        "homogeneousCluster"
    ],
    "framework_module": "sagemaker_pytorch_container.training:main",
    "hosts": [
        "algo-1"
    ],
    "hyperparameters": {
        "config": "/opt/ml/input/data/config/llama_3_8b_instruct_fsdp_qlora.yaml"
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {
        "config": {
            "TrainingInputMode": "File",
            "S3DistributionType": "FullyReplicated",
            "RecordWrapperType": "None"
        },
        "test": {
            "TrainingInputMode": "File",
            "S3DistributionType": "FullyReplicated",
            "RecordWrapperType": "None"
        },
        "train": {
            "TrainingInputMode": "File",
            "S3DistributionType": "FullyReplicated",
            "RecordWrapperType": "None"
        }
    },
    "input_dir": "/opt/ml/input",
    "instance_groups": [
        "homogeneousCluster"
    ],
    "instance_groups_dict": {
        "homogeneousCluster": {
            "instance_group_name": "homogeneousCluster",
            "instance_type": "ml.g5.12xlarge",
            "hosts": [
                "algo-1"
            ]
        }
    },
    "is_hetero": false,
    "is_master": true,
    "is_modelparallel_enabled": null,
    "is_smddpmprun_installed": false,
    "is_smddprun_installed": true,
    "job_name": "llama3-instruct-8b-writing-acc-exp1-2024-09-07-18-34-22-607",
    "log_level": 20,
    "master_hostname": "algo-1",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://sagemaker-eu-central-1-505049265445/llama3-instruct-8b-writing-acc-exp1-2024-09-07-18-34-22-607/source/sourcedir.tar.gz",
    "module_name": "run_fsdp_qlora",
    "network_interface_name": "eth0",
    "num_cpus": 48,
    "num_gpus": 4,
    "num_neurons": 0,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1",
        "current_instance_type": "ml.g5.12xlarge",
        "current_group_name": "homogeneousCluster",
        "hosts": [
            "algo-1"
        ],
        "instance_groups": [
            {
                "instance_group_name": "homogeneousCluster",
                "instance_type": "ml.g5.12xlarge",
                "hosts": [
                    "algo-1"
                ]
            }
        ],
        "network_interface_name": "eth0"
    },
    "user_entry_point": "run_fsdp_qlora.py"
}
Environment variables:
SM_HOSTS=["algo-1"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"config":"/opt/ml/input/data/config/llama_3_8b_instruct_fsdp_qlora.yaml"}
SM_USER_ENTRY_POINT=run_fsdp_qlora.py
SM_FRAMEWORK_PARAMS={"sagemaker_instance_type":"ml.g5.12xlarge","sagemaker_torch_distributed_enabled":true}
SM_RESOURCE_CONFIG={"current_group_name":"homogeneousCluster","current_host":"algo-1","current_instance_type":"ml.g5.12xlarge","hosts":["algo-1"],"instance_groups":[{"hosts":["algo-1"],"instance_group_name":"homogeneousCluster","instance_type":"ml.g5.12xlarge"}],"network_interface_name":"eth0"}
SM_INPUT_DATA_CONFIG={"config":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"},"test":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"},"train":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=["config","test","train"]
SM_CURRENT_HOST=algo-1
SM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge
SM_CURRENT_INSTANCE_GROUP=homogeneousCluster
SM_CURRENT_INSTANCE_GROUP_HOSTS=["algo-1"]
SM_INSTANCE_GROUPS=["homogeneousCluster"]
SM_INSTANCE_GROUPS_DICT={"homogeneousCluster":{"hosts":["algo-1"],"instance_group_name":"homogeneousCluster","instance_type":"ml.g5.12xlarge"}}
SM_DISTRIBUTION_INSTANCE_GROUPS=["homogeneousCluster"]
SM_IS_HETERO=false
SM_MODULE_NAME=run_fsdp_qlora
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=48
SM_NUM_GPUS=4
SM_NUM_NEURONS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-eu-central-1-505049265445/llama3-instruct-8b-writing-acc-exp1-2024-09-07-18-34-22-607/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_instance_type":"ml.g5.12xlarge","sagemaker_torch_distributed_enabled":true},"channel_input_dirs":{"config":"/opt/ml/input/data/config","test":"/opt/ml/input/data/test","train":"/opt/ml/input/data/train"},"current_host":"algo-1","current_instance_group":"homogeneousCluster","current_instance_group_hosts":["algo-1"],"current_instance_type":"ml.g5.12xlarge","distribution_hosts":["algo-1"],"distribution_instance_groups":["homogeneousCluster"],"framework_module":"sagemaker_pytorch_container.training:main","hosts":["algo-1"],"hyperparameters":{"config":"/opt/ml/input/data/config/llama_3_8b_instruct_fsdp_qlora.yaml"},"input_config_dir":"/opt/ml/input/config","input_data_config":{"config":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"},"test":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"},"train":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","instance_groups":["homogeneousCluster"],"instance_groups_dict":{"homogeneousCluster":{"hosts":["algo-1"],"instance_group_name":"homogeneousCluster","instance_type":"ml.g5.12xlarge"}},"is_hetero":false,"is_master":true,"is_modelparallel_enabled":null,"is_smddpmprun_installed":false,"is_smddprun_installed":true,"job_name":"llama3-instruct-8b-writing-acc-exp1-2024-09-07-18-34-22-607","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-eu-central-1-505049265445/llama3-instruct-8b-writing-acc-exp1-2024-09-07-18-34-22-607/source/sourcedir.tar.gz","module_name":"run_fsdp_qlora","network_interface_name":"eth0","num_cpus":48,"num_gpus":4,"num_neurons":0,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_group_name":"homogeneousCluster","current_host":"algo-1","current_instance_type":"ml.g5.12xlarge","hosts":["algo-1"],"instance_groups":[{"hosts":["algo-1"],"instance_group_name":"homogeneousCluster","instance_type":"ml.g5.12xlarge"}],"network_interface_name":"eth0"},"user_entry_point":"run_fsdp_qlora.py"}
SM_USER_ARGS=["--config","/opt/ml/input/data/config/llama_3_8b_instruct_fsdp_qlora.yaml"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_CHANNEL_CONFIG=/opt/ml/input/data/config
SM_CHANNEL_TEST=/opt/ml/input/data/test
SM_CHANNEL_TRAIN=/opt/ml/input/data/train
SM_HP_CONFIG=/opt/ml/input/data/config/llama_3_8b_instruct_fsdp_qlora.yaml
PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages
Invoking script with the following command:
torchrun --nnodes 1 --nproc_per_node 4 run_fsdp_qlora.py --config /opt/ml/input/data/config/llama_3_8b_instruct_fsdp_qlora.yaml
[2024-09-07 18:39:54,956] torch.distributed.run: [WARNING] 
[2024-09-07 18:39:54,956] torch.distributed.run: [WARNING] *****************************************
[2024-09-07 18:39:54,956] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-07 18:39:54,956] torch.distributed.run: [WARNING] *****************************************
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]
0it [00:00, ?it/s]
Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 7000 examples [00:00, 48785.32 examples/s]
Generating train split: 7000 examples [00:00, 48685.17 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 526 examples [00:00, 52408.87 examples/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map:   0%|          | 0/7000 [00:00<?, ? examples/s]
Map:   0%|          | 0/7000 [00:00<?, ? examples/s]
Map:   0%|          | 0/7000 [00:00<?, ? examples/s]
Map:   0%|          | 0/7000 [00:00<?, ? examples/s]
Map:  10%|█         | 707/7000 [00:00<00:00, 7020.48 examples/s]
Map:  10%|▉         | 689/7000 [00:00<00:00, 6836.72 examples/s]
Map:  10%|▉         | 685/7000 [00:00<00:00, 6789.79 examples/s]
Map:  10%|▉         | 690/7000 [00:00<00:00, 6847.81 examples/s]
Map:  23%|██▎       | 1587/7000 [00:00<00:00, 8059.69 examples/s]
Map:  22%|██▏       | 1555/7000 [00:00<00:00, 7900.03 examples/s]
Map:  22%|██▏       | 1530/7000 [00:00<00:00, 7758.01 examples/s]
Map:  22%|██▏       | 1548/7000 [00:00<00:00, 7860.80 examples/s]
Map:  36%|███▌      | 2504/7000 [00:00<00:00, 8561.11 examples/s]
Map:  35%|███▌      | 2464/7000 [00:00<00:00, 8345.33 examples/s]
Map:  34%|███▍      | 2403/7000 [00:00<00:00, 8194.22 examples/s]
Map:  35%|███▌      | 2461/7000 [00:00<00:00, 8293.99 examples/s]
Map:  49%|████▉     | 3422/7000 [00:00<00:00, 8802.99 examples/s]
Map:  48%|████▊     | 3358/7000 [00:00<00:00, 8575.08 examples/s]
Map:  47%|████▋     | 3272/7000 [00:00<00:00, 8383.87 examples/s]
Map:  48%|████▊     | 3342/7000 [00:00<00:00, 8493.73 examples/s]
Map:  62%|██████▏   | 4348/7000 [00:00<00:00, 8964.23 examples/s]
Map:  61%|██████    | 4240/7000 [00:00<00:00, 8660.26 examples/s]
Map:  59%|█████▉    | 4142/7000 [00:00<00:00, 8492.50 examples/s]
Map:  60%|██████    | 4209/7000 [00:00<00:00, 8553.66 examples/s]
Map:  75%|███████▌  | 5275/7000 [00:00<00:00, 9063.27 examples/s]
Map:  73%|███████▎  | 5142/7000 [00:00<00:00, 8780.17 examples/s]
Map:  72%|███████▏  | 5035/7000 [00:00<00:00, 8637.81 examples/s]
Map:  73%|███████▎  | 5101/7000 [00:00<00:00, 8674.32 examples/s]
Map:  89%|████████▊ | 6203/7000 [00:00<00:00, 9130.64 examples/s]
Map:  86%|████████▋ | 6053/7000 [00:00<00:00, 8883.30 examples/s]
Map:  85%|████████▌ | 5958/7000 [00:00<00:00, 8826.93 examples/s]
Map:  86%|████████▌ | 6000/7000 [00:00<00:00, 8758.92 examples/s]
Map: 100%|██████████| 7000/7000 [00:00<00:00, 8874.59 examples/s]
Map: 100%|█████████▉| 6983/7000 [00:00<00:00, 9013.79 examples/s]
Map: 100%|██████████| 7000/7000 [00:00<00:00, 8644.87 examples/s]
Map:  98%|█████████▊| 6854/7000 [00:00<00:00, 8867.47 examples/s]
Map: 100%|██████████| 7000/7000 [00:00<00:00, 8520.52 examples/s]
Map:  99%|█████████▉| 6916/7000 [00:00<00:00, 8882.15 examples/s]
Map: 100%|██████████| 7000/7000 [00:00<00:00, 8545.89 examples/s]
Map:   0%|          | 0/526 [00:00<?, ? examples/s]
Map:   0%|          | 0/526 [00:00<?, ? examples/s]
Map: 100%|██████████| 526/526 [00:00<00:00, 8804.60 examples/s]
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
As an English student, your task is to answer a prompt with a level of grammatical accuracy ranging from 1 to 6. 
Each level reflects increasing proficiency in using correct grammatical structures and sentence patterns in English as your second language.
Here is the scale you should use to build your answer:
1: The student_answer has frequent and noticeable Errors: The student makes frequent and noticeable errors in grammar, significantly impeding intelligibility.
2: The student_answer has basic Control with consistent errors: The student demonstrates control over simple grammatical structures but makes consistent basic errors that often impede intelligibility.
3: The student_answer has control with some errors: The student demonstrates control over common grammatical structures, although occasional errors impede intelligibility.
4: The student_answer has fair control with infrequent errors: The student demonstrates control over a variety of grammatical structures, with noticeable errors made infrequently. These errors do not significantly impede intelligibility, and there may be some self-correction.
5: The student_answer has high control with rare errors: The student demonstrates a high level of control over a variety of grammatical structures, with errors rarely made. However, there might be noticeable errors that occasionally impede intelligibility, and self-correction may be inconsistent.
6: The student_answer has advanced control with rare errors: The student demonstrates advanced control over a variety of grammatical structures, even when engaged in other activities. Errors are rarely noticeable, and there is consistent evidence of self-correction.<|eot_id|><|start_header_id|>user<|end_header_id|>
Answer the following prompt: 'You are participating in an audience survey. Give feedback on some of the shows you have watched recently.' with a level of accuracy: '4.0'<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Recently i have been watching a show called Modern Family. Its about 3 different families and how their lives are interconnected. We get to see people who are from very different background come together and ceeate a wholesome and harmonious environment tigether. Its a hilarious comedy show where each episode creates a sense of homely experience<|eot_id|>
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
As an English student, your task is to answer a prompt with a level of grammatical accuracy ranging from 1 to 6. 
Each level reflects increasing proficiency in using correct grammatical structures and sentence patterns in English as your second language.
Here is the scale you should use to build your answer:
1: The student_answer has frequent and noticeable Errors: The student makes frequent and noticeable errors in grammar, significantly impeding intelligibility.
2: The student_answer has basic Control with consistent errors: The student demonstrates control over simple grammatical structures but makes consistent basic errors that often impede intelligibility.
3: The student_answer has control with some errors: The student demonstrates control over common grammatical structures, although occasional errors impede intelligibility.
4: The student_answer has fair control with infrequent errors: The student demonstrates control over a variety of grammatical structures, with noticeable errors made infrequently. These errors do not significantly impede intelligibility, and there may be some self-correction.
5: The student_answer has high control with rare errors: The student demonstrates a high level of control over a variety of grammatical structures, with errors rarely made. However, there might be noticeable errors that occasionally impede intelligibility, and self-correction may be inconsistent.
6: The student_answer has advanced control with rare errors: The student demonstrates advanced control over a variety of grammatical structures, even when engaged in other activities. Errors are rarely noticeable, and there is consistent evidence of self-correction.<|eot_id|><|start_header_id|>user<|end_header_id|>
Answer the following prompt: 'You're renting out a room. Write a detailed description of it for a room rental website.' with a level of accuracy: '3.0'<|eot_id|><|start_header_id|>assistant<|end_header_id|>
Welcome you dear customer! I have an four BHK appartment next to the Gustave which has Effile Tower. Yes, I know it's sound intresting appartment near the Effile Tower. I decided to rent one BHK of my apartment, Any one intrested please contact me the above mentioned phone number to discuse about further details. Thank you!<|eot_id|>
Map: 100%|██████████| 526/526 [00:00<00:00, 7875.87 examples/s]
NCCL version 2.18.5+cuda12.1
algo-1:66:161 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol
algo-1:65:159 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol
algo-1:67:160 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol
algo-1:64:158 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards:  25%|██▌       | 1/4 [00:13<00:41, 13.80s/it]
Downloading shards:  25%|██▌       | 1/4 [00:13<00:41, 13.80s/it]
Downloading shards:  25%|██▌       | 1/4 [00:13<00:41, 13.81s/it]
Downloading shards:  25%|██▌       | 1/4 [00:13<00:41, 13.80s/it]
Downloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.91s/it]
Downloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.90s/it]
Downloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.92s/it]
Downloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.93s/it]
Downloading shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.89s/it]
Downloading shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.90s/it]
Downloading shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.89s/it]
Downloading shards:  75%|███████▌  | 3/4 [00:43<00:14, 14.90s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 10.30s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 11.78s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 10.29s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 11.78s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 10.29s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 11.78s/it]
Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 10.30s/it]#015Downloading shards: 100%|██████████| 4/4 [00:47<00:00, 11.77s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.07s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.09s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.13s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.10s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.11s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.71s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.69s/it]
Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 1 examples [00:00,  3.51 examples/s]
Generating train split: 617 examples [00:00, 2057.57 examples/s]
Generating train split: 1000 examples [00:00, 1465.05 examples/s]
Generating train split: 1335 examples [00:00, 1391.25 examples/s]
Generating train split: 1993 examples [00:01, 2324.37 examples/s]
Generating train split: 2658 examples [00:01, 2086.37 examples/s]
Generating train split: 2947 examples [00:01, 1797.60 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 221 examples [00:00, 2124.75 examples/s]
Generating train split: 221 examples [00:00, 1937.79 examples/s]
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562
0%|          | 0/230 [00:00<?, ?it/s]
0%|          | 1/230 [00:27<1:46:06, 27.80s/it]
1%|          | 2/230 [00:50<1:33:31, 24.61s/it]
1%|▏         | 3/230 [01:12<1:29:18, 23.60s/it]
2%|▏         | 4/230 [01:34<1:27:07, 23.13s/it]
2%|▏         | 5/230 [01:57<1:25:43, 22.86s/it]
3%|▎         | 6/230 [02:19<1:24:45, 22.70s/it]
3%|▎         | 7/230 [02:42<1:23:56, 22.58s/it]
3%|▎         | 8/230 [03:04<1:23:29, 22.57s/it]
4%|▍         | 9/230 [03:27<1:23:04, 22.55s/it]
4%|▍         | 10/230 [03:49<1:22:31, 22.51s/it]
{'loss': 1.0055, 'grad_norm': 0.244140625, 'learning_rate': 0.0002, 'epoch': 0.22}
4%|▍         | 10/230 [03:49<1:22:31, 22.51s/it]
5%|▍         | 11/230 [04:11<1:22:01, 22.47s/it]
