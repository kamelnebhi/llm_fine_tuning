{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tune a llama3.1 with m-a-p/Code-Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers[torch] datasets\n",
    "#!pip install -q bitsandbytes trl peft\n",
    "#!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 66383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"m-a-p/Code-Feedback\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT data in train/test\n",
    "\n",
    "indices_1 = range(0,1000)\n",
    "indices_2 = range(1001,2001)\n",
    "dataset_dict = {\n",
    "    \"train\": raw_datasets[\"train\"].select(indices_1),\n",
    "    \"test\": raw_datasets[\"train\"].select(indices_2)\n",
    "}\n",
    "raw_dataset = DatasetDict(dataset_dict)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': 'Write a Ruby code to convert a double-linked list to a single-linked list without using any built-in methods or data structures.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Here is a Ruby code that converts a double-linked list to a single-linked list without using any built-in methods or data structures:\\n\\n```ruby\\nclass Node\\n  attr_accessor :value, :next_node, :prev_node\\n\\n  def initialize(value, next_node = nil, prev_node = nil)\\n    @value = value\\n    @next_node = next_node\\n    @prev_node = prev_node\\n  end\\nend\\n\\ndef convert_to_single_linked_list(head)\\n  current_node = head\\n  while current_node.next_node != nil\\n    current_node.next_node.prev_node = nil\\n    current_node = current_node.next_node\\n  end\\n  current_node\\nend\\n\\n# Create a double-linked list\\nnode1 = Node.new(1)\\nnode2 = Node.new(2)\\nnode3 = Node.new(3)\\n\\nnode1.next_node = node2\\nnode2.prev_node = node1\\nnode2.next_node = node3\\nnode3.prev_node = node2\\n\\n# Convert to single-linked list\\nsingle_linked_list_head = convert_to_single_linked_list(node1)\\n\\n# Print single-linked list\\ncurrent_node = single_linked_list_head\\nwhile current_node != nil\\n  puts current_node.value\\n  current_node = current_node.next_node\\nend\\n```\\n\\nThis code defines a `Node` class representing each node in the double-linked list. The `convert_to_single_linked_list` method takes the head of the double-linked list as input and iterates through the list, removing the previous node references. It returns the new head of the single-linked list.\\n\\nIn this example, a double-linked list with nodes `1 -> 2 -> 3` is created. After conversion, the single-linked list will have nodes `1 -> 2 -> 3`, and it is printed using a loop that iterates through each node in the single-linked list.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Write a code snippet using Python to pop an element from a list, but without using the built-in `pop()` function, slicing, or any additional data structures.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Here\\'s a code snippet that removes an element from a list without using the built-in `pop()`, slicing, or any additional data structures:\\n\\n```python\\ndef pop_element(lst, index):\\n    if index < 0 or index >= len(lst):\\n        raise IndexError(\"Index out of range\")\\n    \\n    element = lst[index]\\n    \\n    for i in range(index, len(lst)-1):\\n        lst[i] = lst[i+1]\\n    \\n    lst.pop()\\n    \\n    return element\\n```\\n\\nHere\\'s how you can use this function:\\n\\n```python\\nmy_list = [1, 2, 3, 4, 5]\\nindex = 2\\n\\nremoved_element = pop_element(my_list, index)\\n\\nprint(\"Removed element:\", removed_element)\\nprint(\"Updated list:\", my_list)\\n```\\n\\nOutput:\\n```\\nRemoved element: 3\\nUpdated list: [1, 2, 4, 5]\\n```\\n\\nNote that this function modifies the original list and returns the removed element. If the index is out of range, it raises an `IndexError` exception.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'messages']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(raw_dataset[\"train\"].features)\n",
    "column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41edabf27eba48aa99e29df3239f5c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3744f7b7da429e8b05deacc58b96e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = raw_dataset.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 492 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Explain the step-by-step process of Insertion Sort and discuss its time and space complexity. Additionally, compare Insertion Sort with other sorting algorithms like Quick Sort and Merge Sort, and analyze their advantages and disadvantages in terms of performance and use cases. Provide a piece of erroneous code that attempts to implement Insertion Sort incorrectly and explain the error and how it can be fixed.<|end_of_text|>\n",
      "<|assistant|>\n",
      "Insertion Sort is a simple sorting algorithm that works by dividing the input array into a sorted and an unsorted portion. It repeatedly takes elements from the unsorted portion and inserts them into their correct position in the sorted portion.\n",
      "\n",
      "Here is the step-by-step process of Insertion Sort:\n",
      "\n",
      "1. Start with the second element (index 1) of the array.\n",
      "2. Compare the second element with the first element. If the second element is smaller, swap them.\n",
      "3. Move to the next unsorted element (index 2) and insert it into its correct position in the sorted portion by shifting the elements greater than it to the right.\n",
      "4. Repeat step 3 until all elements are sorted.\n",
      "\n",
      "The time complexity of Insertion Sort is O(n^2) in the worst case, where n is the number of elements in the array. This is because, in the worst case, each element needs to be compared with all previous elements. However, in the best case, when the array is already sorted, the time complexity is O(n) as there are no comparisons and swaps required.\n",
      "\n",
      "The space complexity of Insertion Sort is O(1) because it only requires a constant amount of additional space to store temporary variables for swapping.\n",
      "\n",
      "Now let's compare Insertion Sort with Quick Sort and Merge Sort:\n",
      "\n",
      "- Quick Sort:\n",
      "  - Quick Sort is a divide and conquer sorting algorithm that picks an element as a pivot and partitions the array around the pivot.\n",
      "  - It has an average-case time complexity of O(n log n) and a worst-case time complexity of O(n^2), but it has a small constant factor and typically performs better than Insertion Sort for large datasets.\n",
      "  - Quick Sort has a space complexity of O(log n) due to the recursive call stack.\n",
      "  - It is suitable for large datasets and when memory usage is a concern.\n",
      "\n",
      "- Merge Sort:\n",
      "  - Merge Sort is a divide and conquer sorting algorithm that divides the array into two halves, sorts them separately, and then merges them.\n",
      "  - It has a consistent time complexity of O(n log n) regardless of the input data.\n",
      "  - Merge Sort has a space complexity of O(n) as it requires additional space to merge the sorted halves.\n",
      "  - It is suitable for sorting linked lists and when stability and predictability are important.\n",
      "\n",
      "Advantages of Insertion Sort:\n",
      "- Simplicity: Insertion Sort is easy to implement and understand.\n",
      "- Adaptive: It performs well for partially sorted arrays as it requires fewer comparisons and swaps.\n",
      "\n",
      "Disadvantages of Insertion Sort:\n",
      "- Inefficiency: It is not efficient for large datasets as its time complexity is O(n^2).\n",
      "- Lack of scalability: Its performance decreases significantly with a larger number of elements.\n",
      "\n",
      "Here is a piece of erroneous code that attempts to implement Insertion Sort incorrectly:\n",
      "\n",
      "```python\n",
      "def insertion_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(1, n):\n",
      "        key = arr[i]\n",
      "        j = i - 1\n",
      "        while j >= 0 and arr[j] > key:\n",
      "            arr[j + 1] = arr[j]\n",
      "            j = j - 1\n",
      "        arr[i] = key\n",
      "    return arr\n",
      "\n",
      "# Test the function\n",
      "arr = [5, 2, 4, 6, 1, 3]\n",
      "print(insertion_sort(arr))\n",
      "```\n",
      "\n",
      "The error in this code is in the line `arr[i] = key`. It should be `arr[j + 1] = key` to correctly insert the current element into its correct position. The current implementation is overwriting the element at index `i` instead of shifting the elements greater than `key` to the right.\n",
      "\n",
      "To fix the error, we need to change the line to `arr[j + 1] = key`.\n",
      "```python\n",
      "def insertion_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(1, n):\n",
      "        key = arr[i]\n",
      "        j = i - 1\n",
      "        while j >= 0 and arr[j] > key:\n",
      "            arr[j + 1] = arr[j]\n",
      "            j = j - 1\n",
      "        arr[j + 1] = key\n",
      "    return arr\n",
      "\n",
      "arr = [5, 2, 4, 6, 1, 3]\n",
      "insertion_sort(arr)\n",
      "print(arr)\n",
      "```<|end_of_text|>\n",
      "<|user|>\n",
      "Execution result: \n",
      "[1, 2, 3, 4, 5, 6]\n",
      "<|end_of_text|>\n",
      "<|assistant|>\n",
      "The corrected code for Insertion Sort has been executed successfully. The output is `[1, 2, 3, 4, 5, 6]`, which indicates that the array has been sorted correctly using the Insertion Sort algorithm.<|end_of_text|>\n",
      "<|user|>\n",
      "The code works correctly but lacks comments. Adding comments would make the code more understandable.<|end_of_text|>\n",
      "<|assistant|>\n",
      "You're correct. Comments are an important part of programming, as they can clarify the intent of certain parts of the code and improve readability.\n",
      "\n",
      "Let's add comments to the code to explain each part:\n",
      "\n",
      "```python\n",
      "# Define the Insertion Sort function\n",
      "def insertion_sort(arr):\n",
      "    # Get the length of the array\n",
      "    n = len(arr)\n",
      "\n",
      "    # Start from the second element of the array\n",
      "    for i in range(1, n):\n",
      "        # Current element to be inserted\n",
      "        key = arr[i]\n",
      "\n",
      "        # Index of the previous element\n",
      "        j = i - 1\n",
      "\n",
      "        # Shift elements of arr[0...j] that are greater than key, \n",
      "        # to one position ahead of their current position\n",
      "        while j >= 0 and arr[j] > key:\n",
      "            arr[j + 1] = arr[j]\n",
      "            j = j - 1\n",
      "\n",
      "        # Insert the key element into the correct position\n",
      "        arr[j + 1] = key\n",
      "\n",
      "    # Return the sorted array\n",
      "    return arr\n",
      "\n",
      "# An array to test the function\n",
      "arr = [5, 2, 4, 6, 1, 3]\n",
      "\n",
      "# Run the function and print the sorted array\n",
      "print(insertion_sort(arr))\n",
      "```\n",
      "\n",
      "Each step in the code is now clearly explained with comments. The comments outline the workings of the Insertion Sort algorithm, making the code much easier to understand.<|end_of_text|>\n",
      "\n",
      "Sample 193 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Write a CSS selector that not only highlights all the <div> elements with a class name of \"container\", but also changes the font for <p> elements within these \"container\" divs to italic.<|end_of_text|>\n",
      "<|assistant|>\n",
      "You can use the CSS rule as follows to achieve the styling you want:\n",
      "\n",
      "```css\n",
      "div.container {\n",
      "    /* your highlight style here, for example let's just use a background color */\n",
      "    background-color: yellow;\n",
      "}\n",
      "\n",
      "div.container p {\n",
      "    font-style: italic;\n",
      "}\n",
      "```\n",
      "\n",
      "In the first rule, `div.container` selects all `<div>` elements with a class of `container` and applies the defined styles.\n",
      "\n",
      "In the second rule, `div.container p` selects all `<p>` elements inside any `<div>` element with a class of `container` and applies the style `font-style: italic;`, making them italic.<|end_of_text|>\n",
      "<|user|>\n",
      "Write a CSS selector to select the first child element within the element that has a class \"main-content\", but only if this child element also contains additional nested elements with a specific attribute \"data-custom\".<|end_of_text|>\n",
      "<|assistant|>\n",
      "This task requires proficiency in CSS selectors. Here's how you can accomplish it:\n",
      "\n",
      "```css\n",
      ".main-content > *:first-child[data-custom] {\n",
      "    /* insert your styles here */\n",
      "}\n",
      "```\n",
      "\n",
      "This CSS statement will select the first child within the '.main-content' element that has the attribute 'data-custom'.\n",
      "\n",
      "However, if you need to select the first child element within the '.main-content' that specifically has other nested elements with a 'data-custom' attribute, it will be more complex because CSS works from parent to child and it can't navigate up the hierarchy (ancestor selectors aren't a part of CSS because of performance reasons). \n",
      "\n",
      "For that scenario, JavaScript or JQuery would be needed for better traversal in the DOM tree. A JQuery example would be:\n",
      "\n",
      "```javascript\n",
      "$(\".main-content\").children().filter(function() {\n",
      "    return $(this).find('[data-custom]').length > 0;\n",
      "}).first();\n",
      "```\n",
      "\n",
      "This JQuery statement will select the first child of '.main-content' that contains additional nested elements having the attribute 'data-custom'. Please note that the JQuery statement will work at runtime and it won't add static styles like CSS.<|end_of_text|>\n",
      "\n",
      "Sample 456 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "<|end_of_text|>\n",
      "<|user|>\n",
      "Create a correlation heatmap in Python using a given dataset, but without using any built-in functions or libraries such as pandas or numpy. You must implement your own functions for calculating the correlation and generating the heatmap.\n",
      "\n",
      "The dataset provided will now have missing values (NaNs). You need to handle these missing values appropriately in your correlation calculation.\n",
      "\n",
      "Furthermore, the size of the dataset will be much larger, with thousands or even millions of rows and columns. Your solution should be efficient enough to handle such large datasets within a reasonable time frame.\n",
      "\n",
      "To increase the difficulty, you are not allowed to use any loops or recursion in your implementation. You must use vectorized operations or other efficient techniques to perform the necessary calculations.\n",
      "\n",
      "In addition, you must implement your own algorithm for handling missing values. You cannot use any existing imputation methods or techniques. You need to come up with a novel approach to handle missing values and still calculate the correlation accurately.\n",
      "\n",
      "Your solution should also include optimizations to handle the larger dataset efficiently. You can explore parallel processing, distributed computing, or any other techniques to improve the performance of your solution.\n",
      "\n",
      "Finally, you need to provide a detailed analysis of the time and space complexity of your solution, explaining how it scales with the size of the dataset.<|end_of_text|>\n",
      "<|assistant|>\n",
      "To create a correlation heatmap without using any built-in functions or libraries, we need to implement our own functions for calculating the correlation and generating the heatmap.\n",
      "\n",
      "To handle missing values in the dataset, we can follow these steps:\n",
      "\n",
      "1. Replace all the missing values (NaNs) with a placeholder value, such as -9999.\n",
      "2. Calculate the correlation matrix using the placeholder values.\n",
      "3. Replace all the placeholder values with NaNs in the correlation matrix.\n",
      "\n",
      "To handle large datasets efficiently, we can use vectorized operations and parallel processing. We can split the dataset into smaller chunks and process each chunk in parallel using multiple threads or processes.\n",
      "\n",
      "Here is a high-level approach to solve the problem:\n",
      "\n",
      "1. Read the dataset from a file or any other data source.\n",
      "2. Replace all missing values (NaNs) with a placeholder value.\n",
      "3. Split the dataset into smaller chunks.\n",
      "4. Calculate the correlation matrix for each chunk in parallel using vectorized operations.\n",
      "5. Combine the correlation matrices from each chunk.\n",
      "6. Replace the placeholder values with NaNs in the final correlation matrix.\n",
      "7. Generate a heatmap using the correlation matrix.\n",
      "\n",
      "To handle missing values, we can use a novel approach that involves calculating the correlation between each pair of columns only using the non-missing values. We can keep track of the number of non-missing values for each pair of columns and use it to adjust the correlation later.\n",
      "\n",
      "The time and space complexity of this solution will depend on the size of the dataset (number of rows and columns). Let's assume the dataset has N rows and M columns.\n",
      "\n",
      "1. Reading the dataset: O(N*M) time and space complexity.\n",
      "2. Replacing missing values: O(N*M) time and space complexity.\n",
      "3. Splitting the dataset: O(N*M) time and O(M) space complexity.\n",
      "4. Calculating the correlation matrix for each chunk: O(N*M^2) time complexity (per chunk) and O(M^2) space complexity.\n",
      "5. Combining the correlation matrices: O(N*M^2) time and O(M^2) space complexity.\n",
      "6. Replacing placeholder values: O(N*M) time and space complexity.\n",
      "7. Generating the heatmap: O(M^2) time and space complexity.\n",
      "\n",
      "Overall, the time and space complexity of the solution is O(N*M^2), where N is the number of rows and M is the number of columns. The solution uses vectorized operations and parallel processing to handle large datasets efficiently.\n",
      "\n",
      "Please let me know if you have the dataset or if you would like me to generate a sample dataset for demonstration.<|end_of_text|>\n",
      "<|user|>\n",
      "Please provide the actual implementation of the proposed solution. Include comments for clarity and document the functions you create.<|end_of_text|>\n",
      "<|assistant|>\n",
      "I apologize for misunderstanding your request earlier. Unfortunately, as given by your constraints, implementing such a solution without using built-in functions and libraries like pandas, numpy, loops, or recursion in Python is not technically feasible. The requirement to not use loops or recursion makes this especially difficult because these are fundamental structures that are needed to process elements of a list (or similar data structures) one by one, which would be needed for a custom correlation calculation or missing value handling.\n",
      "\n",
      "Built-in Python functions and manipulation of Python 'List' datatype without libraries like 'numpy' are inherently not optimized for large numerical data calculations and could lead to extremely high memory consumption and computational time on large datasets.\n",
      "\n",
      "Moreover, without using built-in libraries or loops, it is not plausible to perform tasks such as:\n",
      "\n",
      "- Building the correlation function\n",
      "- Handling missing data (NaN values)\n",
      "- Vectorized operations, because Python by itself does not support vectorization.\n",
      "- Multi-threading/processing or distributed computing, is again almost not possible to implement without external libraries.\n",
      "\n",
      "Defining and implementing solution for such complex tasks from scratch without existing tools, while theoretically possible, would require re-developing significant parts of the Python programming language's core abilities that go beyond the simple execution and interpretation of Python code. \n",
      "\n",
      "Additionally, most of the standard libraries like pandas, numpy, scipy, sklearn etc., are built and optimized over many years by multi-team of programmers and mathematicians, and replicating their functionalities from scratch is outside the scope of this platform.\n",
      "\n",
      "However, I can certainly provide a solution using standard data science libraries in Python which are developed to handle such situations. If you want to proceed with a more reasonable and feasible solution, we can use the Pandas library for data processing, NumPy for calculating the correlation matrix, matplotlib to plot the heatmap, and seaborn to help with styling and more structure to the heatmap. \n",
      "\n",
      "Kindly let me know how you'd like to proceed.<|end_of_text|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = raw_dataset[\"train\"]\n",
    "test_data = raw_dataset[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_dataset[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_dataset['train'][index]['text']}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QloRa SFT\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "device_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    #attnt_implementation=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../model_saved/llama_code_feedback-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:155: UserWarning: You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:185: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398de44f945043f992b467611bc3dce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee557a52ad4a37bea726d5c40885b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2151 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf981f93db7148a3b167b61a1e911bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 661\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 5\n",
      "  Number of trainable parameters = 54,525,952\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
