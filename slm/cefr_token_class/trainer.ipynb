{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer (e.g., BERT tokenizer)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "dataset = load_dataset(\"Alex123321/english_cefr_dataset\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and temporary dataset (for validation and test)\n",
    "train_ds, temp_ds = dataset['train'].train_test_split(test_size=0.2).values()\n",
    "\n",
    "# Split the temporary dataset into validation and test\n",
    "val_ds, test_ds = temp_ds.train_test_split(test_size=0.5).values()\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Validation size: {len(val_ds)}\")\n",
    "print(f\"Test size: {len(test_ds)}\")\n",
    "\n",
    "# Create a DatasetDict to organize the splits\n",
    "split_dataset = {\n",
    "    'train': train_ds,\n",
    "    'validation': val_ds,\n",
    "    'test': test_ds\n",
    "}\n",
    "\n",
    "# Alternatively, if you want to have it in a DatasetDict format:\n",
    "from datasets import DatasetDict\n",
    "dataset_split = DatasetDict(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(dataset['train']['ud_word_level'])  # Assuming you have the full dataset\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the word (this will split compound words into subwords if necessary)\n",
    "    tokenized_inputs = tokenizer(examples['ud_word'], padding=True, truncation=True, is_split_into_words=False)\n",
    "\n",
    "    # Convert labels to numeric IDs for the ud_word_level\n",
    "    word_ids = tokenized_inputs.word_ids()  # This gives the mapping from token to word ID\n",
    "    \n",
    "    # Assign the same label to all tokens belonging to the same word\n",
    "    labels = []\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            labels.append(-100)  # Padding tokens should be ignored, hence the label -100\n",
    "        else:\n",
    "            # Assign the label for the entire word (based on the ud_word_level)\n",
    "            labels.append(label2id[examples['ud_word_level']])  # Use the level label for the word\n",
    "\n",
    "    # Add the labels to the tokenized inputs\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each split of the dataset (train, validation, and test)\n",
    "train_ds = dataset_split['train'].map(preprocess_function, batched=False)\n",
    "val_ds = dataset_split['validation'].map(preprocess_function, batched=False)\n",
    "test_ds = dataset_split['test'].map(preprocess_function, batched=False)\n",
    "\n",
    "# Check the first processed item\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize the model (BERT for token classification)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label2id))\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=32,  # Batch size for training\n",
    "    per_device_eval_batch_size=32,  # Batch size for evaluation\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    save_total_limit=1,               # keep only the best model (deletes older checkpoints)\n",
    "    load_best_model_at_end=True,      # load the best model after training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = trainer.evaluate()\n",
    "print(eval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 1, 2, 2, 2, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results/checkpoint-950\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"results/checkpoint-950\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "# Example input text for token classification\n",
    "text = \"All you need is guigui\"\n",
    "\n",
    "# Tokenize the text using the loaded tokenizer\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model's predictions (logits) from the tokenized input\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# The logits are typically in shape (batch_size, sequence_length, num_labels)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted labels by taking the argmax of the logits\n",
    "predictions = logits.argmax(dim=-1)\n",
    "\n",
    "# Convert the predictions to label indices (if needed)\n",
    "predicted_labels = predictions[0].tolist()\n",
    "\n",
    "print(predicted_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love this game. Goodbye.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_2',\n",
       "  'score': 0.32772133,\n",
       "  'index': 1,\n",
       "  'word': 'I',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.31449705,\n",
       "  'index': 2,\n",
       "  'word': 'love',\n",
       "  'start': 2,\n",
       "  'end': 6},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.31998727,\n",
       "  'index': 3,\n",
       "  'word': 'this',\n",
       "  'start': 7,\n",
       "  'end': 11},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.29040098,\n",
       "  'index': 4,\n",
       "  'word': 'game',\n",
       "  'start': 12,\n",
       "  'end': 16},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.26514834,\n",
       "  'index': 5,\n",
       "  'word': '.',\n",
       "  'start': 16,\n",
       "  'end': 17},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.27017993,\n",
       "  'index': 6,\n",
       "  'word': 'Goodbye',\n",
       "  'start': 18,\n",
       "  'end': 25},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.2647442,\n",
       "  'index': 7,\n",
       "  'word': '.',\n",
       "  'start': 25,\n",
       "  'end': 26}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"results/checkpoint-950/\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS], Predicted label: 3, Confidence: 0.2537\n",
      "Token: I, Predicted label: 1, Confidence: 0.1926\n",
      "Token: love, Predicted label: 2, Confidence: 0.2095\n",
      "Token: this, Predicted label: 5, Confidence: 0.2808\n",
      "Token: game, Predicted label: 2, Confidence: 0.2358\n",
      "Token: ,, Predicted label: 3, Confidence: 0.2345\n",
      "Token: goodbye, Predicted label: 5, Confidence: 0.1992\n",
      "Token: [SEP], Predicted label: 1, Confidence: 0.3825\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LordCoffee/bert-base-cased-cefr\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"LordCoffee/bert-base-cased-cefr\")\n",
    "\n",
    "OOV_LABEL_ID = \"UNK\"\n",
    "# Example input text\n",
    "text = \"I love this game, goodbye\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "# Get predicted labels (most likely label)\n",
    "predicted_labels = torch.argmax(probabilities, dim=-1)\n",
    "# Set a confidence threshold for uncertain predictions\n",
    "confidence_threshold = 0\n",
    "\n",
    "# Map predicted labels to token strings and check confidence\n",
    "predicted_labels = predicted_labels[0].tolist()\n",
    "for idx, label_id in enumerate(predicted_labels):\n",
    "    token_str = tokenizer.decode(inputs['input_ids'][0][idx]).strip()\n",
    "    confidence = probabilities[0, idx, label_id].item()\n",
    "    \n",
    "    # If confidence is below threshold, classify as \"UNK\"\n",
    "    if confidence < confidence_threshold:\n",
    "        label_id = OOV_LABEL_ID  # Assign special label \"UNK\"\n",
    "    \n",
    "    print(f\"Token: {token_str}, Predicted label: {label_id}, Confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
