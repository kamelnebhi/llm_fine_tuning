{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate datasets transformers accelerate==1.9.0 wandb safetensors==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate datasets transformers accelerate==1.9.0 wandb safetensors==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q datasets evaluate\n",
    "#!pip install -q sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing performance sur example de model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores: 8\n",
      "RAM total: 30.99 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "print(\"CPU cores:\", psutil.cpu_count(logical=True))\n",
    "print(\"RAM total:\", round(psutil.virtual_memory().total / 1024**3, 2), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Num threads: 1\n",
      "Corrected: This sentence has bad grammar.\n",
      "Inference time: 0.782 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time\n",
    "\n",
    "# ----------- 1. Forcer CPU et nombre de threads -----------\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(2)  # Limite PyTorch à 2 CPU\n",
    "print(\"Using device:\", device)\n",
    "print(\"Num threads:\", torch.get_num_threads())\n",
    "\n",
    "# ----------- 2. Charger le modèle et le tokenizer -----------\n",
    "model_name = \"vennify/t5-base-grammar-correction\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ----------- 3. Phrase à corriger -----------\n",
    "text = \"This sentences has has bads grammar.\"\n",
    "input_text = \"grammar: \" + text\n",
    "\n",
    "# Tokenization\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ----------- 4. Génération avec mesure du temps -----------\n",
    "with torch.inference_mode():  # économise la mémoire\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=5,\n",
    "        min_length=1\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "# Décodage du texte\n",
    "corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Corrected:\", corrected_text)\n",
    "print(f\"Inference time: {end_time - start_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Num threads: 4\n",
      "Corrected: This sentence has bad grammar.\n",
      "Inference time: 0.415 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time\n",
    "\n",
    "# ----------- 1. Forcer CPU et nombre de threads -----------\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(4)  # Limite PyTorch à 2 CPU\n",
    "print(\"Using device:\", device)\n",
    "print(\"Num threads:\", torch.get_num_threads())\n",
    "\n",
    "# ----------- 2. Charger le modèle et le tokenizer -----------\n",
    "model_name = \"vennify/t5-base-grammar-correction\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ----------- 3. Phrase à corriger -----------\n",
    "text = \"This sentences has has bads grammar.\"\n",
    "input_text = \"grammar: \" + text\n",
    "\n",
    "# Tokenization\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ----------- 4. Génération avec mesure du temps -----------\n",
    "with torch.inference_mode():  # économise la mémoire\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=5,\n",
    "        min_length=1\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "# Décodage du texte\n",
    "corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Corrected:\", corrected_text)\n",
    "print(f\"Inference time: {end_time - start_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = \"t5-base\"\n",
    "PREFIX = \"grammar: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "training_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "training_device\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODELNAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODELNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 82466\n",
      "    })\n",
      "})\n",
      "{'_id': '0', 'task': 'gec', 'src': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'tgt': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n{'_id': '0', 'task': 'gec', \\n'src': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', \\n'tgt': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"dim/grammarly_coedit\")\n",
    "\n",
    "print(ds)\n",
    "\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['_id', 'task', 'src', 'tgt'],\n",
    "        num_rows: 82466\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "print(ds[\"train\"][0])\n",
    "\n",
    "\"\"\"\n",
    "{'_id': '0', 'task': 'gec', \n",
    "'src': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', \n",
    "'tgt': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(example):\n",
    "    # Remove the numeric prefix before colon\n",
    "    string_list = example[\"src\"].split(\":\")\n",
    "    text = \" \".join(string_list[1:]).strip()\n",
    "    example[\"src\"] = text\n",
    "    return example\n",
    "\n",
    "# Apply the cleaning function\n",
    "ds[\"train\"] = ds[\"train\"].map(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '0', 'task': 'gec', 'src': 'For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'tgt': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Max src length: 199\n",
      "🔤 Max tgt length: 199\n",
      "📊 95th percentile src length: 52.0\n",
      "📊 95th percentile tgt length: 49.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n🔤 Max src length: 199\\n🔤 Max tgt length: 199\\n📊 95th percentile src length: 52.0\\n📊 95th percentile tgt length: 49.0\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate lengths\n",
    "src_lengths = [len(tokenizer.encode(\"grammar: \" + x[\"src\"])) for x in ds[\"train\"]]\n",
    "tgt_lengths = [len(tokenizer.encode(x[\"tgt\"])) for x in ds[\"train\"]]\n",
    "\n",
    "# Stats\n",
    "print(f\"🔤 Max src length: {max(src_lengths)}\")\n",
    "print(f\"🔤 Max tgt length: {max(tgt_lengths)}\")\n",
    "print(f\"📊 95th percentile src length: {np.percentile(src_lengths, 95)}\")\n",
    "print(f\"📊 95th percentile tgt length: {np.percentile(tgt_lengths, 95)}\")\n",
    "\n",
    "# Optional: Plot histograms\n",
    "plt.hist(src_lengths, bins=50, alpha=0.6, label='src (grammar: input)')\n",
    "plt.hist(tgt_lengths, bins=50, alpha=0.6, label='tgt (corrected)')\n",
    "plt.axvline(128, color='red', linestyle='--', label='max_length=128')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "🔤 Max src length: 199\n",
    "🔤 Max tgt length: 199\n",
    "📊 95th percentile src length: 52.0\n",
    "📊 95th percentile tgt length: 49.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    input_text = PREFIX + example[\"src\"]\n",
    "    target_text = example[\"tgt\"]\n",
    "    model_inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess, batched=False)\n",
    "tokenized_val = val_ds.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Exact match\n",
    "    em = exact_match.compute(predictions=decoded_preds, references=decoded_labels)[\"exact_match\"]\n",
    "\n",
    "    # BLEU via sacrebleu\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
    "    bleu_score = bleu.score\n",
    "\n",
    "    return {\"exact_match\": em, \"bleu\": bleu_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4813/1404983597.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='27834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  210/27834 00:33 < 1:13:28, 6.27 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m      4\u001b[0m training_args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./t5-grammar-corrector\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     do_eval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     report_to\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[39m=\u001b[39mmodel,                             \u001b[39m# The model to train\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                      \u001b[39m# Training configurations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,                     \u001b[39m# Tokenizer (for padding, decoding, etc.)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     28\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mEpoch Training Loss Validation Loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m1 0.185000 0.165196\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mTrainOutput(global_step=27834, training_loss=0.1946344326956995, metrics={'train_runtime': 2212.8832, 'train_samples_per_second': 100.619, 'train_steps_per_second': 12.578, 'total_flos': 7533699872587776.0, 'train_loss': 0.1946344326956995, 'epoch': 3.0})\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2328\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2329\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2330\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2331\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2332\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2333\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2665\u001b[0m context \u001b[39m=\u001b[39m (\n\u001b[1;32m   2666\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mno_sync, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   2667\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_samples) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2668\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2669\u001b[0m     \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext\n\u001b[1;32m   2670\u001b[0m )\n\u001b[1;32m   2671\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m-> 2672\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2674\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2675\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2676\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2677\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2678\u001b[0m ):\n\u001b[1;32m   2679\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py:4009\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4006\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   4008\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4009\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, num_items_in_batch\u001b[39m=\u001b[39;49mnum_items_in_batch)\n\u001b[1;32m   4011\u001b[0m \u001b[39mdel\u001b[39;00m inputs\n\u001b[1;32m   4012\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   4015\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py:4099\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4097\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_items_in_batch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4098\u001b[0m     inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m-> 4099\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   4100\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4101\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/utils/operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 818\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/utils/operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 806\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1726\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[39m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[39m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1726\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1727\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1728\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1729\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1730\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1731\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1732\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1733\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1734\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1736\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1737\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1738\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1739\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1740\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1099\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m   1097\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m-> 1099\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1100\u001b[0m     hidden_states,\n\u001b[1;32m   1101\u001b[0m     causal_mask,\n\u001b[1;32m   1102\u001b[0m     position_bias,\n\u001b[1;32m   1103\u001b[0m     encoder_hidden_states,\n\u001b[1;32m   1104\u001b[0m     encoder_extended_attention_mask,\n\u001b[1;32m   1105\u001b[0m     encoder_decoder_position_bias,  \u001b[39m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m   1106\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1107\u001b[0m     cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1108\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1109\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1110\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1111\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1112\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1113\u001b[0m )\n\u001b[1;32m   1115\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1117\u001b[0m \u001b[39m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[39m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[39m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[39m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), \u001b[39m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m minimum_action \u001b[39min\u001b[39;00m (Action\u001b[39m.\u001b[39mNOTIFY, Action\u001b[39m.\u001b[39mNOTIFY_ALWAYS) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[39m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:686\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39m@deprecate_kwarg\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpast_key_value\u001b[39m\u001b[39m\"\u001b[39m, new_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m, version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m4.58\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\n\u001b[1;32m    671\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m     cache_position\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    685\u001b[0m ):\n\u001b[0;32m--> 686\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[1;32m    688\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    689\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    690\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    691\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    692\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    693\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    694\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    695\u001b[0m     )\n\u001b[1;32m    696\u001b[0m     hidden_states \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    697\u001b[0m     attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m minimum_action \u001b[39min\u001b[39;00m (Action\u001b[39m.\u001b[39mNOTIFY, Action\u001b[39m.\u001b[39mNOTIFY_ALWAYS) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[39m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:602\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39m@deprecate_kwarg\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpast_key_value\u001b[39m\u001b[39m\"\u001b[39m, new_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m, version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m4.58\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    590\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\n\u001b[1;32m    591\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     cache_position\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ):\n\u001b[1;32m    601\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 602\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[1;32m    603\u001b[0m         normed_hidden_states,\n\u001b[1;32m    604\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    605\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    606\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    607\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    608\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    609\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    610\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    612\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    613\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m minimum_action \u001b[39min\u001b[39;00m (Action\u001b[39m.\u001b[39mNOTIFY, Action\u001b[39m.\u001b[39mNOTIFY_ALWAYS) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[39m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:492\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39m# if key_value_states are provided this layer is used as a cross-attention layer for the decoder\u001b[39;00m\n\u001b[1;32m    490\u001b[0m is_cross_attention \u001b[39m=\u001b[39m key_value_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq(hidden_states)\n\u001b[1;32m    493\u001b[0m query_states \u001b[39m=\u001b[39m query_states\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_value_proj_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    495\u001b[0m \u001b[39m# Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-grammar-corrector\",\n",
    "    do_eval=True,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                             # The model to train\n",
    "    args=training_args,                      # Training configurations\n",
    "    train_dataset=tokenized_train,           # Tokenized training data\n",
    "    eval_dataset=tokenized_val,              # Tokenized validation data\n",
    "    tokenizer=tokenizer,                     # Tokenizer (for padding, decoding, etc.)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\"\"\"\n",
    "Epoch Training Loss Validation Loss\n",
    "1 0.185000 0.165196\n",
    "2 0.178100 0.161055\n",
    "3 0.171300 0.160168\n",
    "TrainOutput(global_step=27834, training_loss=0.1946344326956995, metrics={'train_runtime': 2212.8832, 'train_samples_per_second': 100.619, 'train_steps_per_second': 12.578, 'total_flos': 7533699872587776.0, 'train_loss': 0.1946344326956995, 'epoch': 3.0})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./t5-grammar-corrector\")\n",
    "tokenizer.save_pretrained(\"./t5-grammar-corrector\")\n",
    "\n",
    "\"\"\"\n",
    "('./t5-grammar-corrector/tokenizer_config.json',\n",
    " './t5-grammar-corrector/special_tokens_map.json',\n",
    " './t5-grammar-corrector/spiece.model',\n",
    " './t5-grammar-corrector/added_tokens.json')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"📊 Evaluation Results:\", eval_results)\n",
    "\n",
    "\"\"\" \n",
    "[1031/1031 00:25]\n",
    "📊 Evaluation Results: {'eval_loss': 0.1601683497428894, 'eval_runtime': 25.8443, 'eval_samples_per_second': 319.104, 'eval_steps_per_second': 39.893, 'epoch': 3.0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5-grammar-corrector\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5-grammar-corrector\")\n",
    "\n",
    "def correct_grammar(text: str):\n",
    "    input_text = f\"grammar: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(input_ids, max_length=128, num_beams=4)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Try on a sample sentence\n",
    "test_sentence = \"He go to school every day.\"\n",
    "corrected = correct_grammar(test_sentence)\n",
    "print(f\"\\n❌ Original:  {test_sentence}\")\n",
    "print(f\"✅ Corrected: {corrected}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "❌ Original:  He go to school every day.\n",
    "✅ Corrected: He goes to school every day.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
