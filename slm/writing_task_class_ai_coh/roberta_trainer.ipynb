{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "training_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "training_device\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting accelerate==1.9.0\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting safetensors==0.4.3\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==1.9.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==1.9.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==1.9.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==1.9.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==1.9.0) (2.2.2)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate==1.9.0)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.7.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.21.0->accelerate==1.9.0)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.2.1)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: pydantic<3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.9.2)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.35.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3->wandb) (2.23.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.9.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.9.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.9.0) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate==1.9.0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate==1.9.0) (1.3.0)\n",
      "Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Downloading sentry_sdk-2.35.0-py2.py3-none-any.whl (363 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, smmap, sentry-sdk, safetensors, propcache, multidict, hf-xet, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface_hub, gitdb, aiosignal, tokenizers, gitpython, aiohttp, accelerate, wandb, transformers, datasets, evaluate\n",
      "\u001b[2K  Attempting uninstall: fsspec━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/25\u001b[0m [sentry-sdk]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/25\u001b[0m [sentry-sdk]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/25\u001b[0m [sentry-sdk]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/25\u001b[0m [sentry-sdk]\n",
      "\u001b[2K  Attempting uninstall: dill╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/25\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/25\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/25\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/25\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: multiprocess\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/25\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18━━━━━━━━━\u001b[0m \u001b[32m 9/25\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/25\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.18━━━━━━━━━━━\u001b[0m \u001b[32m 9/25\u001b[0m [dill]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [evaluate]/25\u001b[0m [datasets]ers]ub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.0.0 dill-0.3.8 evaluate-0.4.5 frozenlist-1.7.0 fsspec-2025.3.0 gitdb-4.0.12 gitpython-3.1.45 hf-xet-1.1.7 huggingface_hub-0.34.4 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 safetensors-0.4.3 sentry-sdk-2.35.0 smmap-5.0.2 tokenizers-0.21.4 transformers-4.55.2 wandb-0.21.1 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate datasets transformers accelerate==1.9.0 wandb safetensors==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordId</th>\n",
       "      <th>gpt4o_judge_score</th>\n",
       "      <th>nova_judge_score</th>\n",
       "      <th>llama3_judge_score</th>\n",
       "      <th>majority_value</th>\n",
       "      <th>agreement_percentage</th>\n",
       "      <th>writing_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>level_title</th>\n",
       "      <th>cefr_level</th>\n",
       "      <th>ef_level</th>\n",
       "      <th>activity_instructions</th>\n",
       "      <th>student_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALL0021715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>295f109e-cfeb-48ce-82ba-ce6f2bf423d2</td>\n",
       "      <td>d491686c-8998-4062-bd13-27562deecdbe</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>B1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A client who is thinking about expanding into ...</td>\n",
       "      <td>N n n n n n. N nn n. N n n. N. NN n n n nn n n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALL0019540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>8216749c-2498-4ff2-a241-d66d650689c0</td>\n",
       "      <td>2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>B1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Now write your counter-proposal. Remember to a...</td>\n",
       "      <td>df f fws wf f wf wf wef we fw fw ef wef ew few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CALL0012294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>dd1cf6a9-aed3-47cf-b094-8a86988c79ae</td>\n",
       "      <td>9032f3ed-5da1-4efe-b4dd-c9470bb35d65</td>\n",
       "      <td>16-Upper Advanced</td>\n",
       "      <td>C2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Read an email from your friend and give advice...</td>\n",
       "      <td>Hey don give up so quickly, give time some tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALL0005503</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>a2376227-7402-48ee-ad9a-f909f372ccd5</td>\n",
       "      <td>493dffa9-7513-4408-be4b-5b010a1bf051</td>\n",
       "      <td>10-Upper Intermediate</td>\n",
       "      <td>B2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Your manager sent you an email, asking you to ...</td>\n",
       "      <td>Company Update Presentation Outline\\n\\nGreet e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALL0007695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85cda5c8-7fe7-46ab-83ba-60c3e09c6afd</td>\n",
       "      <td>d040caa5-7bd8-412c-aee2-3d9010b91821</td>\n",
       "      <td>12-Upper Intermediate</td>\n",
       "      <td>B2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>You're going to reply to Alice's blog post abo...</td>\n",
       "      <td>Hi Team,\\n\\nHope you are doing good and it was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      recordId  gpt4o_judge_score  nova_judge_score  llama3_judge_score  \\\n",
       "0  CALL0021715                0.0               0.0                 0.0   \n",
       "1  CALL0019540                0.0               0.0                 0.0   \n",
       "2  CALL0012294                2.0               NaN                 2.0   \n",
       "3  CALL0005503                4.0               4.0                 3.0   \n",
       "4  CALL0007695                0.0               0.0                 0.0   \n",
       "\n",
       "   majority_value  agreement_percentage                            writing_id  \\\n",
       "0             0.0            100.000000  295f109e-cfeb-48ce-82ba-ce6f2bf423d2   \n",
       "1             0.0            100.000000  8216749c-2498-4ff2-a241-d66d650689c0   \n",
       "2             2.0             66.666667  dd1cf6a9-aed3-47cf-b094-8a86988c79ae   \n",
       "3             4.0             66.666667  a2376227-7402-48ee-ad9a-f909f372ccd5   \n",
       "4             0.0            100.000000  85cda5c8-7fe7-46ab-83ba-60c3e09c6afd   \n",
       "\n",
       "                                task_id            level_title cefr_level  \\\n",
       "0  d491686c-8998-4062-bd13-27562deecdbe     Telecommunications         B1   \n",
       "1  2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7              Logistics         B1   \n",
       "2  9032f3ed-5da1-4efe-b4dd-c9470bb35d65      16-Upper Advanced         C2   \n",
       "3  493dffa9-7513-4408-be4b-5b010a1bf051  10-Upper Intermediate         B2   \n",
       "4  d040caa5-7bd8-412c-aee2-3d9010b91821  12-Upper Intermediate         B2   \n",
       "\n",
       "   ef_level                              activity_instructions  \\\n",
       "0       NaN  A client who is thinking about expanding into ...   \n",
       "1       NaN  Now write your counter-proposal. Remember to a...   \n",
       "2      16.0  Read an email from your friend and give advice...   \n",
       "3      10.0  Your manager sent you an email, asking you to ...   \n",
       "4      12.0  You're going to reply to Alice's blog post abo...   \n",
       "\n",
       "                                  student_submission  \n",
       "0  N n n n n n. N nn n. N n n. N. NN n n n nn n n...  \n",
       "1  df f fws wf f wf wf wef we fw fw ef wef ew few...  \n",
       "2  Hey don give up so quickly, give time some tim...  \n",
       "3  Company Update Presentation Outline\\n\\nGreet e...  \n",
       "4  Hi Team,\\n\\nHope you are doing good and it was...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "df = pd.read_csv(\"data/coh_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_id\n",
       "cfb893f1-bee1-4894-99f3-43af284493a5    205\n",
       "5c44f26d-2f5c-4039-bcf5-378754d2fe4e    194\n",
       "81ee255e-d4fa-4f81-b77f-dfcf3b5cf541    187\n",
       "5e5c32c9-cdbe-4b4c-9e83-aa4b8b0ec26a    186\n",
       "7e62b6a6-8685-4a0b-b0b2-d902bed0c567    186\n",
       "                                       ... \n",
       "511ccd73-9db8-45e5-8f85-10e244841ce2     26\n",
       "9cf6c0c9-e87b-4e02-84c9-11266ea51d28     24\n",
       "178cfadd-4162-4b5b-aaee-97a1bdcd4d98     24\n",
       "e5780a8d-f6ef-4218-9d34-74e064c3e413     19\n",
       "d2196935-4153-4f5c-a1d2-dae31174ae55     17\n",
       "Name: count, Length: 161, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"task_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = (\n",
    "    \"Prompt Level: \" + df['level_title'].astype(str) +\n",
    "    \" [SEP] Prompt: \" + df['activity_instructions'] +\n",
    "    \" [SEP] Response: \" + df['student_submission']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordId</th>\n",
       "      <th>gpt4o_judge_score</th>\n",
       "      <th>nova_judge_score</th>\n",
       "      <th>llama3_judge_score</th>\n",
       "      <th>majority_value</th>\n",
       "      <th>agreement_percentage</th>\n",
       "      <th>writing_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>level_title</th>\n",
       "      <th>cefr_level</th>\n",
       "      <th>ef_level</th>\n",
       "      <th>activity_instructions</th>\n",
       "      <th>student_submission</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALL0021715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>295f109e-cfeb-48ce-82ba-ce6f2bf423d2</td>\n",
       "      <td>d491686c-8998-4062-bd13-27562deecdbe</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>B1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A client who is thinking about expanding into ...</td>\n",
       "      <td>N n n n n n. N nn n. N n n. N. NN n n n nn n n...</td>\n",
       "      <td>Prompt Level: Telecommunications [SEP] Prompt:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALL0019540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>8216749c-2498-4ff2-a241-d66d650689c0</td>\n",
       "      <td>2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>B1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Now write your counter-proposal. Remember to a...</td>\n",
       "      <td>df f fws wf f wf wf wef we fw fw ef wef ew few...</td>\n",
       "      <td>Prompt Level: Logistics [SEP] Prompt: Now writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CALL0012294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>dd1cf6a9-aed3-47cf-b094-8a86988c79ae</td>\n",
       "      <td>9032f3ed-5da1-4efe-b4dd-c9470bb35d65</td>\n",
       "      <td>16-Upper Advanced</td>\n",
       "      <td>C2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Read an email from your friend and give advice...</td>\n",
       "      <td>Hey don give up so quickly, give time some tim...</td>\n",
       "      <td>Prompt Level: 16-Upper Advanced [SEP] Prompt: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALL0005503</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>a2376227-7402-48ee-ad9a-f909f372ccd5</td>\n",
       "      <td>493dffa9-7513-4408-be4b-5b010a1bf051</td>\n",
       "      <td>10-Upper Intermediate</td>\n",
       "      <td>B2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Your manager sent you an email, asking you to ...</td>\n",
       "      <td>Company Update Presentation Outline\\n\\nGreet e...</td>\n",
       "      <td>Prompt Level: 10-Upper Intermediate [SEP] Prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALL0007695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85cda5c8-7fe7-46ab-83ba-60c3e09c6afd</td>\n",
       "      <td>d040caa5-7bd8-412c-aee2-3d9010b91821</td>\n",
       "      <td>12-Upper Intermediate</td>\n",
       "      <td>B2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>You're going to reply to Alice's blog post abo...</td>\n",
       "      <td>Hi Team,\\n\\nHope you are doing good and it was...</td>\n",
       "      <td>Prompt Level: 12-Upper Intermediate [SEP] Prom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      recordId  gpt4o_judge_score  nova_judge_score  llama3_judge_score  \\\n",
       "0  CALL0021715                0.0               0.0                 0.0   \n",
       "1  CALL0019540                0.0               0.0                 0.0   \n",
       "2  CALL0012294                2.0               NaN                 2.0   \n",
       "3  CALL0005503                4.0               4.0                 3.0   \n",
       "4  CALL0007695                0.0               0.0                 0.0   \n",
       "\n",
       "   majority_value  agreement_percentage                            writing_id  \\\n",
       "0             0.0            100.000000  295f109e-cfeb-48ce-82ba-ce6f2bf423d2   \n",
       "1             0.0            100.000000  8216749c-2498-4ff2-a241-d66d650689c0   \n",
       "2             2.0             66.666667  dd1cf6a9-aed3-47cf-b094-8a86988c79ae   \n",
       "3             4.0             66.666667  a2376227-7402-48ee-ad9a-f909f372ccd5   \n",
       "4             0.0            100.000000  85cda5c8-7fe7-46ab-83ba-60c3e09c6afd   \n",
       "\n",
       "                                task_id            level_title cefr_level  \\\n",
       "0  d491686c-8998-4062-bd13-27562deecdbe     Telecommunications         B1   \n",
       "1  2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7              Logistics         B1   \n",
       "2  9032f3ed-5da1-4efe-b4dd-c9470bb35d65      16-Upper Advanced         C2   \n",
       "3  493dffa9-7513-4408-be4b-5b010a1bf051  10-Upper Intermediate         B2   \n",
       "4  d040caa5-7bd8-412c-aee2-3d9010b91821  12-Upper Intermediate         B2   \n",
       "\n",
       "   ef_level                              activity_instructions  \\\n",
       "0       NaN  A client who is thinking about expanding into ...   \n",
       "1       NaN  Now write your counter-proposal. Remember to a...   \n",
       "2      16.0  Read an email from your friend and give advice...   \n",
       "3      10.0  Your manager sent you an email, asking you to ...   \n",
       "4      12.0  You're going to reply to Alice's blog post abo...   \n",
       "\n",
       "                                  student_submission  \\\n",
       "0  N n n n n n. N nn n. N n n. N. NN n n n nn n n...   \n",
       "1  df f fws wf f wf wf wef we fw fw ef wef ew few...   \n",
       "2  Hey don give up so quickly, give time some tim...   \n",
       "3  Company Update Presentation Outline\\n\\nGreet e...   \n",
       "4  Hi Team,\\n\\nHope you are doing good and it was...   \n",
       "\n",
       "                                                text  \n",
       "0  Prompt Level: Telecommunications [SEP] Prompt:...  \n",
       "1  Prompt Level: Logistics [SEP] Prompt: Now writ...  \n",
       "2  Prompt Level: 16-Upper Advanced [SEP] Prompt: ...  \n",
       "3  Prompt Level: 10-Upper Intermediate [SEP] Prom...  \n",
       "4  Prompt Level: 12-Upper Intermediate [SEP] Prom...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_id</th>\n",
       "      <th>level_title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt Level: Telecommunications [SEP] Prompt:...</td>\n",
       "      <td>d491686c-8998-4062-bd13-27562deecdbe</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prompt Level: Logistics [SEP] Prompt: Now writ...</td>\n",
       "      <td>2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prompt Level: 16-Upper Advanced [SEP] Prompt: ...</td>\n",
       "      <td>9032f3ed-5da1-4efe-b4dd-c9470bb35d65</td>\n",
       "      <td>16-Upper Advanced</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prompt Level: 10-Upper Intermediate [SEP] Prom...</td>\n",
       "      <td>493dffa9-7513-4408-be4b-5b010a1bf051</td>\n",
       "      <td>10-Upper Intermediate</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt Level: 12-Upper Intermediate [SEP] Prom...</td>\n",
       "      <td>d040caa5-7bd8-412c-aee2-3d9010b91821</td>\n",
       "      <td>12-Upper Intermediate</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Prompt Level: Telecommunications [SEP] Prompt:...   \n",
       "1  Prompt Level: Logistics [SEP] Prompt: Now writ...   \n",
       "2  Prompt Level: 16-Upper Advanced [SEP] Prompt: ...   \n",
       "3  Prompt Level: 10-Upper Intermediate [SEP] Prom...   \n",
       "4  Prompt Level: 12-Upper Intermediate [SEP] Prom...   \n",
       "\n",
       "                                task_id            level_title  label  \n",
       "0  d491686c-8998-4062-bd13-27562deecdbe     Telecommunications    0.0  \n",
       "1  2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7              Logistics    0.0  \n",
       "2  9032f3ed-5da1-4efe-b4dd-c9470bb35d65      16-Upper Advanced    2.0  \n",
       "3  493dffa9-7513-4408-be4b-5b010a1bf051  10-Upper Intermediate    4.0  \n",
       "4  d040caa5-7bd8-412c-aee2-3d9010b91821  12-Upper Intermediate    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"text\", \"task_id\", \"level_title\", \"majority_value\"]]\n",
    "df = df.rename(columns={'majority_value': 'label'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_id</th>\n",
       "      <th>level_title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt Level: Telecommunications [SEP] Prompt:...</td>\n",
       "      <td>d491686c-8998-4062-bd13-27562deecdbe</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prompt Level: Logistics [SEP] Prompt: Now writ...</td>\n",
       "      <td>2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prompt Level: 16-Upper Advanced [SEP] Prompt: ...</td>\n",
       "      <td>9032f3ed-5da1-4efe-b4dd-c9470bb35d65</td>\n",
       "      <td>16-Upper Advanced</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prompt Level: 10-Upper Intermediate [SEP] Prom...</td>\n",
       "      <td>493dffa9-7513-4408-be4b-5b010a1bf051</td>\n",
       "      <td>10-Upper Intermediate</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt Level: 12-Upper Intermediate [SEP] Prom...</td>\n",
       "      <td>d040caa5-7bd8-412c-aee2-3d9010b91821</td>\n",
       "      <td>12-Upper Intermediate</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Prompt Level: Telecommunications [SEP] Prompt:...   \n",
       "1  Prompt Level: Logistics [SEP] Prompt: Now writ...   \n",
       "2  Prompt Level: 16-Upper Advanced [SEP] Prompt: ...   \n",
       "3  Prompt Level: 10-Upper Intermediate [SEP] Prom...   \n",
       "4  Prompt Level: 12-Upper Intermediate [SEP] Prom...   \n",
       "\n",
       "                                task_id            level_title  label  \n",
       "0  d491686c-8998-4062-bd13-27562deecdbe     Telecommunications    0.0  \n",
       "1  2e74edfb-ed33-4ca0-b50c-d5c0ebda76b7              Logistics    0.0  \n",
       "2  9032f3ed-5da1-4efe-b4dd-c9470bb35d65      16-Upper Advanced    2.0  \n",
       "3  493dffa9-7513-4408-be4b-5b010a1bf051  10-Upper Intermediate    4.0  \n",
       "4  d040caa5-7bd8-412c-aee2-3d9010b91821  12-Upper Intermediate    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the mapping to the 'labels' column\n",
    "#df['label'] = df['label'].map(label_mapping)\n",
    "df.dropna(subset=['label', 'text'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "5.0    5302\n",
       "4.0    4056\n",
       "0.0    3881\n",
       "2.0    2699\n",
       "3.0    1925\n",
       "1.0    1048\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'task_id', 'level_title', 'label'],\n",
       "    num_rows: 18911\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c735be803084584afcc18649a824527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/18911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'task_id', 'level_title', 'label'],\n",
       "        num_rows: 15128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'task_id', 'level_title', 'label'],\n",
       "        num_rows: 1891\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'task_id', 'level_title', 'label'],\n",
       "        num_rows: 1892\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Value, Sequence\n",
    "new_features = ds.features.copy()\n",
    "new_features[\"label\"] = ClassLabel(names=[0, 1, 2, 3, 4, 5])\n",
    "ds = ds.cast(new_features)\n",
    "\n",
    "# Step 1: Initial train/test split with stratification\n",
    "train_test_ds = ds.train_test_split(test_size=0.20, seed=20)\n",
    "\n",
    "# Step 2: Split the test set into half test, half validation\n",
    "test_valid_split = train_test_ds['test'].train_test_split(test_size=0.5, seed=20)\n",
    "\n",
    "# Step 3: Combine everything into a single DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_test_ds['train'],\n",
    "    'test': test_valid_split['train'],    # This becomes the test set\n",
    "    'validation': test_valid_split['test']  # This becomes the validation set\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: Counter({5: 4242, 4: 3200, 0: 3119, 2: 2161, 3: 1561, 1: 845})\n",
      "Test label counts: Counter({5: 539, 4: 416, 0: 373, 2: 275, 3: 174, 1: 114})\n",
      "Validation label counts: Counter({5: 521, 4: 440, 0: 389, 2: 263, 3: 190, 1: 89})\n"
     ]
    }
   ],
   "source": [
    "# Verify label distribution\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Train label counts:\", Counter(ds['train']['label']))\n",
    "print(\"Test label counts:\", Counter(ds['test']['label']))\n",
    "print(\"Validation label counts:\", Counter(ds['validation']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Fonction utilitaire pour sauvegarder un split en JSONL\n",
    "def save_split_to_jsonl(dataset_split, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for record in dataset_split:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Sauvegarde des trois splits\n",
    "save_split_to_jsonl(ds['train'], 'data/train.jsonl')\n",
    "save_split_to_jsonl(ds['test'], 'data/test.jsonl')\n",
    "save_split_to_jsonl(ds['validation'], 'data/validation.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d02b71c3fc4c009315935c022ecf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score, classification_report\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  # Convertir les logits en classes prédictes\n",
    "\n",
    "    # 🎯 Exactitude (Accuracy)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    # 🎯 Précision, Rappel et F1-score (pondérés)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    # 🎯 Score de Cohen's Kappa (pondéré)\n",
    "    cohen_kappa = cohen_kappa_score(labels, predictions, weights=\"quadratic\")\n",
    "\n",
    "    # 🎯 Corrélation de Pearson\n",
    "    pearson_corr, _ = pearsonr(labels, predictions)  # Retourne (coef, p-valeur), on garde seulement coef\n",
    "\n",
    "     # 🎯 Classification Report\n",
    "    class_report = classification_report(labels, predictions, output_dict=True)  # Get a dictionary of the report\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"cohen_kappa\": cohen_kappa,\n",
    "        \"pearson_corr\": pearson_corr,\n",
    "        \"classification_report\": class_report  # Add classification report to the return\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505ff55774594cb3a43e25e631d67e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a640fca5d34c87944561fa4da0aa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fab9dd694843748a93459ab6bfbae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'task_id', 'level_title', 'label'],\n",
      "        num_rows: 15128\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'task_id', 'level_title', 'label'],\n",
      "        num_rows: 1891\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text', 'task_id', 'level_title', 'label'],\n",
      "        num_rows: 1892\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Charger les fichiers JSONL en DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=\"data/train.jsonl\")[\"train\"],\n",
    "    \"test\": load_dataset(\"json\", data_files=\"data/test.jsonl\")[\"train\"],\n",
    "    \"valid\": load_dataset(\"json\", data_files=\"data/validation.jsonl\")[\"train\"]\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c0d86ad84749e584920e18d4d5c1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b7d262fd314ea7a430befcbc683453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c052a605c4564849abd91ec3704c58f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781fb5ec83884354b60cd4726ab8ac94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390e44a15b084a5599b7b39a6a4fedb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Prompt Level: 15-Advanced [SEP] Prompt: Read Ken's email and then email Josh and Ken a third option. Use your imagination. You can create any facts about your city that will support your proposal. Type in the input box. Write 150-200 words. [SEP] Response: Hello Josh, Ken,\\nI hope I can propose third option for you. It should be the best compromise of two options which you have considered so long.\\nThe idea is based on the fact that two third of our current shop is not seen by our customers. We have cloth making area and administration area in our shop. They can be moved to south end of the town while keeping showroom and measurement space in downtown.\\nThis kind of separation cannot be done in the past because our cloth making section needs to have close contact with measurement service section. However, thanks to IT development, now we can share measurement data immediately and we can start making cloth just after measurement, even though cloth making room is far from our showroom/ measurement space. \\nIn my rough calculation, it saves us $2000 a month while keeping our current quality and visibility.\\nI hope this proposal helps you to break through the challenge you are facing now.\\n\\nSincerely\\nEF student\",\n",
       " 'task_id': 'c7972fb8-c4a3-4fd8-8892-33804a60162e',\n",
       " 'level_title': '15-Advanced',\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 35396, 3320, 12183, 35, 508, 12, 44694, 646, 3388, 510, 742, 42944, 35, 9387, 9, 402, 7735, 6, 3571, 50, 6269, 14, 1102, 7, 47, 682, 6, 8, 6190, 24, 11, 41, 1047, 7, 10, 1441, 4, 5293, 24, 2679, 8, 42690, 6, 8, 2145, 7, 1649, 13, 44811, 5135, 4, 7773, 11, 5, 8135, 2233, 4, 21062, 3982, 12, 2619, 1617, 4, 646, 3388, 510, 742, 19121, 35, 12191, 1560, 6, 50118, 50118, 1185, 351, 75, 679, 99, 1102, 7, 162, 94, 363, 328, 38, 21, 11963, 15, 5, 26711, 2494, 10, 1569, 137, 3630, 77, 38, 1317, 10, 7735, 6496, 31, 20806, 4, 38, 34576, 89, 21, 5907, 1207, 11, 5, 2853, 1553, 27699, 6, 98, 38, 1276, 7, 8861, 24, 4, 125, 172, 6, 10, 92, 543, 2058, 2369, 37747, 162, 4, 38, 1224, 160, 5, 30016, 6, 342, 15, 127, 3581, 268, 8, 3203, 62, 7, 5, 371, 1929, 4, 38, 6536, 15, 5, 1883, 142, 24, 21, 628, 615, 7, 32366, 5670, 1493, 4, 440, 1948, 4, 38, 1381, 7, 1224, 15, 5, 1109, 9, 5, 14106, 6, 53, 24, 399, 75, 173, 4, 22, 713, 1068, 16, 1158, 7, 32580, 225, 162, 1297, 38, 802, 4, 38, 6536, 456, 6, 117, 1948, 6, 117, 27903, 31, 5, 1025, 9, 5, 790, 4, 38, 1276, 6, 172, 6, 7, 1656, 124, 184, 77, 38, 2967, 5, 1219, 9, 167, 4100, 23814, 4428, 35, 9014, 6, 5, 6269, 8, 6664, 18845, 3418, 4758, 9, 5, 12258, 4, 38, 21, 1341, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_test = tokenizer(dataset[\"train\"][1][\"text\"], max_length=256, truncation=True)\n",
    "tok_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05c0e2efe13412dbb21d84c8a8e832a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a85e729aef48ee83352296b2f4cc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9dfc05eea24d139c386be126355948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "tokenized_valid = dataset[\"valid\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = set(dataset['train']['label'])\n",
    "num_labels = len(unique_labels)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c914264ce74e45b2a4f4a6a3f7dbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-large\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"../../../model_saved/roberta-large-ft-acc-writing-task-augmented\",\n",
    "    eval_strategy=\"steps\",  # Évaluation aux mêmes intervalles que la sauvegarde\n",
    "    save_strategy=\"steps\",  # Sauvegarde tous les 500 steps\n",
    "    save_steps=200,\n",
    "    eval_steps=200,  # ⚠ IMPORTANT : Évaluation aux mêmes steps\n",
    "    save_total_limit=4,  # Ne garde que 4 checkpoints max\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\", \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topic = dataset[\"valid\"][\"task_id\"]\n",
    "list_t_set = set(list_topic)\n",
    "unique_t = (list(list_t_set))\n",
    "\n",
    "list_level = dataset[\"valid\"][\"level_title\"]\n",
    "list_l_set = set(list_level)\n",
    "unique_l = (list(list_l_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_r = []\n",
    "\n",
    "# Assuming 'unique_t' is a list of unique item_ids and 'trainer' is already defined\n",
    "for t in unique_t:  # Iterate over the first item in unique_t\n",
    "    sub_ds = tokenized_valid.filter(lambda example: example['task_id'] == t)\n",
    "    # Get predictions using the trainer\n",
    "    predictions = trainer.predict(sub_ds)\n",
    "    # Raw output logits (size [batch_size, num_classes])\n",
    "    outputs = predictions.predictions\n",
    "    # Convert logits to predicted class labels (taking the argmax across the classes)\n",
    "    predicted_labels = np.argmax(outputs, axis=-1)\n",
    "    ref_label = predictions.label_ids\n",
    "    # Print or save the predicted classes (this will be a numpy array with the predicted class indices)\n",
    "    ck = round(cohen_kappa_score(predicted_labels, ref_label, weights=\"quadratic\"), 2)  \n",
    "    pearson_corr, _ = pearsonr(ref_label, predicted_labels)\n",
    "    accuracy = accuracy_score(ref_label, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ref_label, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    r = {\n",
    "        \"task_id\": t,\n",
    "        \"level_title\": sub_ds[\"level_title\"][0],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"ck\": ck,\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"n_samples\": len(sub_ds)\n",
    "    }\n",
    "    list_r.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_r_level = []\n",
    "\n",
    "# Assuming 'unique_t' is a list of unique item_ids and 'trainer' is already defined\n",
    "for l in unique_l:  # Iterate over the first item in unique_t\n",
    "    sub_ds = tokenized_valid.filter(lambda example: example['level_title'] == l)\n",
    "    # Get predictions using the trainer\n",
    "    predictions = trainer.predict(sub_ds)\n",
    "    # Raw output logits (size [batch_size, num_classes])\n",
    "    outputs = predictions.predictions\n",
    "    # Convert logits to predicted class labels (taking the argmax across the classes)\n",
    "    predicted_labels = np.argmax(outputs, axis=-1)\n",
    "    ref_label = predictions.label_ids\n",
    "    # Print or save the predicted classes (this will be a numpy array with the predicted class indices)\n",
    "    ck = round(cohen_kappa_score(predicted_labels, ref_label, weights=\"quadratic\"), 2)  \n",
    "    pearson_corr, _ = pearsonr(ref_label, predicted_labels)\n",
    "    accuracy = accuracy_score(ref_label, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ref_label, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    r = {\n",
    "        \"level_title\": sub_ds[\"level_title\"][0],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"ck\": ck,\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"n_samples\": len(sub_ds)\n",
    "    }\n",
    "    list_r_level.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_eval_results = pd.DataFrame(list_r, columns=[\"task_id\", \"level_title\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"ck\", \"pearson\", \"n_samples\"])\n",
    "df_eval_results.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_results.to_csv(\"result_eval_data_roberta_large_writing_task_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_eval_results_level = pd.DataFrame(list_r_level, columns=[\"level_title\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"ck\", \"pearson\", \"n_samples\"])\n",
    "df_eval_results_level.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_results_level.to_csv(\"result_eval_data_roberta_large_acc_by_level.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (24.2)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (4.25.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnxruntime) (1.14.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [onnxruntime]\u001b[0m [onnxruntime]\n",
      "\u001b[1A\u001b[2KSuccessfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnxruntime-1.16.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Chemin vers ton dossier contenant le .bin et le config.json\n",
    "model_dir = \"model_saved/roberta-large-ft-acc-writing-task-augmented/checkpoint-1800\"\n",
    "onnx_model_path = \"model_saved/roberta-large-ft-acc-writing-task-1800.onnx\"\n",
    "quantized_model_path = \"model_saved/roberta-large-ft-acc-writing-task-1800-quantized.onnx\"\n",
    "\n",
    "# === ÉTAPE 1 : Charger le modèle et tokenizer ===\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n",
    "model.eval()\n",
    "\n",
    "# === ÉTAPE 2 : Préparer un input fictif ===\n",
    "dummy_text = \"Texte d'exemple pour conversion ONNX\"\n",
    "inputs = tokenizer(dummy_text, return_tensors=\"pt\", padding=\"max_length\", max_length=32)\n",
    "\n",
    "# === ÉTAPE 3 : Exporter vers ONNX ===\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    },\n",
    "    opset_version=14  # ⬅️ change ici\n",
    ")\n",
    "\n",
    "print(f\"✅ Modèle exporté en ONNX : {onnx_model_path}\")\n",
    "\n",
    "# === ÉTAPE 4 : Quantization dynamique ===\n",
    "quantize_dynamic(\n",
    "    model_input=onnx_model_path,\n",
    "    model_output=quantized_model_path,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(f\"✅ Modèle quantifié en ONNX : {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "onnx_session_quant = onnxruntime.InferenceSession(quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256  # Ajuste selon la taille maximale de ton modèle\n",
    "\n",
    "# Fonction d'inférence ONNX\n",
    "def onnx_infer(input_texts, onnx_model):\n",
    "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()\n",
    "    attention_mask = inputs[\"attention_mask\"].numpy()\n",
    "    onnx_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "    return onnx_outputs[0]\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_with_metrics(dataset, onnx_model, batch_size=16):\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    # tqdm pour afficher la progression sur les batches\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluation\"):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        texts = batch[\"text\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        logits = onnx_infer(texts, onnx_model)\n",
    "        all_logits.extend(logits)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_logits = np.array(all_logits)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # 🔥 Appliquer compute_metrics\n",
    "    metrics = compute_metrics((all_logits, all_labels))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = ds[\"validation\"]  # Ou \"valid\" selon ton dataset\n",
    "# === Lancer l'évaluation ===\n",
    "results = evaluate_with_metrics(valid_data, onnx_model=onnx_session)\n",
    "print(\"🎯 Evaluation Results ONNX :\")\n",
    "for k, v in results.items():\n",
    "    if k == \"classification_report\":\n",
    "        print(\"\\n📋 Classification Report :\")\n",
    "        for label, metrics in v.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = ds[\"validation\"]  # Ou \"valid\" selon ton dataset\n",
    "# === Lancer l'évaluation ===\n",
    "results = evaluate_with_metrics(valid_data, onnx_model=onnx_session_quant)\n",
    "print(\"🎯 Evaluation Results ONNX :\")\n",
    "for k, v in results.items():\n",
    "    if k == \"classification_report\":\n",
    "        print(\"\\n📋 Classification Report :\")\n",
    "        for label, metrics in v.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX model uploaded to s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/model_1800_roberta_large.onnx\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    \"s3\"\n",
    ")\n",
    "\n",
    "# Define your bucket name and desired path in S3\n",
    "\n",
    "bucket_name = \"sagemaker-studio-oxs6vznjds\"\n",
    "\n",
    "s3_key = \"writing_task_models/accuracy/model_1800_roberta_large.onnx\"  # Change path as needed\n",
    "# Upload the ONNX file\n",
    "bucket_path = \"sagemaker-studio-oxs6vznjds\"\n",
    "quantized_model_path = \"model_saved/roberta-large-ft-acc-writing-task-1800.onnx\"\n",
    "\n",
    "s3.upload_file(quantized_model_path, bucket_path, s3_key)\n",
    "\n",
    "print(f\"✅ ONNX model uploaded to s3://{bucket_name}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Onnx from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define S3 bucket and model key\n",
    "bucket_name = 'sagemaker-studio-oxs6vznjds'\n",
    "model_key = 'writing_task_models/accuracy/model_1800_roberta_large.onnx'\n",
    "local_model_path = '/tmp/roberta-large-ft-acc-writing-task-1800.onnx'  # or wherever you want to save temporarily\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the ONNX model from S3 to local path\n",
    "s3.download_file(bucket_name, model_key, local_model_path)\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "session = ort.InferenceSession(local_model_path)\n",
    "\n",
    "print(\"ONNX model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = ds[\"validation\"]  # Ou \"valid\" selon ton dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 126/126 [20:57<00:00,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Evaluation Results ONNX :\n",
      "accuracy: 0.7571072319201995\n",
      "precision: 0.754690191285516\n",
      "recall: 0.7571072319201995\n",
      "f1: 0.7550283590999475\n",
      "cohen_kappa: 0.9127375223497928\n",
      "pearson_corr: 0.9133259543888621\n",
      "\n",
      "📋 Classification Report :\n",
      "0: {'precision': 0.9378378378378378, 'recall': 0.9353099730458221, 'f1-score': 0.9365721997300944, 'support': 371.0}\n",
      "1: {'precision': 0.5964912280701754, 'recall': 0.41975308641975306, 'f1-score': 0.4927536231884058, 'support': 81.0}\n",
      "2: {'precision': 0.7876344086021505, 'recall': 0.7610389610389611, 'f1-score': 0.774108322324967, 'support': 385.0}\n",
      "3: {'precision': 0.5753424657534246, 'recall': 0.5853658536585366, 'f1-score': 0.5803108808290155, 'support': 287.0}\n",
      "4: {'precision': 0.5757575757575758, 'recall': 0.5757575757575758, 'f1-score': 0.5757575757575758, 'support': 330.0}\n",
      "5: {'precision': 0.8321917808219178, 'recall': 0.8820326678765881, 'f1-score': 0.8563876651982378, 'support': 551.0}\n",
      "accuracy: 0.7571072319201995\n",
      "macro avg: {'precision': 0.7175425494738471, 'recall': 0.6932096862995394, 'f1-score': 0.7026483778380493, 'support': 2005.0}\n",
      "weighted avg: {'precision': 0.754690191285516, 'recall': 0.7571072319201995, 'f1-score': 0.7550283590999475, 'support': 2005.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_with_metrics(valid_data, onnx_model=session)\n",
    "print(\"🎯 Evaluation Results ONNX :\")\n",
    "for k, v in results.items():\n",
    "    if k == \"classification_report\":\n",
    "        print(\"\\n📋 Classification Report :\")\n",
    "        for label, metrics in v.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
