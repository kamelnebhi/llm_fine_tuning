{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "training_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "training_device\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate datasets transformers accelerate==1.9.0 wandb safetensors==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordId</th>\n",
       "      <th>gpt4o_judge_score</th>\n",
       "      <th>nova_judge_score</th>\n",
       "      <th>llama3_judge_score</th>\n",
       "      <th>majority_value</th>\n",
       "      <th>agreement_percentage</th>\n",
       "      <th>writing_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>level_title</th>\n",
       "      <th>cefr_level</th>\n",
       "      <th>ef_level</th>\n",
       "      <th>activity_instructions</th>\n",
       "      <th>student_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALL0015056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3e46481f-4bef-4900-b990-169da9305089</td>\n",
       "      <td>af3c3b87-9a8a-449e-b4f0-737d76275fb9</td>\n",
       "      <td>Meetings</td>\n",
       "      <td>B1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Your meeting is finishing, and your boss is re...</td>\n",
       "      <td>Your meeting is finishing, and your boss is re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALL0000241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>d3ca8e74-ec0f-41a1-8bc3-030edf392772</td>\n",
       "      <td>fa1e732b-7698-4b57-85da-907fc3660ec3</td>\n",
       "      <td>2-Beginner</td>\n",
       "      <td>A1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A new colleague is looking for a place to go f...</td>\n",
       "      <td>From_\\nToAll\\nHi, there.\\n\\nCan you help me? I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CALL0022001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>ad6bb8ac-07c2-415e-8195-19d3791f83d3</td>\n",
       "      <td>03251188-de56-444f-8971-e68e55719bc9</td>\n",
       "      <td>Research</td>\n",
       "      <td>B1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>You work for a market research firm. You take ...</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALL0022165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85dc4eb7-4a04-4142-aeb4-740d466bbad1</td>\n",
       "      <td>3bdca4ab-e3ee-4d24-bab1-4de708580e2e</td>\n",
       "      <td>Research</td>\n",
       "      <td>B1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Write a summary of the report in no more than ...</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALL0000146</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>57e620bf-efb2-49bf-9f36-ada83b46768d</td>\n",
       "      <td>b3285147-d9b6-42f7-9ffb-7482cd450db7</td>\n",
       "      <td>2-Beginner</td>\n",
       "      <td>A1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Write a paragraph about how your friend stays ...</td>\n",
       "      <td>My friend stays healthy and fit by exercising ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      recordId  gpt4o_judge_score  nova_judge_score  llama3_judge_score  \\\n",
       "0  CALL0015056                0.0               0.0                 0.0   \n",
       "1  CALL0000241                0.0               0.0                 0.0   \n",
       "2  CALL0022001                0.0               0.0                 0.0   \n",
       "3  CALL0022165                0.0               0.0                 0.0   \n",
       "4  CALL0000146                4.0               5.0                 5.0   \n",
       "\n",
       "   majority_value  agreement_percentage                            writing_id  \\\n",
       "0             0.0            100.000000  3e46481f-4bef-4900-b990-169da9305089   \n",
       "1             0.0            100.000000  d3ca8e74-ec0f-41a1-8bc3-030edf392772   \n",
       "2             0.0            100.000000  ad6bb8ac-07c2-415e-8195-19d3791f83d3   \n",
       "3             0.0            100.000000  85dc4eb7-4a04-4142-aeb4-740d466bbad1   \n",
       "4             5.0             66.666667  57e620bf-efb2-49bf-9f36-ada83b46768d   \n",
       "\n",
       "                                task_id level_title cefr_level  ef_level  \\\n",
       "0  af3c3b87-9a8a-449e-b4f0-737d76275fb9    Meetings         B1       8.0   \n",
       "1  fa1e732b-7698-4b57-85da-907fc3660ec3  2-Beginner         A1       2.0   \n",
       "2  03251188-de56-444f-8971-e68e55719bc9    Research         B1       8.0   \n",
       "3  3bdca4ab-e3ee-4d24-bab1-4de708580e2e    Research         B1       8.0   \n",
       "4  b3285147-d9b6-42f7-9ffb-7482cd450db7  2-Beginner         A1       2.0   \n",
       "\n",
       "                               activity_instructions  \\\n",
       "0  Your meeting is finishing, and your boss is re...   \n",
       "1  A new colleague is looking for a place to go f...   \n",
       "2  You work for a market research firm. You take ...   \n",
       "3  Write a summary of the report in no more than ...   \n",
       "4  Write a paragraph about how your friend stays ...   \n",
       "\n",
       "                                  student_submission  \n",
       "0  Your meeting is finishing, and your boss is re...  \n",
       "1  From_\\nToAll\\nHi, there.\\n\\nCan you help me? I...  \n",
       "2                                                  s  \n",
       "3                                                  G  \n",
       "4  My friend stays healthy and fit by exercising ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "median_map = {\n",
    "    \"A1\": 2,\n",
    "    \"A2\": 5,\n",
    "    \"B1\": 8,\n",
    "    \"B2\": 11,\n",
    "    \"C1\": 14,\n",
    "    \"C2\": 16\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"data/acc_data.csv\")\n",
    "\n",
    "df['ef_level'] = df.apply(lambda row: median_map[row['cefr_level']] if pd.isna(row['ef_level']) else row['ef_level'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_id\n",
       "cfb893f1-bee1-4894-99f3-43af284493a5    205\n",
       "5c44f26d-2f5c-4039-bcf5-378754d2fe4e    199\n",
       "bdd2883c-b71b-4fb7-a44c-a53e7bbf26f4    196\n",
       "4ce13bf1-4384-44a6-9ff7-5084e02f9e2d    195\n",
       "7e3ee056-1866-488e-88f3-f56af8d22613    193\n",
       "                                       ... \n",
       "e5780a8d-f6ef-4218-9d34-74e064c3e413     25\n",
       "178cfadd-4162-4b5b-aaee-97a1bdcd4d98     25\n",
       "9cf6c0c9-e87b-4e02-84c9-11266ea51d28     25\n",
       "511ccd73-9db8-45e5-8f85-10e244841ce2     23\n",
       "d2196935-4153-4f5c-a1d2-dae31174ae55     19\n",
       "Name: count, Length: 161, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"task_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = (\n",
    "    \"Prompt Level: \" + df['ef_level'].astype(str) +\n",
    "    \" [SEP] Prompt: \" + df['activity_instructions'] +\n",
    "    \" [SEP] Response: \" + df['student_submission']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordId</th>\n",
       "      <th>gpt4o_judge_score</th>\n",
       "      <th>nova_judge_score</th>\n",
       "      <th>llama3_judge_score</th>\n",
       "      <th>majority_value</th>\n",
       "      <th>agreement_percentage</th>\n",
       "      <th>writing_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>level_title</th>\n",
       "      <th>cefr_level</th>\n",
       "      <th>ef_level</th>\n",
       "      <th>activity_instructions</th>\n",
       "      <th>student_submission</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALL0015056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3e46481f-4bef-4900-b990-169da9305089</td>\n",
       "      <td>af3c3b87-9a8a-449e-b4f0-737d76275fb9</td>\n",
       "      <td>Meetings</td>\n",
       "      <td>B1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Your meeting is finishing, and your boss is re...</td>\n",
       "      <td>Your meeting is finishing, and your boss is re...</td>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: Your meeting i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALL0000241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>d3ca8e74-ec0f-41a1-8bc3-030edf392772</td>\n",
       "      <td>fa1e732b-7698-4b57-85da-907fc3660ec3</td>\n",
       "      <td>2-Beginner</td>\n",
       "      <td>A1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A new colleague is looking for a place to go f...</td>\n",
       "      <td>From_\\nToAll\\nHi, there.\\n\\nCan you help me? I...</td>\n",
       "      <td>Prompt Level: 2.0 [SEP] Prompt: A new colleagu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CALL0022001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>ad6bb8ac-07c2-415e-8195-19d3791f83d3</td>\n",
       "      <td>03251188-de56-444f-8971-e68e55719bc9</td>\n",
       "      <td>Research</td>\n",
       "      <td>B1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>You work for a market research firm. You take ...</td>\n",
       "      <td>s</td>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: You work for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALL0022165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85dc4eb7-4a04-4142-aeb4-740d466bbad1</td>\n",
       "      <td>3bdca4ab-e3ee-4d24-bab1-4de708580e2e</td>\n",
       "      <td>Research</td>\n",
       "      <td>B1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Write a summary of the report in no more than ...</td>\n",
       "      <td>G</td>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: Write a summar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALL0000146</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>57e620bf-efb2-49bf-9f36-ada83b46768d</td>\n",
       "      <td>b3285147-d9b6-42f7-9ffb-7482cd450db7</td>\n",
       "      <td>2-Beginner</td>\n",
       "      <td>A1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Write a paragraph about how your friend stays ...</td>\n",
       "      <td>My friend stays healthy and fit by exercising ...</td>\n",
       "      <td>Prompt Level: 2.0 [SEP] Prompt: Write a paragr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      recordId  gpt4o_judge_score  nova_judge_score  llama3_judge_score  \\\n",
       "0  CALL0015056                0.0               0.0                 0.0   \n",
       "1  CALL0000241                0.0               0.0                 0.0   \n",
       "2  CALL0022001                0.0               0.0                 0.0   \n",
       "3  CALL0022165                0.0               0.0                 0.0   \n",
       "4  CALL0000146                4.0               5.0                 5.0   \n",
       "\n",
       "   majority_value  agreement_percentage                            writing_id  \\\n",
       "0             0.0            100.000000  3e46481f-4bef-4900-b990-169da9305089   \n",
       "1             0.0            100.000000  d3ca8e74-ec0f-41a1-8bc3-030edf392772   \n",
       "2             0.0            100.000000  ad6bb8ac-07c2-415e-8195-19d3791f83d3   \n",
       "3             0.0            100.000000  85dc4eb7-4a04-4142-aeb4-740d466bbad1   \n",
       "4             5.0             66.666667  57e620bf-efb2-49bf-9f36-ada83b46768d   \n",
       "\n",
       "                                task_id level_title cefr_level  ef_level  \\\n",
       "0  af3c3b87-9a8a-449e-b4f0-737d76275fb9    Meetings         B1       8.0   \n",
       "1  fa1e732b-7698-4b57-85da-907fc3660ec3  2-Beginner         A1       2.0   \n",
       "2  03251188-de56-444f-8971-e68e55719bc9    Research         B1       8.0   \n",
       "3  3bdca4ab-e3ee-4d24-bab1-4de708580e2e    Research         B1       8.0   \n",
       "4  b3285147-d9b6-42f7-9ffb-7482cd450db7  2-Beginner         A1       2.0   \n",
       "\n",
       "                               activity_instructions  \\\n",
       "0  Your meeting is finishing, and your boss is re...   \n",
       "1  A new colleague is looking for a place to go f...   \n",
       "2  You work for a market research firm. You take ...   \n",
       "3  Write a summary of the report in no more than ...   \n",
       "4  Write a paragraph about how your friend stays ...   \n",
       "\n",
       "                                  student_submission  \\\n",
       "0  Your meeting is finishing, and your boss is re...   \n",
       "1  From_\\nToAll\\nHi, there.\\n\\nCan you help me? I...   \n",
       "2                                                  s   \n",
       "3                                                  G   \n",
       "4  My friend stays healthy and fit by exercising ...   \n",
       "\n",
       "                                                text  \n",
       "0  Prompt Level: 8.0 [SEP] Prompt: Your meeting i...  \n",
       "1  Prompt Level: 2.0 [SEP] Prompt: A new colleagu...  \n",
       "2  Prompt Level: 8.0 [SEP] Prompt: You work for a...  \n",
       "3  Prompt Level: 8.0 [SEP] Prompt: Write a summar...  \n",
       "4  Prompt Level: 2.0 [SEP] Prompt: Write a paragr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_id</th>\n",
       "      <th>ef_level</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: Your meeting i...</td>\n",
       "      <td>af3c3b87-9a8a-449e-b4f0-737d76275fb9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prompt Level: 2.0 [SEP] Prompt: A new colleagu...</td>\n",
       "      <td>fa1e732b-7698-4b57-85da-907fc3660ec3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: You work for a...</td>\n",
       "      <td>03251188-de56-444f-8971-e68e55719bc9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: Write a summar...</td>\n",
       "      <td>3bdca4ab-e3ee-4d24-bab1-4de708580e2e</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt Level: 2.0 [SEP] Prompt: Write a paragr...</td>\n",
       "      <td>b3285147-d9b6-42f7-9ffb-7482cd450db7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Prompt Level: 8.0 [SEP] Prompt: Your meeting i...   \n",
       "1  Prompt Level: 2.0 [SEP] Prompt: A new colleagu...   \n",
       "2  Prompt Level: 8.0 [SEP] Prompt: You work for a...   \n",
       "3  Prompt Level: 8.0 [SEP] Prompt: Write a summar...   \n",
       "4  Prompt Level: 2.0 [SEP] Prompt: Write a paragr...   \n",
       "\n",
       "                                task_id  ef_level  label  \n",
       "0  af3c3b87-9a8a-449e-b4f0-737d76275fb9       8.0    0.0  \n",
       "1  fa1e732b-7698-4b57-85da-907fc3660ec3       2.0    0.0  \n",
       "2  03251188-de56-444f-8971-e68e55719bc9       8.0    0.0  \n",
       "3  3bdca4ab-e3ee-4d24-bab1-4de708580e2e       8.0    0.0  \n",
       "4  b3285147-d9b6-42f7-9ffb-7482cd450db7       2.0    5.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"text\", \"task_id\", \"ef_level\", \"majority_value\"]]\n",
    "df = df.rename(columns={'majority_value': 'label'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_id</th>\n",
       "      <th>ef_level</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: Your meeting i...</td>\n",
       "      <td>af3c3b87-9a8a-449e-b4f0-737d76275fb9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prompt Level: 2.0 [SEP] Prompt: A new colleagu...</td>\n",
       "      <td>fa1e732b-7698-4b57-85da-907fc3660ec3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: You work for a...</td>\n",
       "      <td>03251188-de56-444f-8971-e68e55719bc9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prompt Level: 8.0 [SEP] Prompt: Write a summar...</td>\n",
       "      <td>3bdca4ab-e3ee-4d24-bab1-4de708580e2e</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt Level: 2.0 [SEP] Prompt: Write a paragr...</td>\n",
       "      <td>b3285147-d9b6-42f7-9ffb-7482cd450db7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Prompt Level: 8.0 [SEP] Prompt: Your meeting i...   \n",
       "1  Prompt Level: 2.0 [SEP] Prompt: A new colleagu...   \n",
       "2  Prompt Level: 8.0 [SEP] Prompt: You work for a...   \n",
       "3  Prompt Level: 8.0 [SEP] Prompt: Write a summar...   \n",
       "4  Prompt Level: 2.0 [SEP] Prompt: Write a paragr...   \n",
       "\n",
       "                                task_id  ef_level  label  \n",
       "0  af3c3b87-9a8a-449e-b4f0-737d76275fb9       8.0    0.0  \n",
       "1  fa1e732b-7698-4b57-85da-907fc3660ec3       2.0    0.0  \n",
       "2  03251188-de56-444f-8971-e68e55719bc9       8.0    0.0  \n",
       "3  3bdca4ab-e3ee-4d24-bab1-4de708580e2e       8.0    0.0  \n",
       "4  b3285147-d9b6-42f7-9ffb-7482cd450db7       2.0    5.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the mapping to the 'labels' column\n",
    "#df['label'] = df['label'].map(label_mapping)\n",
    "df.dropna(subset=['label', 'text'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "5.0    5682\n",
       "2.0    3726\n",
       "0.0    3662\n",
       "3.0    3018\n",
       "4.0    2985\n",
       "1.0     968\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'task_id', 'ef_level', 'label'],\n",
       "    num_rows: 20041\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050d2c63fff64c10940d380d008af8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/20041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'task_id', 'ef_level', 'label'],\n",
       "        num_rows: 16032\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'task_id', 'ef_level', 'label'],\n",
       "        num_rows: 2004\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'task_id', 'ef_level', 'label'],\n",
       "        num_rows: 2005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Value, Sequence\n",
    "new_features = ds.features.copy()\n",
    "new_features[\"label\"] = ClassLabel(names=[0, 1, 2, 3, 4, 5])\n",
    "ds = ds.cast(new_features)\n",
    "\n",
    "# Step 1: Initial train/test split with stratification\n",
    "train_test_ds = ds.train_test_split(test_size=0.20, seed=20)\n",
    "\n",
    "# Step 2: Split the test set into half test, half validation\n",
    "test_valid_split = train_test_ds['test'].train_test_split(test_size=0.5, seed=20)\n",
    "\n",
    "# Step 3: Combine everything into a single DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_test_ds['train'],\n",
    "    'test': test_valid_split['train'],    # This becomes the test set\n",
    "    'validation': test_valid_split['test']  # This becomes the validation set\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: Counter({5: 4579, 2: 2970, 0: 2910, 3: 2424, 4: 2374, 1: 775})\n",
      "Test label counts: Counter({5: 552, 0: 381, 2: 371, 3: 307, 4: 281, 1: 112})\n",
      "Validation label counts: Counter({5: 551, 2: 385, 0: 371, 4: 330, 3: 287, 1: 81})\n"
     ]
    }
   ],
   "source": [
    "# Verify label distribution\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Train label counts:\", Counter(ds['train']['label']))\n",
    "print(\"Test label counts:\", Counter(ds['test']['label']))\n",
    "print(\"Validation label counts:\", Counter(ds['validation']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Fonction utilitaire pour sauvegarder un split en JSONL\n",
    "def save_split_to_jsonl(dataset_split, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for record in dataset_split:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Sauvegarde des trois splits\n",
    "save_split_to_jsonl(ds['train'], 'data/train.jsonl')\n",
    "save_split_to_jsonl(ds['test'], 'data/test.jsonl')\n",
    "save_split_to_jsonl(ds['validation'], 'data/validation.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score, classification_report\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  # Convertir les logits en classes prédictes\n",
    "\n",
    "    # 🎯 Exactitude (Accuracy)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    # 🎯 Précision, Rappel et F1-score (pondérés)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    # 🎯 Score de Cohen's Kappa (pondéré)\n",
    "    cohen_kappa = cohen_kappa_score(labels, predictions, weights=\"quadratic\")\n",
    "\n",
    "    # 🎯 Corrélation de Pearson\n",
    "    pearson_corr, _ = pearsonr(labels, predictions)  # Retourne (coef, p-valeur), on garde seulement coef\n",
    "\n",
    "     # 🎯 Classification Report\n",
    "    class_report = classification_report(labels, predictions, output_dict=True)  # Get a dictionary of the report\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"cohen_kappa\": cohen_kappa,\n",
    "        \"pearson_corr\": pearson_corr,\n",
    "        \"classification_report\": class_report  # Add classification report to the return\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5d4dad3a844bf0b00b23940bc442c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f402286c740245f6b30c7c6f6dbb0a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4dd518676f40319253d015929e33df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'task_id', 'ef_level', 'label'],\n",
      "        num_rows: 16032\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'task_id', 'ef_level', 'label'],\n",
      "        num_rows: 2004\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text', 'task_id', 'ef_level', 'label'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Charger les fichiers JSONL en DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=\"data/train.jsonl\")[\"train\"],\n",
    "    \"test\": load_dataset(\"json\", data_files=\"data/test.jsonl\")[\"train\"],\n",
    "    \"valid\": load_dataset(\"json\", data_files=\"data/validation.jsonl\")[\"train\"]\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_test = tokenizer(dataset[\"train\"][1][\"text\"], max_length=256, truncation=True)\n",
    "tok_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "tokenized_valid = dataset[\"valid\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(dataset['train']['label'])\n",
    "num_labels = len(unique_labels)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-large\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"../../../model_saved/roberta-large-ft-acc-writing-task-augmented\",\n",
    "    eval_strategy=\"steps\",  # Évaluation aux mêmes intervalles que la sauvegarde\n",
    "    save_strategy=\"steps\",  # Sauvegarde tous les 500 steps\n",
    "    save_steps=200,\n",
    "    eval_steps=200,  # ⚠ IMPORTANT : Évaluation aux mêmes steps\n",
    "    save_total_limit=4,  # Ne garde que 4 checkpoints max\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\", \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topic = dataset[\"valid\"][\"task_id\"]\n",
    "list_t_set = set(list_topic)\n",
    "unique_t = (list(list_t_set))\n",
    "\n",
    "list_level = dataset[\"valid\"][\"level_title\"]\n",
    "list_l_set = set(list_level)\n",
    "unique_l = (list(list_l_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_r = []\n",
    "\n",
    "# Assuming 'unique_t' is a list of unique item_ids and 'trainer' is already defined\n",
    "for t in unique_t:  # Iterate over the first item in unique_t\n",
    "    sub_ds = tokenized_valid.filter(lambda example: example['task_id'] == t)\n",
    "    # Get predictions using the trainer\n",
    "    predictions = trainer.predict(sub_ds)\n",
    "    # Raw output logits (size [batch_size, num_classes])\n",
    "    outputs = predictions.predictions\n",
    "    # Convert logits to predicted class labels (taking the argmax across the classes)\n",
    "    predicted_labels = np.argmax(outputs, axis=-1)\n",
    "    ref_label = predictions.label_ids\n",
    "    # Print or save the predicted classes (this will be a numpy array with the predicted class indices)\n",
    "    ck = round(cohen_kappa_score(predicted_labels, ref_label, weights=\"quadratic\"), 2)  \n",
    "    pearson_corr, _ = pearsonr(ref_label, predicted_labels)\n",
    "    accuracy = accuracy_score(ref_label, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ref_label, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    r = {\n",
    "        \"task_id\": t,\n",
    "        \"level_title\": sub_ds[\"level_title\"][0],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"ck\": ck,\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"n_samples\": len(sub_ds)\n",
    "    }\n",
    "    list_r.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_r_level = []\n",
    "\n",
    "# Assuming 'unique_t' is a list of unique item_ids and 'trainer' is already defined\n",
    "for l in unique_l:  # Iterate over the first item in unique_t\n",
    "    sub_ds = tokenized_valid.filter(lambda example: example['level_title'] == l)\n",
    "    # Get predictions using the trainer\n",
    "    predictions = trainer.predict(sub_ds)\n",
    "    # Raw output logits (size [batch_size, num_classes])\n",
    "    outputs = predictions.predictions\n",
    "    # Convert logits to predicted class labels (taking the argmax across the classes)\n",
    "    predicted_labels = np.argmax(outputs, axis=-1)\n",
    "    ref_label = predictions.label_ids\n",
    "    # Print or save the predicted classes (this will be a numpy array with the predicted class indices)\n",
    "    ck = round(cohen_kappa_score(predicted_labels, ref_label, weights=\"quadratic\"), 2)  \n",
    "    pearson_corr, _ = pearsonr(ref_label, predicted_labels)\n",
    "    accuracy = accuracy_score(ref_label, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ref_label, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    r = {\n",
    "        \"level_title\": sub_ds[\"level_title\"][0],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"ck\": ck,\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"n_samples\": len(sub_ds)\n",
    "    }\n",
    "    list_r_level.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_eval_results = pd.DataFrame(list_r, columns=[\"task_id\", \"level_title\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"ck\", \"pearson\", \"n_samples\"])\n",
    "df_eval_results.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_results.to_csv(\"result_eval_data_roberta_large_writing_task_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_eval_results_level = pd.DataFrame(list_r_level, columns=[\"level_title\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"ck\", \"pearson\", \"n_samples\"])\n",
    "df_eval_results_level.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_results_level.to_csv(\"result_eval_data_roberta_large_acc_by_level.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c82a9ebdc84e81965ced5d4888bde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad9f6a152a6409a81dc601b075d2c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546a5107b1ca49c4a7507bfeefcee053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9925a74412a4c638151de28d3f5349a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb4e05e6b274bc3a744e1cb18c07c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle exporté en ONNX : model_saved/roberta-large-ft-acc-writing-task-1800.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.11/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.12/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.12/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.13/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.13/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.14/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.14/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.15/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.15/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.16/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.16/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.17/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.17/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.18/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.18/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.19/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.19/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.20/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.20/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.21/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.21/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.22/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.22/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.23/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/roberta/encoder/layer.23/attention/self/MatMul_1]\n",
      "✅ Modèle quantifié en ONNX : model_saved/roberta-large-ft-acc-writing-task-1800-quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Chemin vers ton dossier contenant le .bin et le config.json\n",
    "#model_dir = \"model_saved/roberta-large-ft-acc-writing-task-augmented/checkpoint-1800\"\n",
    "model_dir=\"/tmp/tmpdxu3_htb\"\n",
    "onnx_model_path = \"model_saved/roberta-large-ft-acc-writing-task-1800.onnx\"\n",
    "quantized_model_path = \"model_saved/roberta-large-ft-acc-writing-task-1800-quantized.onnx\"\n",
    "\n",
    "# === ÉTAPE 1 : Charger le modèle et tokenizer ===\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n",
    "model.eval()\n",
    "\n",
    "# === ÉTAPE 2 : Préparer un input fictif ===\n",
    "dummy_text = \"Texte d'exemple pour conversion ONNX\"\n",
    "inputs = tokenizer(dummy_text, return_tensors=\"pt\", padding=\"max_length\", max_length=256)\n",
    "\n",
    "# === ÉTAPE 3 : Exporter vers ONNX ===\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    },\n",
    "    opset_version=14  # ⬅️ change ici\n",
    ")\n",
    "\n",
    "print(f\"✅ Modèle exporté en ONNX : {onnx_model_path}\")\n",
    "\n",
    "# === ÉTAPE 4 : Quantization dynamique ===\n",
    "quantize_dynamic(\n",
    "    model_input=onnx_model_path,\n",
    "    model_output=quantized_model_path,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(f\"✅ Modèle quantifié en ONNX : {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "onnx_session_quant = onnxruntime.InferenceSession(quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256  # Ajuste selon la taille maximale de ton modèle\n",
    "\n",
    "# Fonction d'inférence ONNX\n",
    "def onnx_infer(input_texts, onnx_model):\n",
    "    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()\n",
    "    attention_mask = inputs[\"attention_mask\"].numpy()\n",
    "    onnx_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "    return onnx_outputs[0]\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_with_metrics(dataset, onnx_model, batch_size=16):\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    # tqdm pour afficher la progression sur les batches\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluation\"):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        texts = batch[\"text\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        logits = onnx_infer(texts, onnx_model)\n",
    "        all_logits.extend(logits)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_logits = np.array(all_logits)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # 🔥 Appliquer compute_metrics\n",
    "    metrics = compute_metrics((all_logits, all_labels))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = ds[\"validation\"]  # Ou \"valid\" selon ton dataset\n",
    "# === Lancer l'évaluation ===\n",
    "results = evaluate_with_metrics(valid_data, onnx_model=onnx_session)\n",
    "print(\"🎯 Evaluation Results ONNX :\")\n",
    "for k, v in results.items():\n",
    "    if k == \"classification_report\":\n",
    "        print(\"\\n📋 Classification Report :\")\n",
    "        for label, metrics in v.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = ds[\"validation\"]  # Ou \"valid\" selon ton dataset\n",
    "# === Lancer l'évaluation ===\n",
    "results = evaluate_with_metrics(valid_data, onnx_model=onnx_session_quant)\n",
    "print(\"🎯 Evaluation Results ONNX :\")\n",
    "for k, v in results.items():\n",
    "    if k == \"classification_report\":\n",
    "        print(\"\\n📋 Classification Report :\")\n",
    "        for label, metrics in v.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX model uploaded to s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/model_1800_quantized_roberta_large.onnx\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    \"s3\"\n",
    ")\n",
    "\n",
    "# Define your bucket name and desired path in S3\n",
    "bucket_name = \"sagemaker-studio-oxs6vznjds\"\n",
    "\n",
    "s3_key = \"writing_task_models/accuracy/model_1800_quantized_roberta_large.onnx\"  # Change path as needed\n",
    "# Upload the ONNX file\n",
    "bucket_path = \"sagemaker-studio-oxs6vznjds\"\n",
    "quantized_model_path = \"model_saved/roberta-large-ft-acc-writing-task-1800-quantized.onnx\"\n",
    "\n",
    "s3.upload_file(quantized_model_path, bucket_path, s3_key)\n",
    "\n",
    "print(f\"✅ ONNX model uploaded to s3://{bucket_name}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Onnx from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define S3 bucket and model key\n",
    "bucket_name = 'sagemaker-studio-oxs6vznjds'\n",
    "model_key = 'writing_task_models/accuracy/model_1800_quantized_roberta_large.onnx'\n",
    "local_model_path = '/tmp/roberta-large-ft-acc-writing-task-1800-quant.onnx'  # or wherever you want to save temporarily\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the ONNX model from S3 to local path\n",
    "s3.download_file(bucket_name, model_key, local_model_path)\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "session = ort.InferenceSession(local_model_path)\n",
    "\n",
    "print(\"ONNX model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = ds[\"validation\"]  # Ou \"valid\" selon ton dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 126/126 [16:31<00:00,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Evaluation Results ONNX :\n",
      "accuracy: 0.7581047381546134\n",
      "precision: 0.7566740775830653\n",
      "recall: 0.7581047381546134\n",
      "f1: 0.756985630691875\n",
      "cohen_kappa: 0.9184130088587737\n",
      "pearson_corr: 0.9184389337067936\n",
      "\n",
      "📋 Classification Report :\n",
      "0: {'precision': 0.9400544959128065, 'recall': 0.9299191374663073, 'f1-score': 0.9349593495934959, 'support': 371.0}\n",
      "1: {'precision': 0.5074626865671642, 'recall': 0.41975308641975306, 'f1-score': 0.4594594594594595, 'support': 81.0}\n",
      "2: {'precision': 0.748780487804878, 'recall': 0.7974025974025974, 'f1-score': 0.7723270440251573, 'support': 385.0}\n",
      "3: {'precision': 0.5939597315436241, 'recall': 0.6167247386759582, 'f1-score': 0.6051282051282051, 'support': 287.0}\n",
      "4: {'precision': 0.5968253968253968, 'recall': 0.5696969696969697, 'f1-score': 0.5829457364341085, 'support': 330.0}\n",
      "5: {'precision': 0.8558394160583942, 'recall': 0.8511796733212341, 'f1-score': 0.8535031847133758, 'support': 551.0}\n",
      "accuracy: 0.7581047381546134\n",
      "macro avg: {'precision': 0.707153702452044, 'recall': 0.6974460338304701, 'f1-score': 0.7013871632256338, 'support': 2005.0}\n",
      "weighted avg: {'precision': 0.7566740775830653, 'recall': 0.7581047381546134, 'f1-score': 0.756985630691875, 'support': 2005.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_with_metrics(valid_data, onnx_model=session)\n",
    "print(\"🎯 Evaluation Results ONNX :\")\n",
    "for k, v in results.items():\n",
    "    if k == \"classification_report\":\n",
    "        print(\"\\n📋 Classification Report :\")\n",
    "        for label, metrics in v.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
