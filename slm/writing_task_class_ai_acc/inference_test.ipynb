{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier temporaire: /tmp/tmpdxu3_htb\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/config.json -> /tmp/tmpdxu3_htb/config.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/merges.txt -> /tmp/tmpdxu3_htb/merges.txt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/model.safetensors -> /tmp/tmpdxu3_htb/model.safetensors\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/optimizer.pt -> /tmp/tmpdxu3_htb/optimizer.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/rng_state.pth -> /tmp/tmpdxu3_htb/rng_state.pth\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/scaler.pt -> /tmp/tmpdxu3_htb/scaler.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/scheduler.pt -> /tmp/tmpdxu3_htb/scheduler.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/special_tokens_map.json -> /tmp/tmpdxu3_htb/special_tokens_map.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/tokenizer.json -> /tmp/tmpdxu3_htb/tokenizer.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/tokenizer_config.json -> /tmp/tmpdxu3_htb/tokenizer_config.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/trainer_state.json -> /tmp/tmpdxu3_htb/trainer_state.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/training_args.bin -> /tmp/tmpdxu3_htb/training_args.bin\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/vocab.json -> /tmp/tmpdxu3_htb/vocab.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tempfile\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "def load_roberta_from_s3(bucket_path):\n",
    "    # Séparer bucket et prefix\n",
    "    if \"/\" not in bucket_path:\n",
    "        raise ValueError(\"Le chemin doit être de la forme 'bucket/prefix'\")\n",
    "    bucket_name, prefix = bucket_path.split(\"/\", 1)\n",
    "    prefix = prefix.rstrip(\"/\")  # enlever / final si présent\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Dossier temporaire: {tmp_dir}\")\n",
    "\n",
    "    # Lister les fichiers dans le prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if \"Contents\" not in response:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé dans s3://{bucket_name}/{prefix}\")\n",
    "\n",
    "    # Télécharger tous les fichiers\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if key.endswith(\"/\"):\n",
    "            continue\n",
    "        local_path = os.path.join(tmp_dir, os.path.relpath(key, prefix))\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        print(f\"Téléchargement: s3://{bucket_name}/{key} -> {local_path}\")\n",
    "        s3.download_file(bucket_name, key, local_path)\n",
    "\n",
    "    # Charger modèle et tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(tmp_dir)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(tmp_dir)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# ===== Utilisation =====\n",
    "bucket_path = \"sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/\"\n",
    "\n",
    "tokenizer, model = load_roberta_from_s3(bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_inference(ef_level, activity_instructions, student_submission):\n",
    "    return (\n",
    "        f\"Prompt Level: {ef_level} [SEP] Prompt: {activity_instructions} [SEP] Response: {student_submission}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_json):\n",
    "    \"\"\"\n",
    "    input_json attendu :\n",
    "    {\n",
    "      \"answer\": \"...\",\n",
    "      \"prompt\": \"...\",\n",
    "      \"level\": \"...\"  # chaîne ou int\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Extraire les champs\n",
    "    ef_level = int(input_json[\"ef_level\"])\n",
    "    activity_instructions = input_json[\"activity_instructions\"]\n",
    "    student_submission = input_json[\"student_submission\"]\n",
    "    \n",
    "    # Formater le texte\n",
    "    formatted_text = format_text_inference(ef_level, activity_instructions, student_submission)\n",
    "    \n",
    "    # Tokenizer\n",
    "    inputs = tokenizer(formatted_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Prédiction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze()\n",
    "    predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "    mapped_score = map_score_linear(predicted_class)\n",
    "\n",
    "    \n",
    "    # Construire la sortie JSON\n",
    "    output = {\n",
    "        \"cefr_scoring\": predicted_class,\n",
    "        \"cefr_scoring_100\": mapped_score,\n",
    "        \"scorer\": {\n",
    "            \"version\": \"roberta_large_onnx_scorer\",\n",
    "            \"release\": \"0.3\"\n",
    "        }\n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n",
    "def map_score_linear(score):\n",
    "    evp_to_score = {\n",
    "        0: 17,\n",
    "        1: 33,\n",
    "        2: 50,\n",
    "        3: 67,\n",
    "        4: 83,\n",
    "        5: 100,\n",
    "    }\n",
    "    return evp_to_score.get(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cefr_scoring': 4, 'cefr_scoring_100': 83, 'scorer': {'version': 'roberta_large_onnx_scorer', 'release': '0.3'}}\n"
     ]
    }
   ],
   "source": [
    "example_input = {\n",
    "    \"ef_level\": 10,\n",
    "    \"activity_instructions\": \"Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible. \",\n",
    "    \"student_submission\": \"Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone’s working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300 000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000\"\n",
    "}\n",
    "\n",
    "result = inference(example_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnx inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define S3 bucket and model key\n",
    "bucket_name = 'sagemaker-studio-oxs6vznjds'\n",
    "model_key = 'writing_task_models/accuracy/model_1800_roberta_large.onnx'\n",
    "local_model_path = '/tmp/roberta-large-ft-acc-writing-task-1800.onnx'  # or wherever you want to save temporarily\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the ONNX model from S3 to local path\n",
    "s3.download_file(bucket_name, model_key, local_model_path)\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "session = ort.InferenceSession(local_model_path)\n",
    "\n",
    "print(\"ONNX model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256  # Ajuste selon la taille maximale de ton modèle\n",
    "import torch.nn.functional as F  # pour softmax\n",
    "import numpy as np\n",
    "\n",
    "def inference(input_json, onnx_model):\n",
    "    ef_level = int(input_json[\"ef_level\"])\n",
    "    activity_instructions = input_json[\"activity_instructions\"]\n",
    "    student_submission = input_json[\"student_submission\"]\n",
    "    \n",
    "    formatted_text = format_text_inference(ef_level, activity_instructions, student_submission)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        formatted_text, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].cpu().numpy()\n",
    "    attention_mask = inputs[\"attention_mask\"].cpu().numpy()\n",
    "    onnx_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    \n",
    "    onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "    logits = onnx_outputs[0]\n",
    "    \n",
    "    predicted_class = int(np.argmax(logits, axis=1)[0])\n",
    "    \n",
    "    probs = F.softmax(torch.tensor(logits), dim=1).numpy().squeeze()\n",
    "    predicted_prob = float(probs[predicted_class])\n",
    "    \n",
    "    mapped_score = map_score_linear(predicted_class)\n",
    "    \n",
    "    output = {\n",
    "        \"cefr_scoring\": predicted_class,\n",
    "        \"cefr_scoring_100\": mapped_score,\n",
    "        \"predicted_probability\": round(predicted_prob, 2),\n",
    "        \"scorer\": {\n",
    "            \"version\": \"roberta_large_onnx_scorer\",\n",
    "            \"release\": \"0.1\"\n",
    "        }\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def map_score_linear(score):\n",
    "    evp_to_score = {\n",
    "        0: 17,\n",
    "        1: 33,\n",
    "        2: 50,\n",
    "        3: 67,\n",
    "        4: 83,\n",
    "        5: 100,\n",
    "    }\n",
    "    return evp_to_score.get(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cefr_scoring': 4, 'cefr_scoring_100': 83, 'predicted_probability': 0.88, 'scorer': {'version': 'roberta_large_onnx_scorer', 'release': '0.1'}}\n"
     ]
    }
   ],
   "source": [
    "example_input = {\n",
    "    \"ef_level\": 10,\n",
    "    \"activity_instructions\": \"Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible. \",\n",
    "    \"student_submission\": \"Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone’s working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300 000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000\"\n",
    "}\n",
    "\n",
    "result = inference(example_input, onnx_model=session)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE inference for multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# ── Constants (must match training) ───────────────────────────────────────────\n",
    "NUM_LABELS = 6\n",
    "TASK_NAMES = [\"accuracy\", \"coherence\", \"range\"]\n",
    "LABEL_NAMES = [f\"Score {i}\" for i in range(NUM_LABELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model definition (copy from training script) ──────────────────────────────\n",
    "class MultiTaskCrossEncoderRoberta(nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.shared_projection = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.head_accuracy  = nn.Linear(512, num_labels)\n",
    "        self.head_coherence = nn.Linear(512, num_labels)\n",
    "        self.head_range     = nn.Linear(512, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        cls = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).last_hidden_state[:, 0, :]\n",
    "\n",
    "        shared = self.shared_projection(cls)\n",
    "\n",
    "        logits = torch.cat([\n",
    "            self.head_accuracy(shared),\n",
    "            self.head_coherence(shared),\n",
    "            self.head_range(shared),\n",
    "        ], dim=-1)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=None, logits=logits)\n",
    "\n",
    "\n",
    "# ── Load model ────────────────────────────────────────────────────────────────\n",
    "def load_model(checkpoint_path: str, device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    checkpoint_path : dossier contenant pytorch_model.bin + tokenizer\n",
    "    \"\"\"\n",
    "    import os\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "    model = MultiTaskCrossEncoderRoberta(\n",
    "        model_name=\"FacebookAI/roberta-large\",\n",
    "        num_labels=NUM_LABELS,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "\n",
    "    weights_path = os.path.join(checkpoint_path, \"pytorch_model.bin\")\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"✅ Model loaded from {checkpoint_path}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ── Single inference ──────────────────────────────────────────────────────────\n",
    "def predict(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text_a: str,\n",
    "    text_b: str,\n",
    "    max_length: int = 512,\n",
    "    device: str = \"cpu\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    text_a : prompt formaté  (ex: \"Prompt Level: 8 Prompt: ... Response: ...\")\n",
    "    text_b : correction\n",
    "    Retourne un dict avec score prédit + proba par tâche.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text_a,\n",
    "        text_b,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids      = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits  = outputs.logits  # [1, 18]\n",
    "\n",
    "    results = {}\n",
    "    for i, task in enumerate(TASK_NAMES):\n",
    "        task_logits = logits[0, i * NUM_LABELS:(i + 1) * NUM_LABELS]\n",
    "        probs       = torch.softmax(task_logits, dim=-1).cpu().numpy()\n",
    "        pred_score  = int(np.argmax(probs))\n",
    "        confidence  = float(probs.max())\n",
    "\n",
    "        results[task] = {\n",
    "            \"predicted_score\": pred_score,\n",
    "            \"label\":           LABEL_NAMES[pred_score],\n",
    "            \"confidence\":      round(confidence, 4),\n",
    "            \"probabilities\":   {\n",
    "                LABEL_NAMES[j]: round(float(probs[j]), 4)\n",
    "                for j in range(NUM_LABELS)\n",
    "            },\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ── Pretty print ──────────────────────────────────────────────────────────────\n",
    "def print_results(results: dict):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INFERENCE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for task, info in results.items():\n",
    "        print(f\"\\n  [{task.upper()}]\")\n",
    "        print(f\"    Predicted score : {info['predicted_score']}  ({info['label']})\")\n",
    "        print(f\"    Confidence      : {info['confidence']:.2%}\")\n",
    "        print(\"    Probabilities   :\")\n",
    "        for label, prob in info[\"probabilities\"].items():\n",
    "            bar = \"█\" * int(prob * 30)\n",
    "            print(f\"      {label} : {prob:.4f}  {bar}\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n",
      "\n",
      "[text_a] Prompt Level: 8 Prompt: Describe your last holiday in detail. Response: Last summer I goes to Spain with my family. \n",
      "[text_b] Last summer I went to Spain with my family. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from model_saved/roberta-large-multitask-multitask_cross_encoder_gte60/checkpoint-600\n",
      "\n",
      "============================================================\n",
      "INFERENCE RESULTS\n",
      "============================================================\n",
      "\n",
      "  [ACCURACY]\n",
      "    Predicted score : 2  (Score 2)\n",
      "    Confidence      : 31.35%\n",
      "    Probabilities   :\n",
      "      Score 0 : 0.0442  █\n",
      "      Score 1 : 0.1986  █████\n",
      "      Score 2 : 0.3135  █████████\n",
      "      Score 3 : 0.3016  █████████\n",
      "      Score 4 : 0.1136  ███\n",
      "      Score 5 : 0.0286  \n",
      "\n",
      "  [COHERENCE]\n",
      "    Predicted score : 3  (Score 3)\n",
      "    Confidence      : 26.82%\n",
      "    Probabilities   :\n",
      "      Score 0 : 0.0703  ██\n",
      "      Score 1 : 0.1834  █████\n",
      "      Score 2 : 0.2669  ████████\n",
      "      Score 3 : 0.2682  ████████\n",
      "      Score 4 : 0.1637  ████\n",
      "      Score 5 : 0.0475  █\n",
      "\n",
      "  [RANGE]\n",
      "    Predicted score : 2  (Score 2)\n",
      "    Confidence      : 30.18%\n",
      "    Probabilities   :\n",
      "      Score 0 : 0.0783  ██\n",
      "      Score 1 : 0.2091  ██████\n",
      "      Score 2 : 0.3018  █████████\n",
      "      Score 3 : 0.2237  ██████\n",
      "      Score 4 : 0.1424  ████\n",
      "      Score 5 : 0.0446  █\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ── Main ──────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ── 1. Paramètres ─────────────────────────────────────────────────────\n",
    "    CHECKPOINT_PATH = \"model_saved/roberta-large-multitask-multitask_cross_encoder_gte60/checkpoint-600\"\n",
    "    DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    MAX_LENGTH      = 512   # utiliser la même valeur que l'entraînement\n",
    "\n",
    "    # ── 2. Exemple de test ────────────────────────────────────────────────\n",
    "    ef_level              = 8   # correspond à B1\n",
    "    activity_instructions = \"Describe your last holiday in detail.\"\n",
    "    student_submission    = \"Last summer I goes to Spain with my family. \"\n",
    "    correction            = \"Last summer I went to Spain with my family. \"\n",
    "\n",
    "    # Formatage identique à l'entraînement\n",
    "    text_a = (\n",
    "        f\"Prompt Level: {ef_level} \"\n",
    "        f\"Prompt: {activity_instructions} \"\n",
    "        f\"Response: {student_submission}\"\n",
    "    )\n",
    "    text_b = correction\n",
    "\n",
    "    print(f\"Device : {DEVICE}\")\n",
    "    print(f\"\\n[text_a] {text_a}\")\n",
    "    print(f\"[text_b] {text_b}\")\n",
    "\n",
    "    # ── 3. Chargement + inférence ─────────────────────────────────────────\n",
    "    model, tokenizer = load_model(CHECKPOINT_PATH, device=DEVICE)\n",
    "    results          = predict(model, tokenizer, text_a, text_b,\n",
    "                               max_length=MAX_LENGTH, device=DEVICE)\n",
    "    print_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
