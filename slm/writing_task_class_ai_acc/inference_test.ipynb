{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier temporaire: /tmp/tmpl2e7aqm0\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/config.json -> /tmp/tmpl2e7aqm0/config.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/merges.txt -> /tmp/tmpl2e7aqm0/merges.txt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/model.safetensors -> /tmp/tmpl2e7aqm0/model.safetensors\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/optimizer.pt -> /tmp/tmpl2e7aqm0/optimizer.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/rng_state.pth -> /tmp/tmpl2e7aqm0/rng_state.pth\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/scaler.pt -> /tmp/tmpl2e7aqm0/scaler.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/scheduler.pt -> /tmp/tmpl2e7aqm0/scheduler.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/special_tokens_map.json -> /tmp/tmpl2e7aqm0/special_tokens_map.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/tokenizer.json -> /tmp/tmpl2e7aqm0/tokenizer.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/tokenizer_config.json -> /tmp/tmpl2e7aqm0/tokenizer_config.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/trainer_state.json -> /tmp/tmpl2e7aqm0/trainer_state.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/training_args.bin -> /tmp/tmpl2e7aqm0/training_args.bin\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/vocab.json -> /tmp/tmpl2e7aqm0/vocab.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tempfile\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "def load_roberta_from_s3(bucket_path):\n",
    "    # Séparer bucket et prefix\n",
    "    if \"/\" not in bucket_path:\n",
    "        raise ValueError(\"Le chemin doit être de la forme 'bucket/prefix'\")\n",
    "    bucket_name, prefix = bucket_path.split(\"/\", 1)\n",
    "    prefix = prefix.rstrip(\"/\")  # enlever / final si présent\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Dossier temporaire: {tmp_dir}\")\n",
    "\n",
    "    # Lister les fichiers dans le prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if \"Contents\" not in response:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé dans s3://{bucket_name}/{prefix}\")\n",
    "\n",
    "    # Télécharger tous les fichiers\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if key.endswith(\"/\"):\n",
    "            continue\n",
    "        local_path = os.path.join(tmp_dir, os.path.relpath(key, prefix))\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        print(f\"Téléchargement: s3://{bucket_name}/{key} -> {local_path}\")\n",
    "        s3.download_file(bucket_name, key, local_path)\n",
    "\n",
    "    # Charger modèle et tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(tmp_dir)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(tmp_dir)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# ===== Utilisation =====\n",
    "bucket_path = \"sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/\"\n",
    "\n",
    "tokenizer, model = load_roberta_from_s3(bucket_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = {\n",
    "    \"ef_level\": 10,\n",
    "    \"activity_instructions\": \"Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible. \",\n",
    "    \"student_submission\": \"Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone’s working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300 000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt Level: 10 [SEP] Prompt: Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible.  [SEP] Response: Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone’s working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300\\xa0000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_text_inference(ef_level, activity_instructions, student_submission):\n",
    "    return (\n",
    "        f\"Prompt Level: {ef_level} [SEP] Prompt: {activity_instructions} [SEP] Response: {student_submission}\"\n",
    "    )\n",
    "\n",
    "formatted_text = format_text_inference(\n",
    "    example_input[\"ef_level\"],\n",
    "    example_input[\"activity_instructions\"],\n",
    "    example_input[\"student_submission\"]\n",
    ")\n",
    "\n",
    "formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 : 0.00\n",
      "Class 1 : 0.00\n",
      "Class 2 : 0.00\n",
      "Class 3 : 0.11\n",
      "Class 4 : 0.88\n",
      "Class 5 : 0.00\n",
      "\n",
      "Predicted Class : 4\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits  # tenseur [1, num_classes]\n",
    "\n",
    "# Calcul des probabilités avec softmax sur la dimension des classes\n",
    "probs = torch.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "# Affichage proba par classe\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"Class {i} : {p.item():.2f}\")\n",
    "\n",
    "# Classe prédite\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "print(f\"\\nPredicted Class : {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
