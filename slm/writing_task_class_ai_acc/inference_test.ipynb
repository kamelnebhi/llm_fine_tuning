{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier temporaire: /tmp/tmpdxu3_htb\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/config.json -> /tmp/tmpdxu3_htb/config.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/merges.txt -> /tmp/tmpdxu3_htb/merges.txt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/model.safetensors -> /tmp/tmpdxu3_htb/model.safetensors\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/optimizer.pt -> /tmp/tmpdxu3_htb/optimizer.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/rng_state.pth -> /tmp/tmpdxu3_htb/rng_state.pth\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/scaler.pt -> /tmp/tmpdxu3_htb/scaler.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/scheduler.pt -> /tmp/tmpdxu3_htb/scheduler.pt\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/special_tokens_map.json -> /tmp/tmpdxu3_htb/special_tokens_map.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/tokenizer.json -> /tmp/tmpdxu3_htb/tokenizer.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/tokenizer_config.json -> /tmp/tmpdxu3_htb/tokenizer_config.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/trainer_state.json -> /tmp/tmpdxu3_htb/trainer_state.json\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/training_args.bin -> /tmp/tmpdxu3_htb/training_args.bin\n",
      "Téléchargement: s3://sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/vocab.json -> /tmp/tmpdxu3_htb/vocab.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tempfile\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "def load_roberta_from_s3(bucket_path):\n",
    "    # Séparer bucket et prefix\n",
    "    if \"/\" not in bucket_path:\n",
    "        raise ValueError(\"Le chemin doit être de la forme 'bucket/prefix'\")\n",
    "    bucket_name, prefix = bucket_path.split(\"/\", 1)\n",
    "    prefix = prefix.rstrip(\"/\")  # enlever / final si présent\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Dossier temporaire: {tmp_dir}\")\n",
    "\n",
    "    # Lister les fichiers dans le prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if \"Contents\" not in response:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé dans s3://{bucket_name}/{prefix}\")\n",
    "\n",
    "    # Télécharger tous les fichiers\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if key.endswith(\"/\"):\n",
    "            continue\n",
    "        local_path = os.path.join(tmp_dir, os.path.relpath(key, prefix))\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        print(f\"Téléchargement: s3://{bucket_name}/{key} -> {local_path}\")\n",
    "        s3.download_file(bucket_name, key, local_path)\n",
    "\n",
    "    # Charger modèle et tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(tmp_dir)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(tmp_dir)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# ===== Utilisation =====\n",
    "bucket_path = \"sagemaker-studio-oxs6vznjds/writing_task_models/accuracy/checkpoint-1800/\"\n",
    "\n",
    "tokenizer, model = load_roberta_from_s3(bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_inference(ef_level, activity_instructions, student_submission):\n",
    "    return (\n",
    "        f\"Prompt Level: {ef_level} [SEP] Prompt: {activity_instructions} [SEP] Response: {student_submission}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_json):\n",
    "    \"\"\"\n",
    "    input_json attendu :\n",
    "    {\n",
    "      \"answer\": \"...\",\n",
    "      \"prompt\": \"...\",\n",
    "      \"level\": \"...\"  # chaîne ou int\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Extraire les champs\n",
    "    ef_level = int(input_json[\"ef_level\"])\n",
    "    activity_instructions = input_json[\"activity_instructions\"]\n",
    "    student_submission = input_json[\"student_submission\"]\n",
    "    \n",
    "    # Formater le texte\n",
    "    formatted_text = format_text_inference(ef_level, activity_instructions, student_submission)\n",
    "    \n",
    "    # Tokenizer\n",
    "    inputs = tokenizer(formatted_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Prédiction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze()\n",
    "    predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "    mapped_score = map_score_linear(predicted_class)\n",
    "\n",
    "    \n",
    "    # Construire la sortie JSON\n",
    "    output = {\n",
    "        \"cefr_scoring\": predicted_class,\n",
    "        \"cefr_scoring_100\": mapped_score,\n",
    "        \"scorer\": {\n",
    "            \"version\": \"roberta_large_onnx_scorer\",\n",
    "            \"release\": \"0.3\"\n",
    "        }\n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n",
    "def map_score_linear(score):\n",
    "    evp_to_score = {\n",
    "        0: 17,\n",
    "        1: 33,\n",
    "        2: 50,\n",
    "        3: 67,\n",
    "        4: 83,\n",
    "        5: 100,\n",
    "    }\n",
    "    return evp_to_score.get(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cefr_scoring': 4, 'cefr_scoring_100': 83, 'scorer': {'version': 'roberta_large_onnx_scorer', 'release': '0.3'}}\n"
     ]
    }
   ],
   "source": [
    "example_input = {\n",
    "    \"ef_level\": 10,\n",
    "    \"activity_instructions\": \"Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible. \",\n",
    "    \"student_submission\": \"Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone’s working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300 000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000\"\n",
    "}\n",
    "\n",
    "result = inference(example_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnx inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define S3 bucket and model key\n",
    "bucket_name = 'sagemaker-studio-oxs6vznjds'\n",
    "model_key = 'writing_task_models/accuracy/model_1800_roberta_large.onnx'\n",
    "local_model_path = '/tmp/roberta-large-ft-acc-writing-task-1800.onnx'  # or wherever you want to save temporarily\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the ONNX model from S3 to local path\n",
    "s3.download_file(bucket_name, model_key, local_model_path)\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "session = ort.InferenceSession(local_model_path)\n",
    "\n",
    "print(\"ONNX model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256  # Ajuste selon la taille maximale de ton modèle\n",
    "import torch.nn.functional as F  # pour softmax\n",
    "import numpy as np\n",
    "\n",
    "def inference(input_json, onnx_model):\n",
    "    ef_level = int(input_json[\"ef_level\"])\n",
    "    activity_instructions = input_json[\"activity_instructions\"]\n",
    "    student_submission = input_json[\"student_submission\"]\n",
    "    \n",
    "    formatted_text = format_text_inference(ef_level, activity_instructions, student_submission)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        formatted_text, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].cpu().numpy()\n",
    "    attention_mask = inputs[\"attention_mask\"].cpu().numpy()\n",
    "    onnx_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    \n",
    "    onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "    logits = onnx_outputs[0]\n",
    "    \n",
    "    predicted_class = int(np.argmax(logits, axis=1)[0])\n",
    "    \n",
    "    probs = F.softmax(torch.tensor(logits), dim=1).numpy().squeeze()\n",
    "    predicted_prob = float(probs[predicted_class])\n",
    "    \n",
    "    mapped_score = map_score_linear(predicted_class)\n",
    "    \n",
    "    output = {\n",
    "        \"cefr_scoring\": predicted_class,\n",
    "        \"cefr_scoring_100\": mapped_score,\n",
    "        \"predicted_probability\": round(predicted_prob, 2),\n",
    "        \"scorer\": {\n",
    "            \"version\": \"roberta_large_onnx_scorer\",\n",
    "            \"release\": \"0.1\"\n",
    "        }\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def map_score_linear(score):\n",
    "    evp_to_score = {\n",
    "        0: 17,\n",
    "        1: 33,\n",
    "        2: 50,\n",
    "        3: 67,\n",
    "        4: 83,\n",
    "        5: 100,\n",
    "    }\n",
    "    return evp_to_score.get(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cefr_scoring': 4, 'cefr_scoring_100': 83, 'predicted_probability': 0.88, 'scorer': {'version': 'roberta_large_onnx_scorer', 'release': '0.1'}}\n"
     ]
    }
   ],
   "source": [
    "example_input = {\n",
    "    \"ef_level\": 10,\n",
    "    \"activity_instructions\": \"Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible. \",\n",
    "    \"student_submission\": \"Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone’s working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300 000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000\"\n",
    "}\n",
    "\n",
    "result = inference(example_input, onnx_model=session)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
