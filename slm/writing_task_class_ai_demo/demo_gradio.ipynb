{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX coherence model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define S3 bucket and model key\n",
    "bucket_name = 'sagemaker-studio-oxs6vznjds'\n",
    "\n",
    "model_key = 'writing_task_models/coherence/model_1200_roberta_large.onnx'\n",
    "local_model_path = '/tmp/roberta-large-ft-coh-writing-task-1200.onnx'  # or wherever you want to save temporarily\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the ONNX model from S3 to local path\n",
    "s3.download_file(bucket_name, model_key, local_model_path)\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "session_coh = ort.InferenceSession(local_model_path)\n",
    "\n",
    "print(\"ONNX coherence model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX accuracy model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_key = 'writing_task_models/accuracy/model_1800_quantized_roberta_large.onnx'\n",
    "local_model_path = '/tmp/roberta-large-ft-acc-writing-task-1800-quant.onnx'  # or wherever you want to save temporarily\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the ONNX model from S3 to local path\n",
    "s3.download_file(bucket_name, model_key, local_model_path)\n",
    "\n",
    "# Load the ONNX model using onnxruntime\n",
    "session_acc = ort.InferenceSession(local_model_path)\n",
    "\n",
    "print(\"ONNX accuracy model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256  # Ajuste selon la taille maximale de ton mod√®le\n",
    "import torch.nn.functional as F # pour softmax\n",
    "import torch  \n",
    "import numpy as np\n",
    "\n",
    "def format_text_inference(ef_level, activity_instructions, student_submission):\n",
    "    return (\n",
    "        f\"Prompt Level: {ef_level} [SEP] Prompt: {activity_instructions} [SEP] Response: {student_submission}\"\n",
    "    )\n",
    "\n",
    "def inference(input_json, onnx_model):\n",
    "    ef_level = int(input_json[\"ef_level\"])\n",
    "    activity_instructions = input_json[\"activity_instructions\"]\n",
    "    student_submission = input_json[\"student_submission\"]\n",
    "    \n",
    "    formatted_text = format_text_inference(ef_level, activity_instructions, student_submission)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        formatted_text, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].cpu().numpy()\n",
    "    attention_mask = inputs[\"attention_mask\"].cpu().numpy()\n",
    "    onnx_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    \n",
    "    onnx_outputs = onnx_model.run(None, onnx_inputs)\n",
    "    logits = onnx_outputs[0]\n",
    "    \n",
    "    predicted_class = int(np.argmax(logits, axis=1)[0])\n",
    "    \n",
    "    probs = F.softmax(torch.tensor(logits), dim=1).numpy().squeeze()\n",
    "    predicted_prob = float(probs[predicted_class])\n",
    "    \n",
    "    mapped_score = map_score_linear(predicted_class)\n",
    "    \n",
    "    output = {\n",
    "        \"cefr_scoring\": predicted_class,\n",
    "        \"cefr_scoring_100\": mapped_score,\n",
    "        \"predicted_probability\": round(predicted_prob, 2),\n",
    "        \"scorer\": {\n",
    "            \"version\": \"roberta_large_onnx_scorer\",\n",
    "            \"release\": \"0.1\"\n",
    "        }\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def map_score_linear(score):\n",
    "    evp_to_score = {\n",
    "        0: 17,\n",
    "        1: 33,\n",
    "        2: 50,\n",
    "        3: 67,\n",
    "        4: 83,\n",
    "        5: 100,\n",
    "    }\n",
    "    return evp_to_score.get(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cefr_scoring': 4, 'cefr_scoring_100': 83, 'predicted_probability': 0.66, 'scorer': {'version': 'roberta_large_onnx_scorer', 'release': '0.1'}}\n",
      "{'cefr_scoring': 4, 'cefr_scoring_100': 83, 'predicted_probability': 0.94, 'scorer': {'version': 'roberta_large_onnx_scorer', 'release': '0.1'}}\n"
     ]
    }
   ],
   "source": [
    "example_input = {\n",
    "    \"ef_level\": 10,\n",
    "    \"activity_instructions\": \"Read the email from your manager. Then respond with an email that has several ideas to help her solve the budget problem. Type in the input box. Write between 80 and 100 words. Use your own words where possible. \",\n",
    "    \"student_submission\": \"Response: Hi Carla,\\n\\nThe financial report was shocking. We have a budget crisis and I have a list of options how to deal with this crisis on a long-team basis. \\n\\n-First I would recommend that we would cut down everyone‚Äôs working hours. The company would save about $10000 per worker each year. \\n-Secondly we should think about offering older workers a large retirement bonus if they accept our resignation package. If we lay off senior workers we could save about $300¬†000 every year.\\n-Thirdly I would also recommend updating our offices to present-day. We have many offices which are too huge and expensive and old-fashioned. If we move office space to another location we could save money in rent. By changing location we could possibly save about $10000\"\n",
    "}\n",
    "\n",
    "result_acc = inference(example_input, onnx_model=session_acc)\n",
    "result_coh = inference(example_input, onnx_model=session_coh)\n",
    "\n",
    "print(result_acc)\n",
    "print(result_coh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_inference(ef_level, activity_instructions, student_submission):\n",
    "    return (\n",
    "        f\"Prompt Level: {ef_level} [SEP] Prompt: {activity_instructions} [SEP] Response: {student_submission}\"\n",
    "    )\n",
    "\n",
    "def inference(input_json, onnx_model):\n",
    "    ef_level = int(float(input_json[\"ef_level\"]))\n",
    "    cefr_level = input_json[\"cefr_level\"]\n",
    "    activity_instructions = input_json[\"activity_instructions\"]\n",
    "    student_submission = input_json[\"student_submission\"]\n",
    "\n",
    "    formatted_text = format_text_inference(ef_level, activity_instructions, student_submission)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].cpu().numpy()\n",
    "    attention_mask = inputs[\"attention_mask\"].cpu().numpy()\n",
    "    onnx_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    logits = onnx_model.run(None, onnx_inputs)[0]\n",
    "    probs = F.softmax(torch.tensor(logits), dim=1).numpy().squeeze()\n",
    "\n",
    "    predicted_class = int(np.argmax(probs))\n",
    "    mapped_score = map_score_linear(predicted_class)\n",
    "\n",
    "    return {\n",
    "        \"cefr_scoring\": predicted_class,\n",
    "        \"score_100\": mapped_score\n",
    "    }\n",
    "\n",
    "def map_score_linear(score):\n",
    "    evp_to_score = {\n",
    "        0: 17,\n",
    "        1: 33,\n",
    "        2: 50,\n",
    "        3: 67,\n",
    "        4: 83,\n",
    "        5: 100,\n",
    "    }\n",
    "    return evp_to_score.get(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evp_to_score = {\n",
    "    \"A1\": 17,\n",
    "    \"A2\": 33,\n",
    "    \"B1\": 50,\n",
    "    \"B2\": 67,\n",
    "    \"C1\": 83,\n",
    "    \"C2\": 100,\n",
    "}\n",
    "\n",
    "# Corresponding colors for CEFR bands\n",
    "cefr_colors = {\n",
    "    \"A1\": \"#F44336\",  # red\n",
    "    \"A2\": \"#FF9800\",  # orange\n",
    "    \"B1\": \"#FFEB3B\",  # yellow\n",
    "    \"B2\": \"#CDDC39\",  # lime\n",
    "    \"C1\": \"#4CAF50\",  # green\n",
    "    \"C2\": \"#2E7D32\",  # dark green\n",
    "}\n",
    "\n",
    "def score_to_cefr(score):\n",
    "    # Find the CEFR level closest to the score\n",
    "    for level, thresh in evp_to_score.items():\n",
    "        if score <= thresh:\n",
    "            return level\n",
    "    return \"C2\"\n",
    "\n",
    "\n",
    "def predict(ef_level, cefr_level, prompt, response):\n",
    "    input_json = {\n",
    "        \"ef_level\": ef_level,\n",
    "        \"cefr_level\": cefr_level,\n",
    "        \"activity_instructions\": prompt,\n",
    "        \"student_submission\": response\n",
    "    }\n",
    "\n",
    "    acc_result = inference(input_json, session_acc)\n",
    "    coh_result = inference(input_json, session_coh)\n",
    "\n",
    "    acc_level = score_to_cefr(acc_result[\"score_100\"])\n",
    "    coh_level = score_to_cefr(coh_result[\"score_100\"])\n",
    "\n",
    "    # Return slider values + colored labels\n",
    "    acc_text = f\"**Accuracy Score:** {acc_result['score_100']} ({acc_level})\"\n",
    "    coh_text = f\"**Coherence Score:** {coh_result['score_100']} ({coh_level})\"\n",
    "\n",
    "    return acc_result['score_100'], coh_result['score_100'], acc_text, coh_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.0,\n",
       " 'B2',\n",
       " 'Your manager sent you an email, asking you to do the company update. In your own words, write an outline for your presentation. Your outline should include an introduction, the main points, a summary and next steps. Type in the input box. Write 80-120 words. Use your own words where possible.',\n",
       " 'Outline for Company Update Presentation\\n\\n1. Introduction\\n\\nWelcome the team and thank everyone for attending.\\n\\nBriefly explain the purpose of the update: to share key achievements, challenges, and upcoming priorities.\\n\\n\\n2. Main Points\\n\\nPerformance Highlights: Review recent successes, including metrics and milestones achieved.\\n\\nCurrent Challenges: Discuss any obstacles and how the team is addressing them.\\n\\nUpcoming Projects: Outline key initiatives and goals for the next quarter.\\n\\n\\n3. Summary\\n\\nRecap the key takeaways: progress made, challenges faced, and priorities ahead.\\n\\n\\n4. Next Steps\\n\\nShare specific actions for the team, including deadlines and collaboration opportunities.\\n\\nEncourage feedback and questions.\\n\\n\\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_sample_data = pd.read_csv(\"/home/ec2-user/workspace/llm_fine_tuning/slm/writing_task_class_ai_acc/data/acc_data.csv\", nrows=300)\n",
    "# Filtrer les lignes o√π ef_level n'est pas null\n",
    "df_sample_data = df_sample_data[df_sample_data[\"ef_level\"].notnull()]\n",
    "print(df_sample_data.shape)\n",
    "\n",
    "def get_random_example():\n",
    "    ligne_random = df_sample_data.sample(1).iloc[0]\n",
    "    return ligne_random[\"ef_level\"], ligne_random[\"cefr_level\"],  ligne_random[\"activity_instructions\"], ligne_random[\"student_submission\"]\n",
    "\n",
    "get_random_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://9eda6bdd0d4aad090a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9eda6bdd0d4aad090a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üß† ONNX Scorer (Accuracy & Coherence)\\n\"\n",
    "                \"Enter the prompt level, prompt text, and student response. \"\n",
    "                \"Use Random Example to fill inputs automatically.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        ef_level = gr.Textbox(label=\"Prompt Level\", placeholder=\"e.g. 2\", max_lines=1)\n",
    "        cefr_level = gr.Textbox(label=\"Prompt CEFR Level\", placeholder=\"e.g. A1\", max_lines=1)\n",
    "        prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Enter the prompt text\", lines=2)\n",
    "        response = gr.Textbox(label=\"Response\", placeholder=\"Enter the student response\", lines=2)\n",
    "\n",
    "    with gr.Row():\n",
    "        example_btn = gr.Button(\"üé≤ Random Example\")\n",
    "        reset_btn = gr.Button(\"‚ôªÔ∏è Reset\")\n",
    "\n",
    "    predict_btn = gr.Button(\"Predict\")\n",
    "\n",
    "    with gr.Row():\n",
    "        score_acc = gr.Slider(label=\"Accuracy Score\", minimum=0, maximum=100, interactive=False)\n",
    "        score_coh = gr.Slider(label=\"Coherence Score\", minimum=0, maximum=100, interactive=False)\n",
    "\n",
    "    # Colored CEFR label as Markdown\n",
    "    acc_label = gr.Markdown(\"\")\n",
    "    coh_label = gr.Markdown(\"\")\n",
    "\n",
    "    example_btn.click(fn=get_random_example, outputs=[ef_level, cefr_level, prompt, response])\n",
    "    reset_btn.click(fn=lambda: (\"\", \"\", \"\", \"\"), outputs=[ef_level, cefr_level,  prompt, response])\n",
    "    predict_btn.click(fn=predict, \n",
    "                      inputs=[ef_level, cefr_level, prompt, response], \n",
    "                      outputs=[score_acc, score_coh, acc_label, coh_label])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
